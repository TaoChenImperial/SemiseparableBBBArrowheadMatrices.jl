% SIAM Article Template
\documentclass[review,onefignum,onetabnum]{siamart171218}

% Information that is shared between the article and the supplement
% (title and author information, macros, packages, etc.) goes into
% ex_shared.tex. If there is no supplement, this file can be included
% directly.

% Packages and macros go here
\usepackage{lipsum}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{epstopdf}
%\usepackage{algorithmic}
\usepackage{algorithm} 


\usepackage{todonotes}
\newcommand{\sotodo}{\todo[color=green]}
\newcommand{\sotodoinline}{\todo[color=green,inline=true]}
\newcommand{\tctodo}{\todo[color=red]}
\newcommand{\tctodoinline}{\todo[color=red,inline=true]}

\renewcommand{\thealgorithm}{\thesubsection.\arabic{algorithm}}

\def\bbR{{\mathbb R}}
\def\BPS{{\mathrm{BPS}}_{(r,p),(l,u)}^{n \times n}}
\def\calP{{\mathcal P}}
\def\calV{{\mathcal V}}

\makeatletter
\@addtoreset{algorithm}{subsection}
\makeatother
\usepackage{algpseudocode}
\newtheorem{remark}{Remark}[section]
\newtheorem{my_algorithm}{Algorithm}[section]

\ifpdf
  \DeclareGraphicsExtensions{.eps,.pdf,.png,.jpg}
\else
  \DeclareGraphicsExtensions{.eps}
\fi

% Add a serial/Oxford comma by default.
\newcommand{\creflastconjunction}{, and~}

% Used for creating new theorem and remark environments
\newsiamremark{hypothesis}{Hypothesis}
\crefname{hypothesis}{Hypothesis}{Hypotheses}
\newsiamthm{claim}{Claim}

% Sets running headers as well as PDF title and authors
\headers{QR for Banded-Plus-Semiseparable Matrices}{T. Chen and S. Olver}

% Title. If the supplement option is on, then "Supplementary Material"
% is automatically inserted before the title.
\title{The  QR Factorization for Banded-Plus-Semiseparable Matrices is computable in linear complexity}

% Authors: full names plus addresses.
\author{Tao Chen\thanks{Department of Mathematics, Imperial College London, London, UK
  (\email{t.chen24@imperial.ac.uk}, \email{s.olver@imperial.ac.uk}).}
\and Sheehan Olver\footnotemark[1]}

\usepackage{amsopn}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\triu}{triu}
\DeclareMathOperator{\tril}{tril}


%% Added on Overleaf: enabling xr
\makeatletter
\newcommand*{\addFileDependency}[1]{% argument=file name and extension
  \typeout{(#1)}% latexmk will find this if $recorder=0 (however, in that case, it will ignore #1 if it is a .aux or .pdf file etc and it exists! if it doesn't exist, it will appear in the list of dependents regardless)
  \@addtofilelist{#1}% if you want it to appear in \listfiles, not really necessary and latexmk doesn't use this
  \IfFileExists{#1}{}{\typeout{No file #1.}}% latexmk will find this message if #1 doesn't exist (yet)
}
\makeatother

\newcommand*{\myexternaldocument}[1]{%
    \externaldocument{#1}%
    \addFileDependency{#1.tex}%
    \addFileDependency{#1.aux}%
}

% Optional PDF information
\ifpdf
\hypersetup{
  pdftitle={The  QR factorization of a Banded-Plus-Semiseparable Matrix is Computable with Linear Complexity},
  pdfauthor={D. Doe, P. T. Frank, and J. E. Smith}
}
\fi

% The next statement enables references to information in the
% supplement. See the xr-hyperref package for details.

%% Use \myexternaldocument on Overleaf
%\myexternaldocument{ex_supplement}

% FundRef data to be entered by SIAM
%<funding-group>
%<award-group>
%<funding-source>
%<named-content content-type="funder-name"> 
%</named-content> 
%<named-content content-type="funder-identifier"> 
%</named-content>
%</funding-source>
%<award-id> </award-id>
%</award-group>
%</funding-group>

\begin{document}

\maketitle

\begin{abstract}
We show that each stage of the QR factorization of banded-plus-semiseparable matrices computed using Householder reflections has a specific structured perturbation. This theoretical result enables the design of linear-complexity algorithms for QR factorization and for solving the associated linear systems. For symmetric BPS matrices, we further show that the $RQ$ product---central to eigenvalue computations via the QR algorithm---also preserves the BPS structure, leading to a linear-complexity algorithm for its formation. Numerical experiments validate the optimal linear complexity  and demonstrate substantial speedups compared with existing hierarchical approaches. The algorithms have been implemented in an open-source Julia package, providing an efficient and accessible platform for practical use.
\end{abstract}

\begin{keywords}
banded-plus-semiseparable matrices, QR factorization, linear complexity, structured matrices, direct solvers
\end{keywords}

\begin{AMS}
65F05, 65F50, 15A23, 65Y20
\end{AMS}

\section{Introduction}

Banded-plus-semiseparable (BPS) matrices, expressible as 
\begin{equation*}
A = \underbrace{B}_{\text{banded}} + \underbrace{\tril(UV^\top, -1)}_{\text{lower semiseparable, rank } r} + \underbrace{\triu(WS^\top, 1)}_{\text{upper semiseparable, rank } p} \in \bbR^{n \times n},
\end{equation*}
 arise in numerous applications from spectral methods for differential equations~\cite{knook2024quasi, iserles2025stable} to signal processing, control theory, and eigenvalue problems~\cite{vandebril2005note}. Importantly, the inverse of a banded matrix is itself a BPS matrix, making this structure fundamental for solving banded linear systems via their inverses \cite{rozsa1991band}. Their structure requires only $O(n)$ storage which invites the development of \( O(n) \) algorithms, a goal successfully achieved for iterations of the QR algorithm for symmetric semiseparable systems~\cite{vandebril2005implicit}, and for solving linear systems with \emph{diagonal}-plus-semiseparable matrices~\cite{eidelman1997inversion}. 
  However, generalizing these results to the case where the banded part $B$ is a genuine banded matrix, rather than merely a diagonal one, presents significant algorithmic challenges. 


\sotodoinline{Clarify exact relationship between prior work and ours}

Pioneering work established \( O(n) \) solvers for the BPS matrices via ULV factorization~\cite{chandrasekaran2003fast}. BPS matrices can also be viewed as  hierarchically semiseparable (HSS) matrices, and solvers using HSS structure is a well-developed area~\cite{chandrasekaran2006fast, chandrasekaran2007fast, xia2010fast, massei2020hm}. A parallel line of research extensively developed the theory and algorithms for semiseparable and quasiseparable matrices, including implicit QR algorithms for \emph{symmetric} semiseparable matrices~\cite{vandebril2005implicit}, structure-preserving analyses~\cite{eidelman2005qr, delvaux2006structures, delvaux2006rank}, approaches leveraging rational Krylov techniques~\cite{fasino2005rational, vandebril2008rational}, and alternative representations~\cite{vandebril2005note, delvaux2008givens}. Despite these advances, a clear theoretical guarantee that the standard QR factorization preserves the BPS structure has been missing, with most existing solvers relying on more complex ULV or intricate Givens-based schemes~\cite{mastronardi2001fast, chandrasekaran2003fast, van2004two, delvaux2008qr}. A special case of BPS matrices are almost banded matrices which were used in \cite{olver2013fast} to represent discretisations of differential equations using the ultraspherical spectral method. An optimal complexity adaptive QR factorization was introduced, which also gives an optimal complexity QR factorization for BPS matrices with now lower semiseparable part ($r = 0$). It also introduced an optimal complexity back-substitution for upper-triangular BPS matrices, an algorithm we also use.

In this paper, we close this theoretical gap. We prove that the QR factorization of a BPS matrix yields a factor matrix \( F \), which is the matrix containing both $R$ and the Householder reflectors encoding $Q$,  that is itself BPS, with precisely characterized lower rank \( r \), upper rank \( r+p \), and bandwidths \( l \) and \( l+m \). This pivotal result, proven via an inductive framework involving a new class of Householder-Modified BPS Matrices (HMBPSM), which enables the design of an \( O(n) \) QR factorization. Furthermore, it facilitates a complete direct solver: applying \( Q^\top \) and performing backward substitution on the structured factor \( R \) are also achieved in linear time. Our work thus provides a unified, QR-based, end-to-end \( O(n) \) solution for BPS systems, backed by a rigorous structure-preservation theorem. Furthermore, we show that for symmetric BPS matrices, the $RQ$ product (central to the QR eigenvalue algorithm) also retains the BPS structure, enabling efficient eigenvalue computations.

While this paper focuses on real square matrices, the techniques and results naturally extend to complex matrices by replacing transposes with conjugate transposes. The extension to rectangular matrices is also straightforward, as Householder QR factorization applies similarly to rectangular matrices, and the structural arguments carry over with minor adjustments. These extensions are omitted but represent immediate generalizations of the theory presented here.

The rest of this paper is organized as follows. Section~\ref{sec:main} presents our main theoretical contributions: the definitions, the core lemma on structure preservation under Householder transformations, and the main theorem with its proof. Section~\ref{sec:algorithms} details the resulting $O(n)$ algorithms for QR factorization, application of $Q^\top$, and backward substitution. Building on these foundations, Section~\ref{sec:fastrq} addresses symmetric BPS matrices, proving that the $RQ$ product preserves the BPS structure and presenting a linear-complexity algorithm for its computation. Section~\ref{sec:experiments} presents numerical experiments that confirm the linear complexity and demonstrate performance advantages. We conclude in Section~\ref{sec:conclusions} with a discussion of future work.
\section{Main results}
\label{sec:main}

\subsection{Problem Formulation and Notation}

We begin by establishing notation for matrix subblocks. For a matrix $M \in \mathbb{R}^{n \times n}$, let $M[i:j, m:n]$ denote the submatrix consisting of rows $i$ through $j$ and columns $m$ through $n$ (inclusive). When extracting a single row or column, the result is a row or column vector, respectively:
\begin{itemize}
    \item $M[i:j, m] \in \mathbb{R}^{j-i+1}$: the $m$-th column from row $i$ to row $j$ (a column vector).
    \item $M[i, m:n] \in \mathbb{R}^{n-m+1}$: the $i$-th row from column $m$ to column $n$ (also a column vector).
\end{itemize}
For brevity, we write $M[k,:]$ for the entire $k$-th row and $M[:,k]$ for the entire $k$-th column.

\begin{definition}
    A {\it banded-plus-semseparable matrix} (BPS) with lower-semisepable rank $r$, upper-semiseparable rank $p$, lower-bandwidth $l$\sotodo{I prefer $\ell$} and upper-bandwidth $m$\sotodo{I usually use $u$ for upper-bandwidth}  is $A \in \BPS \subset \bbR^{n \times n}$ such that
\[
A = B + \tril(UV^T, -1) + \triu(WS^T, 1)\label{express_A}
\]
where \( U, V \in \mathbb{R}^{n\times r} \), \( W, S \in \mathbb{R}^{n\times p} \), and\sotodo{Change $i$ to $k$} \( B = (b_{ij})_{i,j=1}^n \in \mathbb{R}^{n \times n} \) is a banded matrix satisfying \( b_{ij}=0 \) for \( i-j>l \) or \( j-i>m \).
\end{definition}


Define vectors \( \bar{\mathbf{u}}_i = U[i,:]^T \in \mathbb{R}^r \), \( \bar{\mathbf{v}}_i = V[i,:]^T \in \mathbb{R}^r \), \( \bar{\mathbf{w}}_i = W[i,:]^T \in \mathbb{R}^p \), and \( \bar{\mathbf{s}}_i = S[i,:]^T \in \mathbb{R}^p \) for \( i=1,\ldots,n \). The matrix \( A \) can then be expressed element-wise as: \sotodo{Only number equations that are cited}
\begin{equation}
A =
\begin{bmatrix}
b_{11} & \bar{\mathbf{w}}_1^T \bar{\mathbf{s}}_2 + b_{12} & \cdots & \bar{\mathbf{w}}_1^T \bar{\mathbf{s}}_n + b_{1n} \\
\bar{\mathbf{u}}_2^T \bar{\mathbf{v}}_1 + b_{21} & b_{22} & \cdots & \bar{\mathbf{w}}_2^T \bar{\mathbf{s}}_n + b_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
\bar{\mathbf{u}}_n^T \bar{\mathbf{v}}_1 + b_{n1} & \bar{\mathbf{u}}_n^T \bar{\mathbf{v}}_2 + b_{n2} & \cdots & b_{nn}
\end{bmatrix}.
\end{equation}

Applying the QR factorization to \( A \) yields a factor matrix \( F \), whose upper triangular part stores the matrix \( R \) and whose lower triangular part contains the Householder reflection vectors \( \mathbf{y} \) generated during the factorization. We will demonstrate that \( F \) itself retains a banded-plus-semiseparable structure. Specifically, its lower semiseparable part has rank \( r \), its upper semiseparable part has rank \( r+p \), its lower bandwidth is \( l \), and its upper bandwidth is \( l+m \).

Before proceeding with the detailed proof, let us clarify the precise structure of the factor matrix \( F \) obtained from the QR factorization. In this work, following the convention of LAPACK~\cite{doi:10.1137/1.9780898719604}, we employ a compact representation that stores the complete information of the QR factorization in a single matrix:

\begin{equation}
F = \begin{bmatrix}
r_{11} & r_{12} & r_{13} & \cdots & r_{1n} \\
y_{2,1} & r_{22} & r_{23} & \cdots & r_{2n} \\
y_{3,1} & y_{3,2} & r_{33} & \cdots & r_{3n} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
y_{n,1} & y_{n,2} & y_{n,3} & \cdots & r_{nn}
\end{bmatrix}
\end{equation}
where: the upper triangular part (including the main diagonal) of \( F \) stores the elements of the upper triangular matrix \( R \), i.e.: $R = \triu(F)$, and thestrictly lower triangular part (excluding the main diagonal) of \( F \) stores the last \( n-k \) elements of (rescaled) Householder reflection vectors \( \mathbf{y}_k \) generated at each step.

More specifically, at the \( k \)-th Householder transformation step (\( k = 1, 2, \ldots, n-1 \)), we construct a reflection vector \( \mathbf{y}_k \) to eliminate the subdiagonal entries of the \( k \)-th column. This vector takes the form:
\begin{equation}\label{y_k}
\mathbf{y}_k = \begin{bmatrix} 0 \\ \vdots \\ 0 \\ 1 \\ y_{k+1,k} \\ \vdots \\ y_{n,k} \end{bmatrix}
\begin{array}{l} \left.\vphantom{\begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}}\right\} k-1 \text{ zeros} \\ \\ \left.\vphantom{\begin{bmatrix} y_{k+1,k} \\ \vdots \\ y_{n,k} \end{bmatrix}}\right\} n-k \text{ elements}
\end{array}
\end{equation}
Following the LAPack format, we normalize \( \mathbf{y}_k \) such that its first nonzero element (the \( k \)-th element) equals 1. Consequently, we only need to store the elements from position \( k+1 \) to \( n \) of this vector, which are placed in the \( k \)-th column of \( F \), from row \( k+1 \) to \( n \).

The advantage of this representation is that it compactly stores the information of both the orthogonal matrix \( Q \) (via the Householder vectors) and the upper triangular matrix \( R \) within a single matrix \( F \). The central result of this paper will demonstrate that for a banded-plus-semiseparable matrix \( A \), this factor matrix \( F \) itself maintains a banded-plus-semiseparable structure.

It is important to note that with this normalization convention (where the first nonzero element of each Householder vector \( \mathbf{y}_k \) is 1), the full Householder transformation at the \( k \)-th step is given by \( I - \tau_k \mathbf{y}_k \mathbf{y}_k^T \), where \( \tau_k \) is a scalar coefficient. Therefore, in addition to the factor matrix \( F \), a vector \( \boldsymbol{\tau} = (\tau_1, \tau_2, \ldots, \tau_{n-1})^T \) is required to completely represent the QR factorization. The orthogonal matrix \( Q \) can be reconstructed as the product \( Q = (I - \tau_1 \mathbf{y}_1 \mathbf{y}_1^T)(I - \tau_2 \mathbf{y}_2 \mathbf{y}_2^T) \cdots (I - \tau_{n-1} \mathbf{y}_{n-1} \mathbf{y}_{n-1}^T) \).

Throughout our analysis, we will focus on the structure of the factor matrix \( F \), while acknowledging that the complete QR representation consists of the pair \( (F, \boldsymbol{\tau}) \). Our main theorem establishes that \( F \) maintains the banded-plus-semiseparable structure; the scaling coefficients \( \tau_k \) can be stored separately without affecting the structural properties of the algorithm.

We proceed to prove this by induction. First, we introduce two key definitions and a pivotal lemma.

\subsection{Core Definitions and a Key Lemma}

While the final factor matrix of a QR factorization is a BPS matrix, at  intermediate  stages it has a specific structured perturbation. Here we describe this structure in terms of a linear space that, at each stage, the perturbation to the principle submatrix lies in:

\begin{definition}\label{def:hmbpsm}
Given 
\[
A = B + \tril(UV^T, -1) + \triu(WS^T, 1)  \in \BPS 
\]
define the vector space:
\begin{align*}
\calP(A) &:= \bigg\{
UQS^T + UKU^TA + UE + XS^T + YU^TA + Z :\\
&\qquad\quad  Q \in \mathbb{R}^{r\times p}, K \in \mathbb{R}^{r\times r}, \\
&\qquad\quad E = [E_s \in \mathbb{R}^{r\times \min(l+m,n)}, \mathbf{0}] \in \mathbb{R}^{r \times n} ,\\
 &\qquad\quad  X = \begin{bmatrix} X_s \in \mathbb{R}^{\min(l,n)\times p}  \\ \mathbf{0} \end{bmatrix}  \in \mathbb{R}^{n \times p}, \\
 &\qquad\quad  Y = \begin{bmatrix} Y_s \in \mathbb{R}^{\min(l,n)\times r}  \\ \mathbf{0} \end{bmatrix}   \in \mathbb{R}^{n \times r},\\ 
&\qquad\quad Z = \begin{bmatrix} Z_s  \in \mathbb{R}^{\min(l,n)\times \min(l+m,n)} & \mathbf{0} \\ \mathbf{0} & \mathbf{0} \end{bmatrix} \in \mathbb{R}^{n \times n}
\bigg\} \subset \bbR^{n \times n}.
\end{align*}
\end{definition}

In addition, we need to describe the structure of the upper-triangular part in terms of a structured vector:


\begin{definition}
Given \( A \in  \BPS \) define the vector space
\begin{align*}
\calV(A) &:= \bigg\{\mathbf{d}^T + \boldsymbol{\alpha}^T (S^T[:,2:n]) + \boldsymbol{\beta}^T ((U^TA)[:,2:n]) : \\
&\qquad\quad \mathbf{d} = \begin{bmatrix} \mathbf{d}_s \in \mathbb{R}^{\min(l+m,n-1)}  \\ \mathbf{0} \end{bmatrix} \in \mathbb{R}^{n-1}, \boldsymbol{\alpha} \in \mathbb{R}^p , \boldsymbol{\beta} \in \mathbb{R}^r\bigg\} \subset \bbR^{1 \times (n-1)}.
\end{align*}
\end{definition}


\begin{lemma}\label{lemma:structure_preserve}
Given \( A \in \BPS \) and  \( P \in \calP(A) \), suppose a Householder transformation is applied to \( A+P \) to eliminate the subdiagonal entries of its first column, yielding\sotodo{generalise so it doesn't need to be Householder} \( \tilde{C} = (I - \tau \mathbf{y} \mathbf{y}^T) (A+P) \). Then the following hold:
\begin{enumerate}
    \item The principal submatrix satisfies \( \tilde{C}[2:n, 2:n] = A[2:n, 2:n] + \tilde P\) for \(\tilde P  \in \calP(A[2:n, 2:n]) \).
    \item The first row satisfies \( \tilde{C}[1, 2:n] \in \calV(A) \).
\end{enumerate}
\end{lemma}

\begin{proof}
    \sotodoinline{I'll start updating the proof tomorrow}
Let us introduce the necessary notation:
\begin{itemize}
    \item \( A = B + \tril(UV^T, -1) + \triu(WS^T, 1) \), where \( U=(\mathbf{u}_1,...,\mathbf{u}_r) \in \mathbb{R}^{n\times r}\), \( V=(\mathbf{v}_1,...,\mathbf{v}_r)\in \mathbb{R}^{n\times r}\), \( W=(\mathbf{w}_1,...,\mathbf{w}_p)\in \mathbb{R}^{n\times p}\), and \(S=(\mathbf{s}_1,...,\mathbf{s}_p)\in \mathbb{R}^{n \times p}\). Here \( \mathbf{u}_i=(u_1^{(i)},...,u_n^{(i)})^T\in \mathbb{R}^n\) and \( \mathbf{v}_i=(v_1^{(i)},...,v_n^{(i)})^T\in \mathbb{R}^n\) for $i=1,...,r$; \( \mathbf{w}_i=(w_1^{(i)},...,w_n^{(i)})^T\in \mathbb{R}^n\) and \( \mathbf{s}_i=(s_1^{(i)},...,s_n^{(i)})^T\in \mathbb{R}^n\) for \( i=1,...,p\). Also, \( B=(b_{ij})_{i,j=1}^n\in \mathbb{R}^{n,n}\) with \( b_{ij}=0\) if \( i-j>l\) or \( j-i>m\).
    \item \( C = A + UQS^T + UKU^TA + UE + XS^T + YU^TA + Z \), where \( Q, K, E, X, Y, Z \) are as in Definition \cref{def:hmbpsm}.
    \item \( \tilde{C} = (I - \tau \mathbf{y} \mathbf{y}^T)C \), where the Householder vector \( \mathbf{y} \) can be expressed as \( \mathbf{y} = \mathbf{e}_1 + U^{(2)}\bar{\mathbf{k}} + \mathbf{b} \). Here, \(\mathbf{e}_1=(1,0,...,0)^T\in \mathbb{R}^n\), \( U^{(2)} \in \mathbb{R}^{n\times r} \) satisfies \( U^{(2)}[1,:] = \mathbf{0} \) and \( U^{(2)}[2:n,:] = U[2:n,:] \), \( \bar{\mathbf{k}} \in \mathbb{R}^r \), \( \mathbf{b} = (0, b_2, \ldots, b_{\min(l+1,n)}, 0, \ldots, 0)^T \in \mathbb{R}^n \), and \(\tau\) is a coefficient found to satisfy the definition of a Householder transformation.
\end{itemize}

Let \( \bar{\mathbf{u}}_1 = (u_1^{(1)}, \ldots, u_1^{(r)})^T \in \mathbb{R}^r \). We can write:
\[
\mathbf{e}_1^T A = \underbrace{\mathbf{d}_1^T}_{\min(m+1,n)\hbox{ nonzero entries}} + \underbrace{\bar{\mathbf{w}}_1^T}_{\in \bbR^{1 \times p}} S^T,
\]
where \( \mathbf{d}_1 = B[1,:]^T \in \mathbb{R}^n \), \( \bar{\mathbf{w}}_1 = (w_1^{(1)}, \ldots, w_1^{(p)})^T \in \mathbb{R}^p,\) and
\[
\mathbf{b}^T A = \underbrace{\bar{\mathbf{d}}^T}_{\min(l+m+1, n) \hbox{ nonzero entries}} + {\underbrace{\mathbf{f}^T}_{\mathbf{b}^T W  \in \mathbb{R}^{1 \times p} }} S^T.
\]



Define the auxiliary vectors:
\begin{align}
    \mathbf{c}_1 &= Q^T U^T \mathbf{y} \in \mathbb{R}^p \\
    \mathbf{c}_2 &= K^T U^T \mathbf{y} \in \mathbb{R}^r \\
    \mathbf{c}_3 &= U^T \mathbf{y} \in \mathbb{R}^r \\
    \mathbf{c}_4 &= X^T \mathbf{y} \in \mathbb{R}^p \\
    \mathbf{c}_5 &= Y^T \mathbf{y} \in \mathbb{R}^r \\
    \mathbf{c}_6 &= Z^T \mathbf{y} \in \mathbb{R}^{n}, \quad \text{which has the form } \mathbf{c}_6 = \begin{bmatrix} \mathbf{c}_{6s} \\ \mathbf{0} \end{bmatrix} \text{ with } \mathbf{c}_{6s} \in \mathbb{R}^{\min(l+m,n)}.
\end{align}
Also, let \( \mathbf{x}^{(1)} = X[1,:]^T \in \mathbb{R}^p \), \( \mathbf{y}^{(1)} = Y[1,:]^T \in \mathbb{R}^r \), and \( \mathbf{z}^{(1)} = Z[1,:]^T \in \mathbb{R}^n \).

We now compute \( (I - \tau \mathbf{y} \mathbf{y}^T) C \) by distributing the transformation over each term in the definition of \( C \).

\textbf{(i) Transformation of \( A \):}
Substituting the expressions \( \mathbf{y}=\mathbf{e}_1+U^{(2)}\bar{\mathbf{k}}+\mathbf{b}\), \( \mathbf{e}_1^TA=\mathbf{d}_1^T+\bar{\mathbf{w}}_1^TS^T\), and \( \mathbf{b}^TA=\bar{\mathbf{d}}^T+\mathbf{f}^TS^T\), we obtain:
\begin{equation}\label{proof_first}
\begin{aligned}
(I - \tau \mathbf{y} \mathbf{y}^T)A = & A + \mathbf{e}_1 \big[ \underbrace{(-\tau \mathbf{d}_1^T - \tau \bar{\mathbf{d}}^T)}_{\min(l+m+1,n)\hbox{ nonzero entries}} + \underbrace{(-\tau \bar{\mathbf{w}}_1^T - \tau \mathbf{f}^T)}_{\in \bbR^{1 \times p}}S^T \\
& + \underbrace{(-\tau \bar{\mathbf{k}}^T)}_{\in \bbR^{1 \times r}}U^{(2)T}A\big] + U^{(2)}\underbrace{(-\tau \bar{\mathbf{k}} \bar{\mathbf{w}}_1^T - \tau \bar{\mathbf{k}} \mathbf{f}^T)}_{\in \bbR^{r \times p}} S^T \\
& + U^{(2)}\underbrace{(-\tau \bar{\mathbf{k}} \bar{\mathbf{k}}^T)}_{\in \bbR^{r \times r}}U^{(2)T}A + U^{(2)}(-\tau \bar{\mathbf{k}} \mathbf{d}_1^T - \tau \bar{\mathbf{k}} \bar{\mathbf{d}}^T) \\
& + (-\tau \mathbf{b} \bar{\mathbf{w}}_1^T - \tau \mathbf{b} \mathbf{f}^T)S^T + (-\tau \mathbf{b} \bar{\mathbf{k}}^T)U^{(2)T}A + (-\tau \mathbf{b} \mathbf{d}_1^T - \tau \mathbf{b} \bar{\mathbf{d}}^T).
\end{aligned}
\end{equation}
Dropping the first column, we  see that the first row of the term in brackets is in $\calV(A)$. Dropping the first row and column of the remaining terms are in $\calP(A[2:n,2:n])$. 

\textbf{(ii) Transformation of \( UQS^T \):} Substituting the expressions \( \mathbf{y}^TUQ=\mathbf{c}_1^T\), \( \mathbf{y}=\mathbf{e}_1+U^{(2)}\bar{\mathbf{k}}+\mathbf{b}\), and \( U=\mathbf{e}_1\bar{\mathbf{u}}_1^T+U^{(2)}\) where \( \bar{\mathbf{u}}_1=(u_1^{(1)},...,u_1^{(r)})\in \mathbb{R}^r\), we obtain
\begin{equation}
\begin{aligned}
(I - \tau \mathbf{y} \mathbf{y}^T)UQS^T = & \mathbf{e}_1(\bar{\mathbf{u}}_1^T Q - \tau \mathbf{c}_1^T)S^T + U^{(2)}(Q - \tau \bar{\mathbf{k}} \mathbf{c}_1^T)S^T + (-\tau \mathbf{b} \mathbf{c}_1^T)S^T.
\end{aligned}
\end{equation}

\textbf{(iii) Transformation of \( UKU^TA \):} Substituting the expressions \( \mathbf{y}^TUK=\mathbf{c}_2^T\), \( \mathbf{y}=\mathbf{e}_1+U^{(2)}\bar{\mathbf{k}}+\mathbf{b}\), \( U=\mathbf{e}_1\bar{\mathbf{u}}_1^T+U^{(2)}\), and \( \mathbf{e}_1^TA=\mathbf{d}_1^T+\bar{\mathbf{w}}_1^TS^T\), we obtain
\begin{equation}
\begin{aligned}
& (I - \tau \mathbf{y} \mathbf{y}^T)UKU^TA \\
= & \mathbf{e}_1(\bar{\mathbf{u}}_1^T K \bar{\mathbf{u}}_1 \mathbf{d}_1^T - \tau \mathbf{c}_2^T \bar{\mathbf{u}}_1 \mathbf{d}_1^T) + \mathbf{e}_1(\bar{\mathbf{u}}_1^T K \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^T - \tau \mathbf{c}_2^T \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^T)S^T + \mathbf{e}_1(\bar{\mathbf{u}}_1^T K - \tau \mathbf{c}_2^T)U^{(2)T}A \\
& + U^{(2)}(K \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^T - \tau \bar{\mathbf{k}} \mathbf{c}_2^T \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^T)S^T + U^{(2)}(K - \tau \bar{\mathbf{k}} \mathbf{c}_2^T)U^{(2)T}A + U^{(2)}(K \bar{\mathbf{u}}_1 \mathbf{d}_1^T - \tau \bar{\mathbf{k}} \mathbf{c}_2^T \bar{\mathbf{u}}_1 \mathbf{d}_1^T) \\
& + (-\tau \mathbf{b} \mathbf{c}_2^T \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^T)S^T + (-\tau \mathbf{b} \mathbf{c}_2^T)U^{(2)T}A + (-\tau \mathbf{b} \mathbf{c}_2^T \bar{\mathbf{u}}_1 \mathbf{d}_1^T).
\end{aligned}
\end{equation}

\textbf{(iv) Transformation of \( UE \):} Substituting the expressions \( \mathbf{y}^TU=\mathbf{c}_3^T\), \( \mathbf{y}=\mathbf{e}_1+U^{(2)}\bar{\mathbf{k}}+\mathbf{b}\), and \( U=\mathbf{e}_1\bar{\mathbf{u}}_1^T+U^{(2)}\), we obtain
\begin{equation}
\begin{aligned}
(I - \tau \mathbf{y} \mathbf{y}^T)UE = \mathbf{e}_1(\bar{\mathbf{u}}_1^T E - \tau \mathbf{c}_3^T E) + U^{(2)}(E - \tau \bar{\mathbf{k}} \mathbf{c}_3^T E) + (-\tau \mathbf{b} \mathbf{c}_3^T E).
\end{aligned}
\end{equation}

\textbf{(v) Transformation of \( XS^T \):} Substituting the expressions \(\mathbf{y}^TX=\mathbf{c}_4^T\) and \(\mathbf{y}=\mathbf{e}_1+U^{(2)}\bar{\mathbf{k}}+\mathbf{b}\), we obtain
\begin{equation}
\begin{aligned}
(I - \tau \mathbf{y} \mathbf{y}^T)XS^T = \mathbf{e}_1(-\tau \mathbf{c}_4^T)S^T + U^{(2)}(-\tau \bar{\mathbf{k}} \mathbf{c}_4^T)S^T + (X - \tau \mathbf{b} \mathbf{c}_4^T)S^T.
\end{aligned}
\end{equation}

\textbf{(vi) Transformation of \( YU^TA \):} Substituting the expressions \(\mathbf{y}^TY=\mathbf{c}_5^T\), \(\mathbf{y}=\mathbf{e}_1+U^{(2)}\bar{\mathbf{k}}+\mathbf{b}\), \(U=\mathbf{e}_1\bar{\mathbf{u}}_1^T+U^{(2)}\), and \(\mathbf{e}_1^TA=\mathbf{d}_1^T+\bar{\mathbf{w}}_1^TS^T\), we obtain
\begin{equation}
\begin{aligned}
& (I - \tau \mathbf{y} \mathbf{y}^T)YU^TA \\
= & \mathbf{e}_1(-\tau \mathbf{c}_5^T \bar{\mathbf{u}}_1 \mathbf{d}_1^T) + \mathbf{e}_1(-\tau \mathbf{c}_5^T \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^T)S^T + \mathbf{e}_1(-\tau \mathbf{c}_5^T)U^{(2)T}A \\
& + U^{(2)}(-\tau \bar{\mathbf{k}} \mathbf{c}_5^T \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^T)S^T + U^{(2)}(-\tau \bar{\mathbf{k}} \mathbf{c}_5^T)U^{(2)T}A + U^{(2)}(-\tau \bar{\mathbf{k}} \mathbf{c}_5^T \bar{\mathbf{u}}_1 \mathbf{d}_1^T) \\
& + (Y \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^T - \tau \mathbf{b} \mathbf{c}_5^T \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^T)S^T + (Y - \tau \mathbf{b} \mathbf{c}_5^T)U^{(2)T}A + (Y \bar{\mathbf{u}}_1 \mathbf{d}_1^T - \tau \mathbf{b} \mathbf{c}_5^T \bar{\mathbf{u}}_1 \mathbf{d}_1^T).
\end{aligned}
\end{equation}

\textbf{(vii) Transformation of \( Z \):} Substituting the expressions \(\mathbf{y}^TZ=\mathbf{c}_6^T\) and \(\mathbf{y}=\mathbf{e}_1+U^{(2)}\bar{\mathbf{k}}+\mathbf{b}\), we obtain

\begin{equation}\label{proof_last}
\begin{aligned}
(I - \tau \mathbf{y} \mathbf{y}^T)Z = \mathbf{e}_1(-\tau \mathbf{c}_6^T) + U^{(2)}(-\tau \bar{\mathbf{k}} \mathbf{c}_6^T) + (Z - \tau \mathbf{b} \mathbf{c}_6^T).
\end{aligned}
\end{equation}

Combining equations \eqref{proof_first} through \eqref{proof_last}, we can now identify the structure of the resulting matrix \( \tilde{C} \).

\textbf{Firstly}, the submatrix \( \tilde{C}[2:n, 2:n] \) satisfies:
\begin{equation}\label{submatrix_structure}
\tilde{C}[2:n, 2:n] = \tilde{A} + \tilde{U} \tilde{Q} \tilde{S}^T + \tilde{U} \tilde{K} \tilde{U}^T \tilde{A} + \tilde{U} \tilde{E} + \tilde{X} \tilde{S}^T + \tilde{Y} \tilde{U}^T \tilde{A} + \tilde{Z},
\end{equation}
where
\begin{align*}
    \tilde{A} &= A[2:n, 2:n] \\
    \tilde{U} &= U[2:n, :] \\
    \tilde{S} &= S[2:n, :]
\end{align*}
and the updated modification matrices are given by:
\begin{align*}
    \tilde{Q} &= -\tau \bar{\mathbf{k}} \bar{\mathbf{w}}_1^T - \tau \bar{\mathbf{k}} \mathbf{f}^T + Q - \tau \bar{\mathbf{k}} \mathbf{c}_1^T + K \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^T - \tau \bar{\mathbf{k}} \mathbf{c}_2^T \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^T - \tau \bar{\mathbf{k}} \mathbf{c}_4^T - \tau \bar{\mathbf{k}} \mathbf{c}_5^T \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^T, \\
    \tilde{K} &= -\tau \bar{\mathbf{k}} \bar{\mathbf{k}}^T + K - \tau \bar{\mathbf{k}} \mathbf{c}_2^T - \tau \bar{\mathbf{k}} \mathbf{c}_5^T, \\
    \tilde{E} &= [\tilde{E}_s, \mathbf{0}] \in \mathbb{R}^{r \times (n-1)}, \quad \text{with} \\
    \tilde{E}_s &= (-\tau \bar{\mathbf{k}} \mathbf{d}_1^T - \tau \bar{\mathbf{k}} \bar{\mathbf{d}}^T + K \bar{\mathbf{u}}_1 \mathbf{d}_1^T - \tau \bar{\mathbf{k}} \mathbf{c}_2^T \bar{\mathbf{u}}_1 \mathbf{d}_1^T + E - \tau \bar{\mathbf{k}} \mathbf{c}_3^T E \\
    &\quad - \tau \bar{\mathbf{k}} \mathbf{c}_5^T \bar{\mathbf{u}}_1 \mathbf{d}_1^T - \tau \bar{\mathbf{k}} \mathbf{c}_6^T)[:, 2:\min(l+m+1, n)], \\
    \tilde{X} &= \begin{bmatrix} \tilde{X}_s \\ \mathbf{0} \end{bmatrix} \in \mathbb{R}^{(n-1) \times p}, \quad \text{with} \\
    \tilde{X}_s &= (-\tau \mathbf{b} \bar{\mathbf{w}}_1^T - \tau \mathbf{b} \mathbf{f}^T - \tau \mathbf{b} \mathbf{c}_1^T - \tau \mathbf{b} \mathbf{c}_2^T \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^T + X - \tau \mathbf{b} \mathbf{c}_4^T \\
    &\quad + Y \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^T - \tau \mathbf{b} \mathbf{c}_5^T \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^T)[2:\min(l+1, n), :], \\
    \tilde{Y} &= \begin{bmatrix} \tilde{Y}_s \\ \mathbf{0} \end{bmatrix} \in \mathbb{R}^{(n-1) \times r}, \quad \text{with} \\
    \tilde{Y}_s &= (-\tau \mathbf{b} \bar{\mathbf{k}}^T - \tau \mathbf{b} \mathbf{c}_2^T + Y - \tau \mathbf{b} \mathbf{c}_5^T)[2:\min(l+1, n), :], \\
    \tilde{Z} &= \begin{bmatrix} \tilde{Z}_s & \mathbf{0} \\ \mathbf{0} & \mathbf{0} \end{bmatrix} \in \mathbb{R}^{(n-1) \times (n-1)}, \quad \text{with} \\
    \tilde{Z}_s &= (-\tau \mathbf{b} \mathbf{d}_1^T - \tau \mathbf{b} \bar{\mathbf{d}}^T - \tau \mathbf{b} \mathbf{c}_2^T \bar{\mathbf{u}}_1 \mathbf{d}_1^T - \tau \mathbf{b} \mathbf{c}_3^T E + Y \bar{\mathbf{u}}_1 \mathbf{d}_1^T - \tau \mathbf{b} \mathbf{c}_5^T \bar{\mathbf{u}}_1 \mathbf{d}_1^T \\
    &\quad + Z - \tau \mathbf{b} \mathbf{c}_6^T)[2:\min(l+1, n), 2:\min(l+m+1, n)].
\end{align*}

The forms of \( \tilde{Q}, \tilde{K}, \tilde{E}, \tilde{X}, \tilde{Y}, \tilde{Z} \) confirm that \( \tilde{C}[2:n, 2:n] \) is an HMBPSM related to \( A[2:n, 2:n] \), thus establishing the first part of the lemma.

\textbf{Secondly}, the first row of the transformed matrix, \( \tilde{C}[1, 2:n] \), can be expressed as:
\begin{equation}
\tilde{C}[1, 2:n] = \hat{\mathbf{d}}^T + \hat{\boldsymbol{\alpha}}^T (S^T[:, 2:n]) + \hat{\boldsymbol{\beta}}^T ((U^{(2)T}A)[:, 2:n]),
\end{equation}
where
\begin{align*}
    \hat{\mathbf{d}} &= \begin{bmatrix} \hat{\mathbf{d}}_s \\ \mathbf{0} \end{bmatrix} \in \mathbb{R}^{n-1}, \quad \text{with} \\
    \hat{\mathbf{d}}_s &= (\mathbf{d}_1^T - \tau \mathbf{d}_1^T - \tau \bar{\mathbf{d}}^T + \bar{\mathbf{u}}_1^T K \bar{\mathbf{u}}_1 \mathbf{d}_1^T - \tau \mathbf{c}_2^T \bar{\mathbf{u}}_1 \mathbf{d}_1^T + \bar{\mathbf{u}}_1^T E - \tau \mathbf{c}_3^T E \\
    &\quad + \mathbf{y}^{(1)T} \bar{\mathbf{u}}_1 \mathbf{d}_1^T - \tau \mathbf{c}_5^T \bar{\mathbf{u}}_1 \mathbf{d}_1^T + \mathbf{z}^{(1)T} - \tau \mathbf{c}_6^T)^T[2:\min(l+m+1, n)], \\
    \hat{\boldsymbol{\alpha}} &= (\bar{\mathbf{w}}_1^T - \tau \bar{\mathbf{w}}_1^T - \tau \mathbf{f}^T + \bar{\mathbf{u}}_1^T Q - \tau \mathbf{c}_1^T + \bar{\mathbf{u}}_1^T K \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^T - \tau \mathbf{c}_2^T \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^T \\
    &\quad + \mathbf{x}^{(1)T} - \tau \mathbf{c}_4^T + \mathbf{y}^{(1)T} \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^T - \tau \mathbf{c}_5^T \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^T)^T \in \mathbb{R}^p, \\
    \hat{\boldsymbol{\beta}} &= (-\tau \bar{\mathbf{k}}^T + \bar{\mathbf{u}}_1^T K - \tau \mathbf{c}_2^T + \mathbf{y}^{(1)T} - \tau \mathbf{c}_5^T)^T \in \mathbb{R}^r.
\end{align*}

Noting that \( U^{(2)} = U - \mathbf{e}_1 \bar{\mathbf{u}}_1^T \) and \( \mathbf{e}_1^T A = \mathbf{d}_1^T + \bar{\mathbf{w}}_1^T S^T \), we have \( U^{(2)T} A = U^T A - \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^T S^T - \bar{\mathbf{u}}_1 \mathbf{d}_1^T \). Substituting this yields an alternative expression:
\begin{equation}\label{row_structure}
\tilde{C}[1, 2:n] = \tilde{\mathbf{d}}^T + \tilde{\boldsymbol{\alpha}}^T (S^T[:, 2:n]) + \tilde{\boldsymbol{\beta}}^T ((U^T A)[:, 2:n]),
\end{equation}
where
\begin{align*}
    \tilde{\mathbf{d}} &= \begin{bmatrix} \tilde{\mathbf{d}}_s \\ \mathbf{0} \end{bmatrix} \in \mathbb{R}^{n-1}, \quad \text{with} \\
    \tilde{\mathbf{d}}_s &= (\mathbf{d}_1^T - \tau \mathbf{d}_1^T - \tau \bar{\mathbf{d}}^T + \bar{\mathbf{u}}_1^T E - \tau \mathbf{c}_3^T E + \mathbf{z}^{(1)T} - \tau \mathbf{c}_6^T + \tau \bar{\mathbf{k}}^T \bar{\mathbf{u}}_1 \mathbf{d}_1^T)^T[2:\min(l+m+1, n)], \\
    \tilde{\boldsymbol{\alpha}} &= (\bar{\mathbf{w}}_1^T - \tau \bar{\mathbf{w}}_1^T - \tau \mathbf{f}^T + \bar{\mathbf{u}}_1^T Q - \tau \mathbf{c}_1^T + \mathbf{x}^{(1)T} - \tau \mathbf{c}_4^T + \tau \bar{\mathbf{k}}^T \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^T)^T \in \mathbb{R}^p, \\
    \tilde{\boldsymbol{\beta}} &= (-\tau \bar{\mathbf{k}}^T + \bar{\mathbf{u}}_1^T K - \tau \mathbf{c}_2^T + \mathbf{y}^{(1)T} - \tau \mathbf{c}_5^T)^T \in \mathbb{R}^r.
\end{align*}

This confirms that \( \tilde{C}[1, 2:n] \) is an HMBPSV related to \( A \), completing the proof of the lemma.
\end{proof}

\subsection{Main Theorem and its Proof}

Equipped with \cref{lemma:structure_preserve}, we now state and prove the main theorem concerning the structure of the QR factor matrix \( F \).

\begin{theorem}\label{thm:structure_preserve}
After applying the QR factorization to a banded-plus-semiseparable matrix \( A \) (as expressed in Eq. \eqref{express_A}) with lower semiseparable rank \( r \), upper semiseparable rank \( p \), lower bandwidth \( l \), and upper bandwidth \( m \), the resulting factor matrix \( F \) is also a banded-plus-semiseparable matrix. Specifically:
\begin{itemize}
    \item Its lower semiseparable part has rank \( r \).
    \item Its upper semiseparable part has rank \( r + p \).
    \item Its banded part has lower bandwidth \( l \) and upper bandwidth \( l + m \).
\end{itemize}
\end{theorem}

\begin{proof}
The QR factorization is computed by performing a sequence of \( n-1 \) Householder transformations, eliminating the subdiagonal entries of \( A \) column by column.

Let \( A^{(i)} \) denote the matrix after the \( i \)-th Householder transformation, with \( A^{(0)} = A \). Define \( A_i = A[i:n, i:n] \), \( U_i = U[i:n, :] \), \( V_i = V[i:n, :] \), \( S_i = S[i:n, :] \), and \( W_i = W[i:n, :] \). Let \( \bar{\mathbf{u}}_i = (u_i^{(1)}, \ldots, u_i^{(r)})^T \) and \( \bar{\mathbf{w}}_i = (w_i^{(1)}, \ldots, w_i^{(p)})^T \). Let \( \bar{S} = U^T A \in \mathbb{R}^{r \times n} \).

We prove by induction that after the \( j \)-th transformation (\( 0 \le j < n \)):
\begin{enumerate}
    \item The submatrix \( A^{(j)}[j+1:n, j+1:n] \) is an HMBPSM related to \( A_{j+1} \).
    \item The \( j \)-th row of the final factor \( F \), \( F[j, j+1:n] \), is an HMBPSV related to \( A_j \).
    \item The \( j \)-th column of \( F \) below the diagonal, \( F[j+1:n, j] \), has the form \( (U \bar{\mathbf{k}}_{j+1})[j+1:n] + \mathbf{b}_{j+1}[2:n+1-j] \), where \( \bar{\mathbf{k}}_{j+1} \in \mathbb{R}^r \) and \( \mathbf{b}_{j+1} \in \mathbb{R}^{n+1-j} \) is non-zero only in its first \( \min(l+1, n+1-j) \) entries.
\end{enumerate}

\textbf{Base Case (j=0):} Initially, \( A^{(0)} = A \) is trivially an HMBPSM (with \( Q, K, E, X, Y, Z = \mathbf{0} \)) related to \( A_1 = A \).

\textbf{Inductive Step:} Assume the induction hypothesis holds for \( j \). That is, \( A^{(j)}[j+1:n, j+1:n] \) is an HMBPSM related to \( A_{j+1} \), and for \( i = 1, \ldots, j \):
\begin{equation}
F[i+1:n, i] = (U \bar{\mathbf{k}}_{i+1})[i+1:n] + \mathbf{b}_{i+1}[2:n+1-i], \label{F_tril}
\end{equation}
\begin{equation}
F[i, i+1:n] = \tilde{\mathbf{d}}_{i+1} + (\tilde{\boldsymbol{\alpha}}_{i+1}^T S^T)[i+1:n] + (\tilde{\boldsymbol{\beta}}_{i+1}^T \bar{S})[i+1:n], \label{F_triu}
\end{equation}
with \( \tilde{\boldsymbol{\alpha}}_{i+1} \in \mathbb{R}^p \), \( \tilde{\boldsymbol{\beta}}_{i+1} \in \mathbb{R}^r \), and \( \tilde{\mathbf{d}}_{i+1} \in \mathbb{R}^{n-i} \) non-zero only in its first \( \min(l+m, n-i) \) entries.

Furthermore, assume:
\begin{equation}
\begin{aligned}
A^{(j)}[j+1:n, j+1:n] = & A_{j+1} + U_{j+1} Q_{j+1} S_{j+1}^T + U_{j+1} K_{j+1} U_{j+1}^T A_{j+1} \\
& + U_{j+1} E_{j+1} + X_{j+1} S_{j+1}^T + Y_{j+1} U_{j+1}^T A_{j+1} + Z_{j+1}, \label{After_HT}
\end{aligned}
\end{equation}
where the modification matrices \( Q_{j+1}, K_{j+1}, E_{j+1}, X_{j+1}, Y_{j+1}, Z_{j+1} \) possess the sparsity patterns specified in Definition 2.1.

If \( j < n-1 \), we now perform the \( (j+1) \)-th Householder transformation on this HMBPSM. Let \( \mathbf{y}_{j+1} \) be the corresponding Householder vector(unlike the form given in \eqref{y_k}, here \( \mathbf{y}_{j+1} \) is a vector of length $n-j$ applied to the corresponding submatrix, and we will follow this convention until the end of section \ref{sec:fastqr}). It can be expressed as \( \mathbf{y}_{j+1} = \mathbf{e}_{j+1} + U^{(j+2)} \bar{\mathbf{k}}_{j+2} + \mathbf{b}_{j+2} \), where \( \mathbf{e}_{j+1} \in \mathbb{R}^{n-j} \) is the first standard basis vector, \( U^{(j+2)} \in \mathbb{R}^{(n-j) \times r} \) satisfies \( U^{(j+2)}[1,:] = \mathbf{0} \) and \( U^{(j+2)}[2:n-j, :] = U[j+2:n, :] \), \( \bar{\mathbf{k}}_{j+2} \in \mathbb{R}^r \), and \( \mathbf{b}_{j+2} \in \mathbb{R}^{n-j} \) is non-zero only in its first \( \min(l+1, n-j) \) entries. This vector defines the \( (j+1) \)-th column of \( F \):
\begin{equation}
F[j+2:n, j+1] = (U \bar{\mathbf{k}}_{j+2})[j+2:n] + \mathbf{b}_{j+2}[2:n-j]. \label{F_tril_2}
\end{equation}

We now apply Lemma \cref{lemma:structure_preserve} to the HMBPSM \( C = A^{(j)}[j+1:n, j+1:n] \), which is related to \( A_{j+1} \). The Householder transformation \( (I - \tau_{j+1} \mathbf{y}_{j+1} \mathbf{y}_{j+1}^T) \) is applied to \( C \), here \(\tau_{j+1}\) is a coefficient found to satisfy the definition of a Householder transformation.

From the lemma, the resulting submatrix \( A^{(j+1)}[j+2:n, j+2:n] \) is an HMBPSM related to \( A_{j+2} \). Its structure is given by equations analogous to \eqref{submatrix_structure}, with updated modification matrices \( Q_{j+2}, K_{j+2}, E_{j+2}, X_{j+2}, Y_{j+2}, Z_{j+2} \), which retain the required sparsity patterns. This satisfies condition 1 for \( j+1 \).

Furthermore, the lemma states that \( A^{(j+1)}[j+1, j+2:n] \) is an HMBPSV related to \( A_{j+1} \). This row becomes \( F[j+1, j+2:n] \) in the final factor matrix. Following the derivation \eqref{row_structure} in the lemma, and using the relation
\begin{equation}\label{eq:relation_UA}
U_{j+1}^T A_{j+1}[:, 2:n-j] = (U^T A - \sum_{t=1}^{j} \bar{\mathbf{u}}_t \bar{\mathbf{w}}_t^T S^T)[:, j+2:n] - \sum_{t=\max(j-m+2,1)}^{j} \bar{\mathbf{u}}_t (B[t, j+2:n]),
\end{equation}
we can express this row in the form:
\begin{equation}
\begin{aligned}
F[j+1, j+2:n] = & \tilde{\mathbf{d}}_{j+2}^T + (\tilde{\boldsymbol{\alpha}}_{j+2}^T S^T)[j+2:n] + (\tilde{\boldsymbol{\beta}}_{j+2}^T \bar{S})[j+2:n], \label{F_triu_2}
\end{aligned}
\end{equation}

where \( \tilde{\mathbf{d}}_{j+2} \) only nonzero in the first $\min(l+m, n-j-1)$ entries, \( \tilde{\boldsymbol{\alpha}}_{j+2}\in \mathbb{R}^p\), and \( \tilde{\boldsymbol{\beta}}_{j+2}\in \mathbb{R}^r \). This satisfies condition 2 for \( j+1 \). Condition 3 for \( j+1 \) is already established by \eqref{F_tril_2}.

By the principle of induction, the hypotheses hold for all \( j = 0, \ldots, n-1 \).

Upon completion of all \( n-1 \) transformations, the factor matrix \( F \) is fully determined. Aggregating the results from \eqref{F_tril} and \eqref{F_triu}, we conclude that \( F \) can be written in the form:
\begin{equation}
F = B_F + \tril(U \bar{K}^T, -1) + \triu([\bar{A},\bar{B}] [S,\bar{S}^T]^T, 1), \label{express_F}
\end{equation}
where
\begin{itemize}
    \item \( \bar{K} \in \mathbb{R}^{n \times r} \) is defined by \( \bar{K}[i,:] = \bar{\mathbf{k}}_{i+1}^T \) for \( i=1,\ldots,n-1 \) and \( \bar{K}[n,:] = \mathbf{0} \).
    \item \( \bar{A} \in \mathbb{R}^{n \times p} \) is defined by \( \bar{A}[i,:] = \boldsymbol{\alpha}_{i+1}^T \) for \( i=1,\ldots,n-1 \) and \( \bar{A}[n,:] = \mathbf{0} \).
    \item \( \bar{B} \in \mathbb{R}^{n \times r} \) is defined by \( \bar{B}[i,:] = \boldsymbol{\beta}_{i+1}^T \) for \( i=1,\ldots,n-1 \) and \( \bar{B}[n,:] = \mathbf{0} \).
    \item \( B_F \) is a banded matrix with lower bandwidth \( l \) and upper bandwidth \( l+m \), defined by:
    \[
    B_F[i,j] =
    \begin{cases}
        A^{(i)}[i,i], & i = j < n \\
        A^{(n-1)}[n,n], & i = j = n \\
        \mathbf{b}_{j+1}[i-j+1], & 0 < i - j \le l \\
        \tilde{\mathbf{d}}_{i+1}[j-i], & 0 < j - i \le l + m \\
        0, & \text{otherwise}.
    \end{cases}
    \]
\end{itemize}

The representation in \eqref{express_F} explicitly shows that \( F \) is a banded-plus-semiseparable matrix with a lower semiseparable rank of \( r \), an upper semiseparable rank of \( r+p \), a lower bandwidth of \( l \), and an upper bandwidth of \( l+m \). This completes the proof.
\end{proof}



\section{Main algorithms}
\label{sec:algorithms}

\subsection{Fast QR factorization for BPS Matrices}
\label{sec:fastqr}

Based on the structure-preserving theorem proven in Section~\ref{sec:main}, we now present the detailed $O(n)$ algorithm for computing the QR factorization of a banded-plus-semiseparable matrix. The algorithm exploits the proven fact that the factor matrix $F$ maintains a BPS structure.

\begin{algorithm}[H]
\caption{Fast QR Factorization for BPS Matrices}
\label{algo:fastqr}
\begin{algorithmic}[1]
\State \textbf{Input} $B \in \mathbb{R}^{n \times n}$: banded matrix with lower bandwidth $\ell$ upper bandwidth $m$;
 $U, V \in \mathbb{R}^{n \times r}$: generators for lower semiseparable part (rank $r$);
 $W, S \in \mathbb{R}^{n \times p}$: generators for upper semiseparable part (rank $p$)
\State \textbf{Output} $B_F$: banded part of factor matrix $F$ with bandwidths $\ell$ and $\ell+m$;
$\bar{K} \in \mathbb{R}^{n \times r}$: generator for lower semiseparable part of $F$;
$\bar{A} \in \mathbb{R}^{n \times p}, \bar{B} \in \mathbb{R}^{n \times r}$: generators for upper semiseparable part of $F$;
$\boldsymbol{\tau} \in \mathbb{R}^{n}$: Householder scaling coefficients.

\State \textbf{Initialization:}
\State Compute $\bar{S} \gets U^\top A$ (in $O(n)$ utilizing the semiseparable structure of $A$)
\State Initialize $B_F \gets \mathbf{0}_{n \times n}$, $\bar{K} \gets \mathbf{0}_{n \times r}$, $\bar{A} \gets \mathbf{0}_{n \times p}$, $\bar{B} \gets \mathbf{0}_{n \times r}$
\State Initialize HMBPSM modification matrices: $Q \gets \mathbf{0}_{r \times p}$, $K \gets \mathbf{0}_{r \times r}$, $E_s \gets \mathbf{0}_{r \times \min(\ell+m,n)}$, $X_s \gets \mathbf{0}_{\min(\ell,n) \times p}$, $Y_s \gets \mathbf{0}_{\min(\ell,n) \times r}$, $Z_s \gets \mathbf{0}_{\min(\ell,n) \times \min(\ell+m,n)}$

\For{$k = 1$ to $n-1$}
    \State \textbf{Step 1: Form Householder vector $\mathbf{y}_k$}
    \State Run $ \text{FormHouseholderVector}(A^{(k-1)}[k:n, k:n], U[k:n, :], \ell)$ (Algorithm \ref{algo:form_householder}) to get:
    \State $\bar{\mathbf{k}}_{k+1} \gets \text{low-rank part from } \mathbf{y}_k$
    \State $\mathbf{b}_{k+1} \gets \text{banded part from } \mathbf{y}_k$
    \State $\tau_k \gets \text{scaling coefficient}$
    \State $o \gets \text{the diagonal element after the Householder transformation}$ 
    \State $\boldsymbol{\tau}[k] \gets \tau_k$
    
    \State \textbf{Step 2: Store $k$-th column of $F$}
    \State $B_F[k, k] \gets o$ (Diagonal entry)
    \State $\bar{K}[k, :] \gets \bar{\mathbf{k}}_{k+1}^\top$ (Store low-rank generator)
    \For{$j = k+1$ to $\min(k+\ell, n)$}
        \State $B_F[j, k] \gets \mathbf{b}_{k+1}[j-k+1]$ (Store banded part)
    \EndFor
    
    \State \textbf{Step 3: Compute and store $k$-th row of $F$}
    \State $[\tilde{\boldsymbol{\alpha}}_{k+1}, \tilde{\boldsymbol{\beta}}_{k+1}, \tilde{\mathbf{d}}_{k+1}] \gets \text{ComputeRowUpdate}(A^{(k-1)}, U, S, \bar{S}, \mathbf{k}_{k+1}, \mathbf{b}_{k+1}, \tau_k)$ (Algorithm \ref{algo:compute_row})
    \State $\bar{A}[k, :] \gets \tilde{\boldsymbol{\alpha}}_{k+1}^\top$
    \State $\bar{B}[k, :] \gets \tilde{\boldsymbol{\beta}}_{k+1}^\top$
    \For{$j = k+1$ to $\min(k+\ell+m, n)$}
        \State $B_F[k, j] \gets \tilde{\mathbf{d}}_{k+1}[j-k]$ (Store upper banded part)
    \EndFor
    
    \State \textbf{Step 4: Update HMBPSM matrices}
    \State $[Q, K, E_s, X_s, Y_s, Z_s] \gets \text{UpdateHMBPSM}(Q, K, E_s, X_s, Y_s, Z_s, \bar{\mathbf{k}}_{k+1}, \mathbf{b}_{k+1}, \tau_k)$ (Algorithm \ref{algo:update_hmbpsm})
\EndFor

\State \textbf{Final step:}
\State $B_F[n, n] \gets A^{(n-1)}[n, n]$
\State \Return $B_F, \bar{K}, \bar{A}, \bar{B}, \boldsymbol{\tau}$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{FormHouseholderVector (Step 1 of Algorithm \ref{algo:fastqr})}
\label{algo:form_householder}
\begin{algorithmic}[1]
\State \textbf{Input} $C \in \mathbb{R}^{(n-k+1) \times (n-k+1)}$: current HMBPSM submatrix;
$U_k \in \mathbb{R}^{(n-k+1) \times r}$: $U[k:n, :]$;
$\ell$: lower bandwidth.
\State \textbf{Output} $\bar{\mathbf{k}} \in \mathbb{R}^r$, $\mathbf{b} \in \mathbb{R}^{n-k+1}$, $\tau\in \mathbb{R}$ s.t. $\mathbf{y}$: = $\mathbf{e}_1 + U_k^{(2)}\bar{\mathbf{k}} + \mathbf{b}$ is the Householder vector and $I-\tau \mathbf{y}\mathbf{y}^T$ is the Householder transformation; $o\in \mathbb{R}$ s.t. $\tilde{C}[1,1]=o$ where $\tilde{C}$ is the submatrix after the Householder transformation.

\State Extract first column: $\mathbf{a} \gets C[:, 1]$
\State $\bar{\mathbf{k}} \gets U_k^{\top}$(the semiseparable part in $\mathbf{a}$)
\State $\mathbf{b} \gets \mathbf{a} - U_k^{(2)}\bar{\mathbf{k}} - \mathbf{e}_1 \cdot \mathbf{a}[1]$
\State $\mathbf{b}$ is only nonzero in entries $2$ to $\min(\ell+1, n-k+1)$.
\State $\tau \gets 2/\mathbf{y}^T\mathbf{y}$
\State $o \gets -\text{sign}(\textbf{a}[1])\mathbf{a}^T\mathbf{a}$
\State \Return $\bar{\mathbf{k}}$, $\mathbf{b}$, $\tau$, $o$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{ComputeRowUpdate (Step 3 of Algorithm \ref{algo:fastqr})}
\label{algo:compute_row}
\begin{algorithmic}[1]
\State \textbf{Input} $C$: current HMBPSM represented by modification matrices;
 $U, S, \bar{S}$: generator matrices;
$\mathbf{k}_{k+1}$, $\mathbf{b}_{k+1}$, $\tau_k$: Householder vector and coefficient.
\State \textbf{Output} $\tilde{\boldsymbol{\alpha}} \in \mathbb{R}^p$, $\tilde{\boldsymbol{\beta}} \in \mathbb{R}^r$, $\tilde{\mathbf{d}} \in \mathbb{R}^{n-k}$

\State Compute auxiliary vectors $\mathbf{c}_1, \dots, \mathbf{c}_6$ as in Lemma~\ref{lemma:structure_preserve} proof
\State $\tilde{\boldsymbol{\alpha}} \gets \bar{\mathbf{w}}_k - \tau_k \bar{\mathbf{w}}_k - \tau_k \mathbf{f} + \bar{\mathbf{u}}_k^\top Q - \tau_k \mathbf{c}_1 + \cdots$, full expression from proof
\State $\tilde{\boldsymbol{\beta}} \gets -\tau_k \bar{\mathbf{k}}_{k+1} + K^\top \bar{\mathbf{u}}_k - \tau_k \mathbf{c}_2 + \cdots$,
\State $\tilde{\mathbf{d}} \gets \text{extract banded part from proof, length } \min(\ell+m, n-k)$
\State \Return $\tilde{\boldsymbol{\alpha}}, \tilde{\boldsymbol{\beta}}, \tilde{\mathbf{d}}$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{UpdateHMBPSM (Step 4 of Algorithm \ref{algo:fastqr})}
\label{algo:update_hmbpsm}
\begin{algorithmic}[1]
\State \textbf{Input} $Q, K, E_s, X_s, Y_s, Z_s$: current HMBPSM modification matrices;
$\bar{\mathbf{k}}_{k+1}$, $\mathbf{b}_{k+1}$, $\tau_k$.
\Ensure Updated $Q, K, E_s, X_s, Y_s, Z_s$

\State Compute updates using formulas from Lemma~\ref{lemma:structure_preserve} proof:
\State $Q_{\text{new}} \gets Q - \tau_k \bar{\mathbf{k}}_{k+1} \bar{\mathbf{w}}_k^\top - \tau_k \bar{\mathbf{k}}_{k+1} \mathbf{f}^\top - \tau_k \bar{\mathbf{k}}_{k+1} \mathbf{c}_1^\top + \cdots$
\State $K_{\text{new}} \gets K - \tau_k \bar{\mathbf{k}}_{k+1} \bar{\mathbf{k}}_{k+1}^\top - \tau_k \bar{\mathbf{k}}_{k+1} \mathbf{c}_2^\top + \cdots$
\State Update $E_s, X_s, Y_s, Z_s$ similarly with banded truncations
\State \Return $Q_{\text{new}}, K_{\text{new}}, E_s^{\text{new}}, X_s^{\text{new}}, Y_s^{\text{new}}, Z_s^{\text{new}}$
\end{algorithmic}
\end{algorithm}

\subsubsection*{Complexity Analysis}

The algorithm runs for $n-1$ steps. The cost per step can be expressed as a polynomial in term of $r$, $p$, $l$, and $m$. Since these are constants independent of $n$, the total complexity is $O(n)$. The memory footprint is also $O(n)$, as we store only the generators and banded components.

\begin{remark}

To maintain the $O(1)$ per-step complexity in Algorithm 3.1, two key quantities must be computed efficiently during the Householder updates:

\begin{itemize}
    \item \textbf{Inner product matrix $U_k^T U_k$}: The computation of intermediate vectors $\mathbf{c}_1, \dots, \mathbf{c}_6$ requires evaluating expressions like $U_k^T \mathbf{y}_k = U_k^T(\mathbf{e}_k + U^{(k+1)}\bar{\mathbf{k}}_{k+1} + \mathbf{b}_{k+1})$, which involves $U_k^T U^{(k+1)}$ that is equal to $U_{k+1}^TU_{k+1}$. we precompute a lookup table:
    \[
    \text{UU\_lookup}[k] = U[k:n,:]^T U[k:n,:] \quad \text{for } k = 1,\dots,n
    \]
    This can be computed in $O(nr^2)$ time via a backward accumulation.

    \item \textbf{Partial sum $\sum_{t=1}^{j} \bar{\mathbf{u}}_t \bar{\mathbf{w}}_t^T$}: The update of the upper triangular part in equation \eqref{eq:relation_UA} requires this sum. We precompute:
    \[
    \text{UV\_lookup}[j] = \sum_{t=1}^{j} \bar{\mathbf{u}}_t \bar{\mathbf{w}}_t^T \quad \text{for } j = 1,\dots,n-1
    \]
    This is computed in $O(nrp)$ time via forward accumulation.
\end{itemize}
Both precomputations require $O(n)$ total time and enable $O(1)$ access to the required quantities at each step of the factorization, thus preserving the overall $O(n)$ complexity.
\end{remark}

\subsection{Fast Solver for BPS Matrices}
\label{sec:solver}

Theorem \ref{thm:structure_preserve} not only enables an efficient QR factorization but also facilitates a complete direct solver for linear systems of the form \( A\mathbf{x} = \mathbf{b} \), where \( A \) is a banded-plus-semiseparable matrix. The solver consists of two phases after the QR factorization \( A = QR \):
1. Application of \( Q^T \) to the right-hand side vector \( \mathbf{b} \) to form \( \mathbf{c} = Q^T \mathbf{b} \).
2. Solution of the upper triangular system \( R\mathbf{x} = \mathbf{c} \) via backward substitution.

We now present \( O(n) \) algorithms for both phases, leveraging the structured representation of the factorization output by Algorithm \ref{algo:fastqr}.

\subsubsection{Fast Application of \( Q^T \)}

The orthogonal matrix \( Q \) is represented as a product of Householder transformations:
\[
Q = (I - \tau_1 \mathbf{y}_1 \mathbf{y}_1^T)(I - \tau_2 \mathbf{y}_2 \mathbf{y}_2^T) \cdots (I - \tau_{n-1} \mathbf{y}_{n-1} \mathbf{y}_{n-1}^T).
\]
Applying \( Q^T \) to a vector \( \mathbf{b} \) thus requires computing:
\[
Q^T\mathbf{b} = (I - \tau_{n-1} \mathbf{y}_{n-1} \mathbf{y}_{n-1}^T) \cdots (I - \tau_2 \mathbf{y}_2 \mathbf{y}_2^T)(I - \tau_1 \mathbf{y}_1 \mathbf{y}_1^T)\mathbf{b}.
\]

The Householder vectors \( \mathbf{y}_k \) are stored in the factor matrix \( F \) according to the normalization convention established in Section~\ref{sec:main}:
\[
\mathbf{y}_k[j] = 
\begin{cases}
0, & j < k \\
1, & j = k \\
F[j,k], & j > k
\end{cases}
\quad \text{for } k = 1, \ldots, n-1.
\]

From Theorem \ref{thm:structure_preserve}, the factor matrix \( F \) admits the BPS representation:
\begin{equation}
F = B_F + \tril(U_F V_F^T, -1) + \triu(W_F S_F^T, 1),
\label{eq:F_structure_solver}
\end{equation}
where \( U_F, V_F \in \mathbb{R}^{n \times r} \), \( W_F, S_F \in \mathbb{R}^{n \times (r+p)} \), and \( B_F \) is banded with lower bandwidth \( l \) and upper bandwidth \( l+m \).

This structure implies that each Householder vector \( \mathbf{y}_k \) can be expressed as:
\begin{equation}
\mathbf{y}_k = \bar{\mathbf{e}}_k + U_F^{(k+1)} \bar{\mathbf{v}}_k + \mathbf{d}_k,
\label{eq:y_structure}
\end{equation}
where:
\begin{itemize}
    \item \( \bar{\mathbf{e}}_k \in \mathbb{R}^n \) is the \( k \)-th standard basis vector,
    \item \( U_F^{(k+1)} \in \mathbb{R}^{n \times r} \) satisfies \( U_F^{(k+1)}[1:k,:] = \mathbf{0} \) and \( U_F^{(k+1)}[k+1:n,:] = U_F[k+1:n,:] \),
    \item \( \bar{\mathbf{v}}_k = V_F[k,:]^T \in \mathbb{R}^r \),
    \item \( \mathbf{d}_k \in \mathbb{R}^n \) is non-zero only in positions \( k+1 \) to \( \min(k+l, n) \), with \( \mathbf{d}_k[j] = B_F[j,k] \) for \( j = k+1, \ldots, \min(k+l, n) \).
\end{itemize}

Algorithm~\ref{algo_applyQ} exploits this structure to compute \( Q^T\mathbf{b} \) in \( O(n) \) operations by maintaining a compressed representation of the intermediate vectors throughout the transformation process.

\begin{algorithm}
\caption{Fast Application of \( Q^T \) to a Vector}\label{algo_applyQ}
\begin{algorithmic}[1]
\State \textbf{Input:} Factor matrix \( F \) in BPS form: \( F = B_F + \tril(U_F V_F^T, -1) + \triu(W_F S_F^T, 1) \); coefficient vector \( \boldsymbol{\tau} = [\tau_1, \ldots, \tau_{n-1},0]^T \in \mathbb{R}^{n} \); right-hand side vector \( \mathbf{b} \in \mathbb{R}^n \)
\State \textbf{Output:} \( \mathbf{c} = Q^T\mathbf{b} \in \mathbb{R}^n \)

\State Initialize:
\begin{itemize}
    \item \( O \gets \mathbf{0}_{n \times r} \): Storage for accumulated low-rank updates
    \item \( G \gets \mathbf{0}_{n \times (l+1)} \): Storage for banded component updates
    \item \( \mathbf{h} \gets \mathbf{0}_r \): Accumulator for semiseparable component
    \item Let \( \mathbf{o}_i \) denote the \( i \)-th column of \( O \)
    \item Let \( \mathbf{g}_i \) denote the \( i \)-th column of \( G \)
\end{itemize}

\State Express initial vector: \( \mathbf{b}^{(0)} = \mathbf{b} + U_F^{(1)} \mathbf{h} + \sum_{i=1}^r \mathbf{o}_i + \sum_{i=1}^{l+1} \mathbf{g}_i \)

\For{\( k = 1 \) to \( n-1 \)}
    \State Compute inner product: \( c \gets \mathbf{y}_k^T \mathbf{b}^{(k-1)} \) (exploit BPS structure of \( \mathbf{y}_k \) and precompute some lookup tables for for \( O(1) \) computation)
    
    \State Update low-rank storage: \( O[k,:] \gets U_F[k,:] \odot \mathbf{h}^T \) (element-wise multiplication)
    
    \State Update semiseparable accumulator: \( \mathbf{h} \gets \mathbf{h} - \tau_k c \cdot V_F[k,:]^T \)
    
    \State Update banded component:
    \State \( G[k,1] \gets -\tau_k c \) (diagonal contribution)
    \For{\( t = 1 \) to \( \min(l, n-k) \)}
        \State \( G[k+t,t+1] \gets -\tau_k c \cdot B_F[k+t,k] \) (subdiagonal contributions)
    \EndFor
    
    \State Current representation: \( \mathbf{b}^{(k)} = \mathbf{b} + U_F^{(k+1)} \mathbf{h} + \sum_{i=1}^r \mathbf{o}_i + \sum_{i=1}^{l+1} \mathbf{g}_i \)
\EndFor

\State Compute final result explicitly: \( \mathbf{c} \gets \mathbf{b} + U_F^{(n)} \mathbf{h} + \sum_{i=1}^r \mathbf{o}_i + \sum_{i=1}^{l+1} \mathbf{g}_i \)

\State \Return \( \mathbf{c} \)
\end{algorithmic}
\end{algorithm}

\begin{theorem}
Algorithm~\ref{algo_applyQ} correctly computes \( \mathbf{c} = Q^T\mathbf{b} \) in \( O(n) \) operations.
\end{theorem}

\begin{proof}
The proof proceeds by induction on the transformation steps. Let \( \mathbf{b}^{(0)} = \mathbf{b} \) and assume that after \( k-1 \) steps, the algorithm maintains the representation:
\[
\mathbf{b}^{(k-1)} = \mathbf{b} + U_F^{(k)} \mathbf{h}^{(k-1)} + \sum_{i=1}^r \mathbf{o}_i^{(k-1)} + \sum_{i=1}^{l+1} \mathbf{g}_i^{(k-1)},
\]
where the superscripts on $\mathbf{h}$, $\mathbf{o}$, and $\mathbf{g}$ denote the state after the \( (k-1) \)-th iteration.

The \( k \)-th Householder transformation gives:
\[
\mathbf{b}^{(k)} = (I - \tau_k \mathbf{y}_k \mathbf{y}_k^T) \mathbf{b}^{(k-1)} = \mathbf{b}^{(k-1)} - \tau_k (\mathbf{y}_k^T \mathbf{b}^{(k-1)}) \mathbf{y}_k.
\]

Substituting the structured form of \( \mathbf{y}_k \) from \eqref{eq:y_structure} and the inductive representation:
\begin{align*}
\mathbf{b}^{(k)} &= \mathbf{b} + U_F^{(k)} \mathbf{h}^{(k-1)} + \sum_{i=1}^r \mathbf{o}_i^{(k-1)} + \sum_{i=1}^{l+1} \mathbf{g}_i^{(k-1)} \\
&\quad - \tau_k c (\bar{\mathbf{e}}_k + U_F^{(k+1)} \bar{\mathbf{v}}_k + \mathbf{d}_k) \\
&= \mathbf{b} + U_F^{(k+1)} (\mathbf{h}^{(k-1)} - \tau_k c \bar{\mathbf{v}}_k) \\
&\quad + \left( \sum_{i=1}^r \mathbf{o}_i^{(k-1)} + (U_F^{(k)} - U_F^{(k+1)}) \mathbf{h}^{(k-1)} \right) \\
&\quad + \left( \mathbf{g}_1^{(k-1)} - \tau_k c \bar{\mathbf{e}}_k \right) + \left( \sum_{i=2}^{l+1} \mathbf{g}_i^{(k-1)} - \tau_k c \mathbf{d}_k \right).
\end{align*}

The algorithm updates precisely these components:
\begin{itemize}
    \item \( \mathbf{h}^{(k)} = \mathbf{h}^{(k-1)} - \tau_k c \bar{\mathbf{v}}_k \),
    \item \( O[k,:] = U_F[k,:] \odot \mathbf{h}^{(k-1)T} \) captures \( (U_F^{(k)} - U_F^{(k+1)}) \mathbf{h}^{(k-1)} \),
    \item Banded updates in \( G \) capture the remaining terms.
\end{itemize}

Thus, the representation is maintained correctly throughout all \( n-1 \) steps. Each step requires \( O(1) \) operations due to the constant-bounded parameters \( r, p, l, m \), yielding overall \( O(n) \) complexity.
\end{proof}

\subsubsection{Fast Backward Substitution}

After computing \( \mathbf{c} = Q^T\mathbf{b} \), we solve the upper triangular system \( R\mathbf{x} = \mathbf{c} \), where \( R = \triu(F) \) inherits the BPS structure of \( F \). Specifically, the upper triangular part of \( F \) satisfies:
\[
R = B_R + \triu(W_F S_F^T, 1),
\]
where \( B_R = \triu(B_F) \) is the upper triangular part of the banded component, maintaining upper bandwidth \( l+m \).

Algorithm~\ref{algo_backsub}, which is equivalent to the one introduced in \cite{olver2013fast}, exploits this structure to perform backward substitution in \( O(n) \) operations by maintaining a running sum for the semiseparable contributions.

\begin{algorithm}
\caption{Fast Backward Substitution for Structured \( R \)}\label{algo_backsub}
\begin{algorithmic}[1]
\State \textbf{Input:} Upper triangular matrix \( R = \triu(F) \) in structured form; transformed right-hand side \( \mathbf{c} \in \mathbb{R}^n \)
\State \textbf{Output:} Solution \( \mathbf{x} \in \mathbb{R}^n \) satisfying \( R\mathbf{x} = \mathbf{c} \)

\State Initialize:
\begin{itemize}
    \item \( \mathbf{x} \gets \mathbf{0}_n \): solution vector
    \item \( \mathbf{s} \gets \mathbf{0}_{r+p} \): Accumulator for semiseparable contributions
\end{itemize}

\For{\( j = n \) down to \( 1 \)}
    \State Initialize residual: \( \text{res} \gets 0 \)
    
    \State Add semiseparable contribution: \( \text{res} \gets \text{res} + W_F[j,:] \cdot \mathbf{s} \)
    
    \State Add banded contributions:
    \For{\( k = j+1 \) to \( \min(j+l+m, n) \)}
        \State \( \text{res} \gets \text{res} + B_R[j,k] \cdot \mathbf{x}[k] \)
    \EndFor
    
    \State Solve for \( x_j \): \( \mathbf{x}[j] \gets (\mathbf{c}[j] - \text{res}) / B_R[j,j] \)
    
    \State Update semiseparable accumulator: \( \mathbf{s} \gets \mathbf{s} + S_F[j,:]^T \cdot \mathbf{x}[j] \)
\EndFor

\State \Return \( \mathbf{x} \)
\end{algorithmic}
\end{algorithm}

\begin{theorem}
Algorithm~\ref{algo_backsub}  solves \( R\mathbf{x} = \mathbf{c} \) in \( O(n) \) operations.
\end{theorem}

\begin{proof}
For completeness we include the proof from \cite{olver2013fast}. The algorithm implements standard backward substitution while exploiting the structure of \( R \). For each index \( j \) from \( n \) down to \( 1 \), the equation:
\[
R[j,j] x_j + \sum_{k=j+1}^n R[j,k] x_k = c_j
\]
is solved for \( x_j \).

The key insight is that the off-diagonal entries \( R[j,k] \) for \( k > j \) can be decomposed as:
\[
R[j,k] = B_R[j,k] + W_F[j,:] \cdot S_F[k,:]^T.
\]

The banded contributions \( B_R[j,k] \) are non-zero only for \( k = j+1, \ldots, \min(j+l+m, n) \), requiring \( O(1) \) operations per row. The semiseparable contributions are accumulated in the vector \( \mathbf{s} \), which stores:
\[
\mathbf{s} = \sum_{i=j+1}^n S_F[i,:]^T x_i.
\]

At step \( j \), the product \( W_F[j,:] ^\top \mathbf{s} \) thus captures all semiseparable contributions from previously computed solution components. After computing \( x_j \), the accumulator is updated to include its contribution.

Each iteration requires \( O(1) \) operations, yielding overall \( O(n) \) complexity. The correctness follows by induction from \( j = n \) down to \( 1 \).
\end{proof}

\subsubsection{Overall Solver Complexity}

Combining the QR factorization (Algorithm \ref{algo:fastqr}), the fast application of \( Q^T \) (Algorithm~\ref{algo_applyQ}), and the fast backward substitution (Algorithm~\ref{algo_backsub}) yields a complete direct solver for BPS linear systems with \( O(n) \) complexity.

\begin{corollary}
For a banded-plus-semiseparable matrix \( A \in \mathbb{R}^{n \times n} \) with constant-bounded ranks and bandwidths, the linear system \( A\mathbf{x} = \mathbf{b} \) can be solved in \( O(n) \) operations using the QR-based approach.
\end{corollary}

\begin{proof}
Algorithm \ref{algo:fastqr} computes the QR factorization in \( O(n) \) operations. Algorithm~\ref{algo_applyQ} applies \( Q^T \) in \( O(n) \) operations. Algorithm~\ref{algo_backsub} solves the triangular system in \( O(n) \) operations. The overall complexity is therefore linear in the problem size \( n \).
\end{proof}

\section{Fast RQ Computation for Symmetric BPS Matrices}
\label{sec:fastrq}

The fast QR factorization developed in the previous section not only provides a direct linear system solver but also forms the foundation for iterative algorithms such as the QR algorithm for computing eigenvalues. A core step in the QR iteration is the formation of the $RQ$ product. For symmetric banded-plus-semiseparable (BPS) matrices, we show that the $RQ$ product also preserves the BPS structure, leading to the design of a linear-complexity algorithm for its fast computation.

\subsection{Structure, Definitions, and a Key Lemma for the Symmetric Case}

Consider a symmetric BPS matrix $A$ of the form
\begin{equation}\label{express_A_symm}
    A = B + \tril(UV^\top, -1) + \triu(VU^\top, 1),
\end{equation}
where $B$ is a symmetric banded matrix with lower and upper bandwidth $l$, and $U, V \in \mathbb{R}^{n \times r}$ generate the lower and upper semiseparable parts of rank $r$. Let $A=QR$ be its QR factorization. Since $A$ is symmetric, we have $RQ = Q^\top A Q$, and thus $RQ$ is also symmetric.

To describe the structure of intermediate matrices in the computation of $RQ$, we introduce definitions analogous to those in Section 2 but tailored for the symmetric case and the $R$ factor.

\begin{definition}[Householder-modified upper-triangular matrix (HMUTM)]\label{def:hmutm}
Given an $n\times n$ upper triangular matrix $R$ and an $n\times r$ matrix $U$, a matrix $\Gamma$ is called a Householder-modified upper-triangular matrix (HMUTM) of $R$ under $U$ if
\begin{equation}
    \Gamma = R + RU\Omega U^\top + \Phi U^\top + RU\Psi + \Lambda,
\end{equation}
where $\Omega \in \mathbb{R}^{r\times r}$; $\Phi=\begin{bmatrix} \Phi_s \\ \mathbf{0} \end{bmatrix}\in \mathbb{R}^{n\times r}$ with $\Phi_s\in \mathbb{R}^{\min(l,n)\times r}$; $\Psi=[\Psi_s, \mathbf{0}]\in \mathbb{R}^{r \times n}$ with $\Psi_s\in \mathbb{R}^{r\times \min(l,n)}$; and $\Lambda=\begin{bmatrix} \Lambda_s & \mathbf{0} \\ \mathbf{0} & \mathbf{0} \end{bmatrix}\in \mathbb{R}^{n \times n}$ with $\Lambda_s \in \mathbb{R}^{\min(l,n)\times \min(l,n)}$.
\end{definition}

\begin{definition}[Householder-modified upper-triangular vector (HMUTV)]\label{def:hmutv}
Given an $n\times n$ upper triangular matrix $R$ and an $n\times r$ matrix $U$, a vector $\boldsymbol{\sigma}$ of length $n-1$ is called a Householder-modified upper-triangular vector (HMUTV) of $R$ under $U$ if
\begin{equation}
    \boldsymbol{\sigma} = \boldsymbol{\eta} + (RU\boldsymbol{\mu})[2:n],
\end{equation}
for some $\boldsymbol{\eta}=\begin{bmatrix} \boldsymbol{\eta}_s \\ \mathbf{0} \end{bmatrix} \in \mathbb{R}^{n-1}$ with $\boldsymbol{\eta}_s\in \mathbb{R}^{\min(l,n)}$ and $\boldsymbol{\mu}\in \mathbb{R}^r$.
\end{definition}

These definitions characterize the structured perturbations that appear in the $R$ factor when it is right-multiplied by a sequence of Householder reflectors (i.e., by $Q$). The following lemma is the symmetric counterpart to Lemma \ref{lemma:structure_preserve} and is the engine of the inductive proof.

\begin{lemma}[Structure preservation under right-multiplication by a Householder reflector]\label{lemma:structure_preserve_rq}
Given an $n\times n$ upper triangular matrix $R$, an $n\times r$ matrix $U$, and an HMUTM $\Gamma$ of $R$ under $U$, consider its right-multiplication by a Householder reflector: $\widetilde{\Gamma} = \Gamma (I - \tau \mathbf{y} \mathbf{y}^\top)$. Assume the Householder vector has the form $\mathbf{y} = \mathbf{e}_1 + U^{(2)}\bar{\mathbf{k}} + \mathbf{b}$, where $\mathbf{e}_1$ is the first standard basis vector, $U^{(2)}\in\mathbb{R}^{n\times r}$ satisfies $U^{(2)}[1,:]=\mathbf{0}$ and $U^{(2)}[2:n,:]=U[2:n,:]$, $\bar{\mathbf{k}}\in\mathbb{R}^r$, and $\mathbf{b}\in\mathbb{R}^{n}$ is nonzero only in its entries $2$ through $\min(l+1, n)$,. Then the following hold:
\begin{enumerate}
    \item The submatrix $\widetilde{\Gamma}[2:n, 2:n]$ is an HMUTM of $R[2:n,2:n]$ under $U[2:n,:]$.
    \item The vector $\widetilde{\Gamma}[2:n, 1]$ is an HMUTV of $R$ under $U$.
\end{enumerate}
\end{lemma}

\begin{proof}
Let $U = (\mathbf{u}_1,\ldots,\mathbf{u}_r)$ where $\mathbf{u}_i = (u_1^{(i)},\ldots,u_n^{(i)})^\top$, and denote $\bar{\mathbf{u}}_1 = (u_1^{(1)},\ldots,u_1^{(r)})^\top \in \mathbb{R}^r$. Write $\Gamma = R + RU\Omega U^\top + \Phi U^\top + RU\Psi + \Lambda$ with matrices $\Omega, \Phi, \Psi, \Lambda$ having the sparsity patterns specified in Definition \ref{def:hmutm}.

Define the auxiliary vectors:
\begin{align*}
    \boldsymbol{\delta}_1 &= \Omega U^\top \mathbf{y} \in \mathbb{R}^r, &
    \boldsymbol{\delta}_2 &= U^\top \mathbf{y} \in \mathbb{R}^r, \\
    \boldsymbol{\delta}_3 &= \Psi \mathbf{y} \in \mathbb{R}^r, &
    \boldsymbol{\delta}_4 &= \Lambda \mathbf{y} \in \mathbb{R}^n.
\end{align*}
Note that $\boldsymbol{\delta}_4$ has the form $\boldsymbol{\delta}_4 = \begin{bmatrix} \boldsymbol{\delta}_{4s} \\ \mathbf{0} \end{bmatrix}$ with $\boldsymbol{\delta}_{4s} \in \mathbb{R}^{\min(l,n)}$ due to the structure of $\Lambda$.

Also let $\mathbf{r}^{(1)} = R[1,:]^\top$, $\boldsymbol{\phi}^{(1)} = \Phi[1,:]^\top$, $\boldsymbol{\psi}^{(1)} = \Psi[:,1]$, and $\boldsymbol{\lambda}^{(1)} = \Lambda[:,1]$, and $\boldsymbol{\gamma} = Rb$.

We compute $\widetilde{\Gamma} = \Gamma - \tau \Gamma \mathbf{y} \mathbf{y}^\top$ by distributing the operation over each term in $\Gamma$.

\textbf{(i) Transformation of $R$:} Writing $\mathbf{y} = \mathbf{e}_1 + U^{(2)}\bar{\mathbf{k}} + \mathbf{b}$ and $R\mathbf{b} = \boldsymbol{\gamma}$ where $\boldsymbol{\gamma}$ is nonzero only in its first $\min(l+1,n)$ entries, we obtain:
\begin{equation}\label{eq:r_transform}
\begin{aligned}
R(I - \tau \mathbf{y}\mathbf{y}^\top) = &R + R(-\tau\mathbf{e}_1)\mathbf{e}_1^\top + RU^{(2)}(-\tau\bar{\mathbf{k}})\mathbf{e}_1^\top + (-\tau\boldsymbol{\gamma})\mathbf{e}_1^\top \\
&-\tau R\mathbf{e}_1\bar{\mathbf{k}}^\top U^{(2)\top} -\tau R\mathbf{e}_1\mathbf{b}^\top + RU^{(2)}(-\tau\bar{\mathbf{k}}\bar{\mathbf{k}}^\top)U^{(2)\top} \\
&+ (-\tau\boldsymbol{\gamma}\bar{\mathbf{k}}^\top)U^{(2)\top} + RU^{(2)}(-\tau\bar{\mathbf{k}}\mathbf{b}^\top) + (-\tau\boldsymbol{\gamma}\mathbf{b}^\top).
\end{aligned}
\end{equation}

\textbf{(ii) Transformation of $RU\Omega U^\top$:} Using $\mathbf{y} = \mathbf{e}_1 + U^{(2)}\bar{\mathbf{k}} + \mathbf{b}$, $\Omega U^\top\mathbf{y} = \boldsymbol{\delta}_1$, and $U = \mathbf{e}_1\bar{\mathbf{u}}_1^\top + U^{(2)}$, we get:
\begin{equation}
\begin{aligned}
RU\Omega U^\top(I &- \tau\mathbf{y}\mathbf{y}^\top) = R\mathbf{e}_1(\bar{\mathbf{u}}_1^\top\Omega\bar{\mathbf{u}}_1 - \tau\bar{\mathbf{u}}_1^\top\boldsymbol{\delta}_1)\mathbf{e}_1^\top + RU^{(2)}(\Omega\bar{\mathbf{u}}_1 - \tau\boldsymbol{\delta}_1)\mathbf{e}_1^\top \\
&+ R\mathbf{e}_1\bar{\mathbf{u}}_1^\top\Omega U^{(2)\top} -\tau R\mathbf{e}_1\bar{\mathbf{u}}_1^\top\boldsymbol{\delta}_1\bar{\mathbf{k}}^\top U^{(2)\top} -\tau R\mathbf{e}_1\bar{\mathbf{u}}_1^\top\boldsymbol{\delta}_1\mathbf{b}^\top \\
&+ RU^{(2)}(\Omega - \tau \boldsymbol{\delta}_1\bar{\mathbf{k}}^\top)U^{(2)\top} + RU^{(2)}(-\tau\boldsymbol{\delta}_1\mathbf{b}^\top).
\end{aligned}
\end{equation}

\textbf{(iii) Transformation of $\Phi U^\top$:} Using $\mathbf{y} = \mathbf{e}_1 + U^{(2)}\bar{\mathbf{k}} + \mathbf{b}$, $U^\top\mathbf{y} = \boldsymbol{\delta}_2$, and $U = \mathbf{e}_1\bar{\mathbf{u}}_1^\top + U^{(2)}$, we get:
\begin{equation}
\Phi U^\top(I - \tau\mathbf{y}\mathbf{y}^\top) = (\Phi\bar{\mathbf{u}}_1 - \tau\Phi\boldsymbol{\delta}_2)\mathbf{e}_1^\top + (\Phi - \tau\Phi\boldsymbol{\delta}_2\bar{\mathbf{k}}^\top)U^{(2)\top} + (-\tau\Phi\boldsymbol{\delta}_2\mathbf{b}^\top).
\end{equation}

\textbf{(iv) Transformation of $RU\Psi$:} Using $\mathbf{y} = \mathbf{e}_1 + U^{(2)}\bar{\mathbf{k}} + \mathbf{b}$, $\Psi\mathbf{y} = \boldsymbol{\delta}_3$, and $U = \mathbf{e}_1\bar{\mathbf{u}}_1^\top + U^{(2)}$,  we get:
\begin{equation}
\begin{aligned}
RU\Psi(I &- \tau\mathbf{y}\mathbf{y}^\top) = (-\tau R\mathbf{e}_1\bar{\mathbf{u}}_1^\top\boldsymbol{\delta}_3)\mathbf{e}_1^\top + RU^{(2)}(-\tau\boldsymbol{\delta}_3)\mathbf{e}_1^\top + R\mathbf{e}_1\bar{\mathbf{u}}_1^\top\Psi \\
&-\tau R\mathbf{e}_1\bar{\mathbf{u}}_1^\top\boldsymbol{\delta}_3\bar{\mathbf{k}}^\top U^{(2)\top} -\tau R\mathbf{e}_1\bar{\mathbf{u}}_1^\top\boldsymbol{\delta}_3\mathbf{b}^\top \\
&+ RU^{(2)}(-\tau\boldsymbol{\delta}_3\bar{\mathbf{k}}^\top)U^{(2)\top} + RU^{(2)}(\Psi - \tau\boldsymbol{\delta}_3\mathbf{b}^\top).
\end{aligned}
\end{equation}

\textbf{(v) Transformation of $\Lambda$:} Using $\mathbf{y} = \mathbf{e}_1 + U^{(2)}\bar{\mathbf{k}} + \mathbf{b}$ and $\Lambda \mathbf{y} = \boldsymbol{\delta}_4$, we get:
\begin{equation}\label{eq:lambda_transform}
\Lambda(I - \tau\mathbf{y}\mathbf{y}^\top) = (-\tau\boldsymbol{\delta}_4)\mathbf{e}_1^\top + (-\tau\boldsymbol{\delta}_4\bar{\mathbf{k}}^\top)U^{(2)\top} + (\Lambda - \tau \boldsymbol{\delta}_4\mathbf{b}^\top).
\end{equation}

Summing contributions (i)--(v), we identify the structure of $\widetilde{\Gamma}$.

\textbf{First}, the submatrix $\widetilde{\Gamma}[2:n, 2:n]$ satisfies:
\begin{equation}\label{eq:submatrix_structure_rq}
\widetilde{\Gamma}[2:n, 2:n] = \tilde{R} + \tilde{R}\tilde{U}\tilde{\Omega}\tilde{U}^\top + \tilde{\Phi}\tilde{U}^\top + \tilde{R}\tilde{U}\tilde{\Psi} + \tilde{\Lambda},
\end{equation}
where $\tilde{R} = R[2:n,2:n]$, $\tilde{U} = U[2:n,:]$, and the updated matrices are:
\begin{align*}
\tilde{\Omega} &= -\tau\bar{\mathbf{k}}\bar{\mathbf{k}}^\top + \Omega - \tau\boldsymbol{\delta}_1\bar{\mathbf{k}}^\top - \tau\boldsymbol{\delta}_3\bar{\mathbf{k}}^\top \in \mathbb{R}^{r\times r}, \\
\tilde{\Phi} &= \begin{bmatrix} \tilde{\Phi}_s \\ \mathbf{0} \end{bmatrix}\in \mathbb{R}^{(n-1)\times r}, \quad
\tilde{\Phi}_s = (-\tau\boldsymbol{\gamma}\bar{\mathbf{k}}^\top + \Phi - \tau\Phi\boldsymbol{\delta}_2\bar{\mathbf{k}}^\top - \tau\boldsymbol{\delta}_4\bar{\mathbf{k}}^\top)[2:\min(l+1,n),:] \in \mathbb{R}^{\min(l,n-1)\times r}, \\
\tilde{\Psi} &= [\tilde{\Psi}_s, \mathbf{0}]\in \mathbb{R}^{r\times (n-1)}, \quad
\tilde{\Psi}_s = (-\tau\bar{\mathbf{k}}\mathbf{b}^\top - \tau\boldsymbol{\delta}_1\mathbf{b}^\top + \Psi - \tau\boldsymbol{\delta}_3\mathbf{b}^\top)[:,2:\min(l+1,n)] \in \mathbb{R}^{r\times \min(l,n-1)}, \\
\tilde{\Lambda} &= \begin{bmatrix} \tilde{\Lambda}_s & \mathbf{0} \\ \mathbf{0} & \mathbf{0} \end{bmatrix}\in \mathbb{R}^{(n-1)\times (n-1)}, \\
\tilde{\Lambda}_s &= (-\tau\boldsymbol{\gamma}\mathbf{b}^\top - \tau\Phi\boldsymbol{\delta}_2\mathbf{b}^\top + \Lambda - \tau\boldsymbol{\delta}_4\mathbf{b}^\top)[2:\min(l+1,n),2:\min(l+1,n)] \in \mathbb{R}^{\min(l,n-1)\times \min(l,n-1)}.
\end{align*}
This confirms that $\widetilde{\Gamma}[2:n, 2:n]$ is an HMUTM of $\tilde{R}$ under $\tilde{U}$.

\textbf{Second}, the vector $\widetilde{\Gamma}[2:n, 1]$ is given by:
\begin{equation}\label{eq:col_structure_rq}
\widetilde{\Gamma}[2:n, 1] = \boldsymbol{\eta} + (RU\boldsymbol{\mu})[2:n],
\end{equation}
where
\begin{align*}
\boldsymbol{\eta} &= \begin{bmatrix} \boldsymbol{\eta}_s \\ \mathbf{0} \end{bmatrix}, \quad
\boldsymbol{\eta}_s = (-\tau \boldsymbol{\gamma} + \Phi\bar{\mathbf{u}}_1 - \tau\Phi\boldsymbol{\delta}_2 - \tau\boldsymbol{\delta}_4 + \boldsymbol{\lambda}^{(1)})[2:\min(l+1,n)] \in \mathbb{R}^{\min(l,n-1)}, \\
\boldsymbol{\mu} &= -\tau \bar{\mathbf{k}} + \Omega\bar{\mathbf{u}}_1 - \tau\boldsymbol{\delta}_1 - \tau\boldsymbol{\delta}_3 + \boldsymbol{\psi}^{(1)} \in \mathbb{R}^r.
\end{align*}
This matches the form of an HMUTV (Definition \ref{def:hmutv}).

\textbf{Finally}, the $(1,1)$ entry of $\widetilde{\Gamma}$ is updated as:
\begin{equation}\label{eq:diag_update_rq}
\begin{aligned}
\tilde{\Gamma}[1,1] = &R[1,1] -\tau R[1,1] -\tau\mathbf{r}^{(1)\top}U^{(2)}\bar{\mathbf{k}} -\tau\gamma[1] \\
&+ R[1,1](\bar{\mathbf{u}}_1^\top\Omega\bar{\mathbf{u}}_1 -\tau\bar{\mathbf{u}}_1^\top\boldsymbol{\delta}_1) +\mathbf{r}^{(1)\top}U^{(2)}(\Omega\bar{\mathbf{u}}_1 -\tau\boldsymbol{\delta}_1) \\
&+\boldsymbol{\phi}^{(1)\top}\bar{\mathbf{u}}_1 -\tau\boldsymbol{\phi}^{(1)\top}\boldsymbol{\delta}_2 -\tau R[1,1]\bar{\mathbf{u}}_1^\top\boldsymbol{\delta}_3 +R[1,1]\bar{\mathbf{u}}_1^\top\boldsymbol{\psi}^{(1)} \\
&-\tau\mathbf{r}^{(1)\top}U^{(2)}\boldsymbol{\delta}_3 +\mathbf{r}^{(1)\top}U^{(2)}\boldsymbol{\psi}^{(1)} -\tau \boldsymbol{\delta}_4[1] +\Lambda[1,1].
\end{aligned}
\end{equation}

The updates \eqref{eq:submatrix_structure_rq}, \eqref{eq:col_structure_rq}, and \eqref{eq:diag_update_rq} provide the complete formulas needed to advance the structured representation by one Householder transformation.
\end{proof}

\subsection{Structure-Preserving Theorem for $RQ$}

Equipped with Lemma \ref{lemma:structure_preserve_rq}, we can now state and prove the main structural result for the $RQ$ product.

\begin{theorem}[Structure of $RQ$ for symmetric BPS matrices]\label{thm:structure_rq}
Let $A$ be a symmetric BPS matrix as in \eqref{express_A_symm} with semiseparable rank $r$ and bandwidth $l$, and let $A=QR$ be its QR factorization. Then the matrix $RQ$ is also a symmetric BPS matrix. Specifically, it can be expressed as
\begin{equation}\label{express_RQ}
    RQ = B_R + \tril(\Theta \Delta^\top, -1) + \triu(\Delta \Theta^\top, 1),
\end{equation}
where
\begin{itemize}
    \item $\Theta = R U \in \mathbb{R}^{n \times r}$,
    \item $\Delta \in \mathbb{R}^{n \times r}$ is a low-rank generator matrix,
    \item $B_R$ is a symmetric banded matrix with lower and upper bandwidth $l$.
\end{itemize}
Thus, the $RQ$ product has a lower and upper semiseparable rank of $r$ and a lower and upper bandwidth of $l$.
\end{theorem}

\begin{proof}
The proof proceeds by induction on the steps of applying $Q$ (as a product of Householder reflectors) to $R$ from the right. Define $R^{(0)} = R$ and $R^{(j)} = R (I - \tau_1 \mathbf{y}_1 \mathbf{y}_1^\top) \cdots (I - \tau_j \mathbf{y}_j \mathbf{y}_j^\top)$ for $j=1,\ldots,n-1$, so that $R^{(n-1)} = RQ$.

From the proof of Theorem \ref{thm:structure_preserve}, the Householder vectors have the form:
\begin{equation}
\mathbf{y}_k[k:n] = \mathbf{e}_1 + U^{(k+1)}\bar{\mathbf{k}}_{k+1} + \mathbf{b}_{k+1}, \quad k=1,\ldots,n-1,
\end{equation}
where $\mathbf{e}_1\in\mathbb{R}^{n-k+1}$, $U^{(k+1)}\in\mathbb{R}^{(n-k+1)\times r}$ satisfies $U^{(k+1)}[1,:]=\mathbf{0}$ and $U^{(k+1)}[2:n-k+1,:]=U[k+1:n,:]$, $\bar{\mathbf{k}}_{k+1}\in\mathbb{R}^r$, and $\mathbf{b}_{k+1}\in\mathbb{R}^{n-k+1}$ is nonzero only in its entries $2$ through $\min(l+1, n-k+1)$.

Let $R_k = R[k:n, k:n]$ and $U_k = U[k:n, :]$. We prove by induction that for $j=0,\ldots,n-1$:
\begin{enumerate}
    \item $R^{(j)}[j+1:n, j+1:n]$ is an HMUTM of $R_{j+1}$ under $U_{j+1}$.
    \item $R^{(j)}[j+1:n, j]$ is an HMUTV of $R_{j+1}$ under $U_{j+1}$.
\end{enumerate}

\textbf{Base case ($j=0$):} $R^{(0)}=R$ is trivially an HMUTM of $R_1$ under $U_1$ with all perturbation matrices zero.

\textbf{Inductive step:} Assume the statement holds for some $j < n-1$. That is,
\begin{equation}
R^{(j)}[j+1:n, j+1:n] = R_{j+1} + R_{j+1}U_{j+1}\Omega_{j+1}U_{j+1}^\top + \Phi_{j+1}U_{j+1}^\top + R_{j+1}U_{j+1}\Psi_{j+1} + \Lambda_{j+1},
\end{equation}
with $\Omega_{j+1}, \Phi_{j+1}, \Psi_{j+1}, \Lambda_{j+1}$ having the required sparsity patterns.

We now compute $R^{(j+1)} = R^{(j)}(I - \tau_{j+1}\mathbf{y}_{j+1}\mathbf{y}_{j+1}^\top)$. Applying Lemma \ref{lemma:structure_preserve_rq} with $\Gamma = R^{(j)}[j+1:n, j+1:n]$, $R = R_{j+1}$, $U = U_{j+1}$, and $\mathbf{y} = \mathbf{y}_{j+1}[j+1:n]$, we obtain:
\begin{enumerate}
    \item $\widetilde{\Gamma}[2:n-j, 2:n-j] = R^{(j+1)}[j+2:n, j+2:n]$ is an HMUTM of $R_{j+2}$ under $U_{j+2}$.
    \item $\widetilde{\Gamma}[2:n-j, 1] = R^{(j+1)}[j+2:n, j+1]$ is an HMUTV of $R_{j+1}$ under $U_{j+1}$.
\end{enumerate}
The lemma also provides explicit update formulas for the parameters, showing they retain the correct sparsity patterns. This completes the inductive step.

By induction, the structured form holds for all $j$. In particular, for each $j=0,\ldots,n-2$, we have an HMUTV representation:
\begin{equation}
R^{(j+1)}[j+2:n, j+1] = \boldsymbol{\eta}_{j+2} + (R_{j+1}U_{j+1}\boldsymbol{\mu}_{j+2})[2:n-j].
\end{equation}
Since $R_{j+1}U_{j+1}[2:n-j,:] = RU[j+2:n,:]$, this can be rewritten as:
\begin{equation}
\begin{aligned}
&(RQ)[j+1, j+2:n] = (RQ)[j+2:n, j+1] \\&= R^{(j+1)}[j+2:n, j+1] =\boldsymbol{\eta}_{j+2} + (RU\boldsymbol{\mu}_{j+2})[j+2:n].
\end{aligned}
\end{equation}
Let $\Delta \in \mathbb{R}^{n \times r}$ be defined by $\Delta[j,:] = \boldsymbol{\mu}_{j+1}^\top$ for $j=1,\ldots,n-1$ and $\Delta[n,:] = \mathbf{0}$. Let $\Theta = RU$. Then the semiseparable part of $RQ$ is exactly $\tril(\Theta\Delta^\top,-1) + \triu(\Delta\Theta^\top,1)$.

The banded part $B_R$ is constructed from the diagonal entries of $R^{(j)}$ and the vectors $\boldsymbol{\eta}_{j+1}$, which are nonzero only in their first $\min(l, n-j)$ entries. By the update formulas in Lemma \ref{lemma:structure_preserve_rq}, these entries depend only on local information (within distance $l$ from the diagonal), ensuring $B_R$ is banded with bandwidth $l$. The symmetry of $B_R$ follows from the symmetry of $RQ$ and the symmetry of the semiseparable representation.

Thus, $RQ$ admits the representation \eqref{express_RQ}, confirming it is a symmetric BPS matrix with the stated properties.
\end{proof}

\subsection{Fast RQ Algorithm}

Theorem \ref{thm:structure_rq} is constructive and leads directly to a fast algorithm for computing the $RQ$ product without explicitly forming the dense orthogonal matrix $Q$.

\begin{algorithm}\caption{Fast RQ for Symmetric BPS Matrices}\label{algo:fastrq}
\begin{algorithmic}[1]
\State \textbf{Input:} A symmetric BPS matrix $A$ given by its generators: symmetric banded $B$ (bandwidth $l$), and $U, V \in \mathbb{R}^{n \times r}$ satisfying $A = B + \tril(UV^\top, -1) + \triu(VU^\top, 1)$.
\State \textbf{Output:} The $RQ$ product in structured form: symmetric banded matrix $B_R$, and low-rank generators $\Theta$ and $\Delta$.

\Statex
\State 1. \textbf{Compute fast QR factorization:}
\State \quad Run Algorithm \ref{algo:fastqr} on $A$ to obtain the structured factor matrix $F$ and coefficients $\boldsymbol{\tau}$. Extract the structured representation of the $R$ factor and all Householder vectors $\mathbf{y}_k$ (with parameters $\bar{\mathbf{k}}_k$, $\mathbf{b}_k$).
\State \quad Compute and store $\Theta = R U$ using the structured $R$.
\Statex
\State 2. \textbf{Initialize HMUTM parameters:}
\State \quad Set $\Omega_1 \gets \mathbf{0}_{r \times r}$, $\Phi_1 \gets \mathbf{0}_{n \times r}$, $\Psi_1 \gets \mathbf{0}_{r \times n}$, $\Lambda_1 \gets \mathbf{0}_{n \times n}$.
\State \quad Initialize $\Delta \gets \mathbf{0}_{n \times r}$.
\Statex
\State 3. \textbf{Forward recursion (apply $Q$ from the right):}
\For{$k = 1$ to $n-1$}
    \State a. Retrieve the Householder parameters $\bar{\mathbf{k}}_{k+1}$ and $\mathbf{b}_{k+1}$ for step $k$.
    \State b. Using the update formulas from Lemma \ref{lemma:structure_preserve_rq} (as derived in its proof), compute the new HMUTM parameters $(\Omega_{k+1}, \Phi_{k+1}, \Psi_{k+1}, \Lambda_{k+1})$ from $(\Omega_k, \Phi_k, \Psi_k, \Lambda_k, \bar{\mathbf{k}}_{k+1}, \mathbf{b}_{k+1})$.
    \State c. Compute the HMUTV parameter $\boldsymbol{\mu}_{k+1}$ and $\boldsymbol{\eta}_{k+1}$ from the relevant update formulas in the proof of Lemma \ref{lemma:structure_preserve_rq}.
    \State d. Set $\Delta[k,:] \gets \boldsymbol{\mu}_{k+1}^\top$ and $B_R[k+1:\min(k+1,n),k] \gets \boldsymbol{\eta}_{k+1}[k+1:\min(k+1,n)]$.
    \State e. Compute the diagonal element from the formula in the proof of Lemma \ref{lemma:structure_preserve_rq} and store it in $B_R[k,k]$. 
\EndFor
\Statex
\State 4. \textbf{Final State}
\State \quad Set $B_R[n,n] \gets R^{(n-1)}[n,n]$
\Statex
\State 5. \textbf{Return} $B_R$, $\Theta$, $\Delta$.
\end{algorithmic}
\end{algorithm}

\textbf{Complexity analysis.} Step 1, the QR factorization, requires $O(n)$ operations by Theorem \ref{thm:structure_preserve}. The computation of $\Theta = RU$ can be performed in $O(nr)$ time due to the structure of $R$. The forward recursion in Step 3 performs a constant amount of work per iteration, as all matrix operations involve matrices whose dimensions (e.g., $r \times r$, $r \times l$, $l \times l$) are independent of $n$. Therefore, Algorithm \ref{algo:fastrq} runs in $O(n)$ time and uses $O(n)$ storage.

\begin{remark}\label{rem:symmetric_tracking}
A subtle but crucial point in Algorithm \ref{algo:fastrq} is that during the forward recursion, we track only the evolving structure of the submatrix $R^{(j)}[j+1:n, j+1:n]$, even though the entire matrix $R^{(j)}[:, j+1:n]$ is being modified. More precisely, after applying the first $j$ Householder transformations from the right, the first $j$ columns of $R^{(j)}$ have reached their final state and will not change in subsequent steps. 
However, the first $j$ rows of $R^{(j)}$ (in columns $j+1:n$) are still subject to modification by later transformations. 
Nevertheless, because the final product $RQ$ is known to be symmetric, these pending row entries are not independent: they must eventually equal the corresponding entries in the already-fixed columns. 
Thus, while they appear to be "in flux" during intermediate steps, their ultimate values are implicitly determined by symmetry. 
This allows the algorithm to safely disregard the explicit updating of these rows and focus solely on the lower-right submatrix, which is the only part whose future evolution is not predetermined. 
Lemma \ref{lemma:structure_preserve_rq} guarantees that this submatrix maintains an HMUTM structure, enabling its efficient propagation. 
This exploitation of symmetry is key to achieving $O(n)$ complexity, as it confines the work at each step to a submatrix with a structured representation of constant size independent of $n$.
\end{remark}


\begin{remark}[Shifted QR Algorithm]
The results extend directly to the shifted QR algorithm for eigenvalue computations. Given a shift $\mu \in \mathbb{R}$, the shifted matrix $A - \mu I = (B - \mu I) + \tril(UV^\top, -1) + \triu(VU^\top, 1)$ remains a symmetric BPS matrix, requiring only a diagonal adjustment to $B$. The resulting $RQ$ product can then be shifted back to obtain $RQ + \mu I$, which maintains the same BPS structure. Hence, each iteration of the shifted QR algorithm---factorization of $A^{(k)} - \mu I = QR$ followed by formation of $A^{(k+1)} = RQ + \mu I$---can both be performed in $O(n)$ time while preserving the BPS structure throughout.
\end{remark}

\section{Numerical results}
\label{sec:experiments}

To validate the theoretical complexity and demonstrate the practical efficiency of our proposed algorithms, we implemented the fast QR factorization and the complete linear solver in Julia. The implementation is publicly available in the \texttt{BandedPlusSemiseparableMatrices.jl} package~\cite{BandedPlusSemiseparableMatrices2024}, providing an open-source resource for the scientific computing community. All numerical tests use banded-plus-semiseparable matrices with fixed structural parameters $l=4$, $m=5$, $r=2$, $p=3$ to isolate the scaling behavior with respect to the matrix size $n$. Computations were carried out on a MacBook Air equipped with an Apple M2 chip (8-core CPU, 8 GB RAM), without GPU acceleration or access to external computing resources.

\subsection{Linear Complexity Verification}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{plot_linearity.pdf}
\caption{Log-log plot of the total solver time (QR factorization + application of $Q^T$ + backward substitution) versus matrix size $n$. The dashed reference line has slope 1, indicating ideal linear scaling. \sotodoinline{Add different parameters for ranks and bands}\sotodoinline{Add more ticks on the y-axis}}
\label{fig:scaling}
\end{figure}

Figure~\ref{fig:scaling} demonstrates the linear time complexity of our complete solver for banded-plus-semiseparable linear systems. The total execution time, encompassing all three phases (QR factorization, application of $Q^T$, and backward substitution), scales as $O(n)$ across five orders of magnitude, from $n=100$ to $n=10^6$. The close alignment with the reference line of slope 1 confirms the complexity analysis in Section~\ref{sec:algorithms}.

\subsection{Comparison with HODLR QR}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{plot_comparison.pdf}
\caption{Comparison of QR factorization times between our fast BPS QR algorithm and the HODLR QR implementation from~\cite{massei2020hm}. Both algorithms operate on banded-plus-semiseparable matrices with parameters $l=4$, $m=5$, $r=2$, $p=3$. \sotodoinline{Use different markers so clearer when black-and-white.} \sotodoinline{Go to $n = 10^6$}} \sotodoinline{add a stability plot}
\label{fig:comparison}
\end{figure}

We compare our fast QR factorization against the state-of-the-art HODLR (Hierarchically Off-Diagonal Low-Rank) QR implementation from the hm-toolbox~\cite{massei2020hm}. The hm-toolbox provides efficient MATLAB routines for various structured matrices, including HODLR and HSS matrices, and represents one of the most mature implementations for hierarchical matrix computations.

Figure~\ref{fig:comparison} shows the execution times for QR factorization of BPS matrices using both approaches. Our algorithm demonstrates superior scaling for larger matrix sizes. This performance advantage stems from several factors:

\begin{itemize}
\item \textbf{Specialized structure exploitation}: Our algorithm is specifically designed for the banded-plus-semiseparable structure, avoiding the overhead of general hierarchical representations.
\item \textbf{Reduced Computational Overhead}: By working directly with the semiseparable generators rather than building a hierarchical representation, we avoid the logarithmic factors inherent in tree-based approaches.
\end{itemize}

The performance gap widens with increasing $n$, confirming that our method is particularly well-suited for large-scale problems. For $n=150,000$, our implementation achieves approximately 7$\times$ speedup over the HODLR approach, demonstrating the practical benefits of our specialized algorithm.


\section{Conclusions}
\label{sec:conclusions}

In this paper, we have established a fundamental theoretical result for BPS matrices and developed efficient algorithms based on this foundation. Our main contribution is the proof that the QR factorization of a BPS matrix preserves the banded-plus-semiseparable structure, with precisely characterized ranks and bandwidths in the resulting factor matrix. This theoretical insight enabled the design of a complete $O(n)$ direct solver for BPS linear systems, comprising:
\begin{itemize}
\item A structure-preserving QR factorization algorithm (Algorithm~\ref{algo:fastqr})
\item An efficient $O(n)$ application of $Q^T$ (Algorithm~\ref{algo_applyQ})
\item A fast backward substitution routine (Algorithm~\ref{algo_backsub})
\end{itemize}

Furthermore, for symmetric BPS matrices, we have shown that the $RQ$ product---a key operation in the QR algorithm for eigenvalues---also maintains the BPS structure. This led to the development of another $O(n)$ algorithm (Algorithm~\ref{algo:fastrq}) for computing $RQ$ without explicitly forming the dense orthogonal matrix $Q$, opening the door to efficient eigenvalue computations for symmetric BPS matrices.

The numerical experiments confirm the linear scaling of our approach and demonstrate significant performance advantages over existing HODLR-based methods. Our implementation in the SemiseparableMatrices.jl package provides the scientific computing community with efficient, open-source tools for working with this important class of structured matrices.

\subsection*{Future Work}


A compelling extension involves applying our methodology to specific blocked banded matrices arising in $hp$-FEM~\cite{knook2024quasi}. These have optimal complexity so-called reverse Cholesky factorizations (Cholesky from the bottom right instead of the top left) for positive definite problems. One of our motativations for the present work is developing an optimal complexity QL factorization for these special block banded matrices.  The key challenge is generalizing our framework to \emph{block} banded-plus-semiseparable matrices while maintaining $O(N)$ complexity. The primary difficulty lies in applying Householder transformations from one block to subsequent blocks in $O(n)$ time(where $n$ is block size and $N$ the total size), rather than $O(n^3)$. While our current framework doesn't directly apply, the core insight of structure preservation provides a promising foundation for this challenging extension.

\bibliographystyle{siamplain}
\bibliography{references}
\end{document}
