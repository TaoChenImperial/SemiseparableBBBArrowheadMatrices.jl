% SIAM Article Template
\documentclass[review,onefignum,onetabnum]{siamart171218}

% Information that is shared between the article and the supplement
% (title and author information, macros, packages, etc.) goes into
% ex_shared.tex. If there is no supplement, this file can be included
% directly.

% Packages and macros go here
\usepackage{lipsum}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{epstopdf}
%\usepackage{algorithmic}
\usepackage{algorithm} 

\usepackage{todonotes}
\newcommand{\sotodo}{\todo[color=green]}
\newcommand{\sotodoinline}{\todo[color=green,inline=true]}
\newcommand{\tctodo}{\todo[color=red]}
\newcommand{\tctodoinline}{\todo[color=red,inline=true]}

\renewcommand{\thealgorithm}{\thesubsection.\arabic{algorithm}}

\def\bbR{{\mathbb R}}
\def\BPS{{\mathrm{BPS}}_{(r,p),(\ell,u)}^{n \times n}}
\def\SBPS{{\mathrm{SBPS}}_{r,\ell}^{n \times n}}
\def\calP{{\mathcal P}}
\def\calV{{\mathcal V}}
\def\calH{{\mathcal H}}
\def\calW{{\mathcal W}}

\makeatletter
\@addtoreset{algorithm}{subsection}
\makeatother
\usepackage{algpseudocode}
\newtheorem{remark}{Remark}[section]

\ifpdf
  \DeclareGraphicsExtensions{.eps,.pdf,.png,.jpg}
\else
  \DeclareGraphicsExtensions{.eps}
\fi

% Add a serial/Oxford comma by default.
\newcommand{\creflastconjunction}{, and~}

% Used for creating new theorem and remark environments
\newsiamremark{hypothesis}{Hypothesis}
\crefname{hypothesis}{Hypothesis}{Hypotheses}
\newsiamthm{claim}{Claim}

% Sets running headers as well as PDF title and authors
\headers{QR for Banded-Plus-Semiseparable Matrices}{T. Chen and S. Olver}

% Title. If the supplement option is on, then "Supplementary Material"
% is automatically inserted before the title.
\title{The  QR Factorization for Banded-Plus-Semiseparable Matrices is computable in linear complexity}

% Authors: full names plus addresses.
\author{Tao Chen\thanks{Department of Mathematics, Imperial College London, London, UK
  (\email{t.chen24@imperial.ac.uk}, \email{s.olver@imperial.ac.uk}).}
\and Sheehan Olver\footnotemark[1]}

\usepackage{amsopn}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\triu}{triu}
\DeclareMathOperator{\tril}{tril}


%% Added on Overleaf: enabling xr
\makeatletter
\newcommand*{\addFileDependency}[1]{% argument=file name and extension
  \typeout{(#1)}% latexmk will find this if $recorder=0 (however, in that case, it will ignore #1 if it is a .aux or .pdf file etc and it exists! if it doesn't exist, it will appear in the list of dependents regardless)
  \@addtofilelist{#1}% if you want it to appear in \listfiles, not really necessary and latexmk doesn't use this
  \IfFileExists{#1}{}{\typeout{No file #1.}}% latexmk will find this message if #1 doesn't exist (yet)
}
\makeatother

\newcommand*{\myexternaldocument}[1]{%
    \externaldocument{#1}%
    \addFileDependency{#1.tex}%
    \addFileDependency{#1.aux}%
}

% Optional PDF information
\ifpdf
\hypersetup{
  pdftitle={The  QR factorization of a Banded-Plus-Semiseparable Matrix is Computable with Linear Complexity},
  pdfauthor={D. Doe, P. T. Frank, and J. E. Smith}
}
\fi

% The next statement enables references to information in the
% supplement. See the xr-hyperref package for details.

%% Use \myexternaldocument on Overleaf
%\myexternaldocument{ex_supplement}

% FundRef data to be entered by SIAM
%<funding-group>
%<award-group>
%<funding-source>
%<named-content content-type="funder-name"> 
%</named-content> 
%<named-content content-type="funder-identifier"> 
%</named-content>
%</funding-source>
%<award-id> </award-id>
%</award-group>
%</funding-group>

\begin{document}

\maketitle

\begin{abstract}
We show that each stage of the QR factorization of banded-plus-semiseparable matrices computed using Householder reflections has a specific structured perturbation. This theoretical result enables the design of linear-complexity algorithms for QR factorization and for solving the associated linear systems. For symmetric BPS matrices, we further show that the $RQ$ product---central to eigenvalue computations via the QR algorithm---also preserves the BPS structure, leading to a linear-complexity algorithm for its formation. Numerical experiments validate the optimal linear complexity, confirm high numerical accuracy,  and demonstrate substantial speedups compared with existing hierarchical approaches. The algorithms have been implemented in an open-source Julia package, providing an efficient and accessible platform for practical use.
\end{abstract}

\begin{keywords}
banded-plus-semiseparable matrices, QR factorization, linear complexity, structured matrices, direct solvers
\end{keywords}

\begin{AMS}
65F05, 65F50, 15A23, 65Y20
\end{AMS}

\section{Introduction}

Banded-plus-semiseparable (BPS) matrices, expressible as 
\begin{equation*}
A = \underbrace{B}_{\text{banded}} + \underbrace{\tril(UV^\top, -1)}_{\text{lower semiseparable, rank } r} + \underbrace{\triu(WS^\top, 1)}_{\text{upper semiseparable, rank } p} \in \bbR^{n \times n},
\end{equation*}
 arise in numerous applications from spectral methods for differential equations~\cite{iserles2025stable} to signal processing, control theory, and eigenvalue problems~\cite{vandebril2005note}. Importantly, the inverse of a banded matrix is itself a BPS matrix, making this structure fundamental for solving banded linear systems via their inverses~\cite{rozsa1991band}. Their structure requires only $O(n)$ storage which invites the development of \( O(n) \) algorithms, a goal successfully achieved for iterations of the QR algorithm for symmetric semiseparable systems~\cite{vandebril2005implicit}, and for solving linear systems with diagonal-plus-semiseparable matrices~\cite{eidelman1997inversion}. 
  However, generalizing these results to the case where the matrix is nonsymmetric, or the banded part $B$ is a genuine banded matrix, rather than merely a diagonal one, presents significant algorithmic challenges. 


Pioneering work has established \( O(n) \) solvers for the BPS matrices via ULV factorization~\cite{chandrasekaran2003fast}. BPS matrices can also be viewed as  hierarchically semiseparable (HSS) matrices, and solvers using HSS structure is a well-developed area~\cite{chandrasekaran2007fast, chandrasekaran2006fast, massei2020hm, xia2010fast}. A parallel line of research extensively developed the theory and algorithms for semiseparable matrices, including approaches leveraging rational Krylov techniques~\cite{fasino2005rational, vandebril2008rational}, structure-preserving analyses~\cite{delvaux2006rank, delvaux2006structures, eidelman2005qr}, and alternative representations~\cite{delvaux2008givens, vandebril2005note}. Despite these advances, a clear theoretical guarantee that the standard QR factorization preserves the BPS structure has been missing, with most existing solvers relying on more complex ULV~\cite{chandrasekaran2003fast} or intricate Givens-based schemes~\cite{delvaux2008qr, mastronardi2001fast, van2004two}. A special case of BPS matrices are almost banded matrices which were used in~\cite{olver2013fast} to represent discretisations of differential equations using the ultraspherical spectral method. An optimal complexity adaptive QR factorization was introduced, which also gives an optimal complexity QR factorization for BPS matrices with now lower semiseparable part ($r = 0$). It also introduced an optimal complexity back-substitution for upper-triangular BPS matrices, an algorithm we also use.

In this paper, we close this theoretical gap. We prove that the QR factorization of a BPS matrix yields a factor matrix \( F \), which is the matrix containing both $R$ and the Householder reflectors encoding $Q$,  that is itself BPS. This pivotal result, proven via an inductive framework, enables the design of an \( O(n) \) QR factorization. Furthermore, it facilitates a complete direct solver: applying \( Q^\top \) and performing backward substitution on the structured factor \( R \) are also achieved in linear time. Our work thus provides a QR-based \( O(n) \) solution for BPS systems. Furthermore, we show that for symmetric BPS matrices, the $RQ$ product (central to the QR eigenvalue algorithm) also retains the BPS structure, enabling efficient eigenvalue computations.

While this paper focuses on real square matrices, the techniques and results naturally extend to complex matrices by replacing transposes with conjugate transposes. The extension to rectangular matrices is also straightforward, as Householder QR factorization applies similarly to rectangular matrices, and the structural arguments carry over with minor adjustments. These extensions are omitted but represent immediate generalizations of the theory presented here.

The rest of this paper is organized as follows. Section~\ref{sec:main} presents our main theoretical contributions: the definitions, the core lemma on structure preservation under Householder transformations, and the main theorem with its proof. Section~\ref{sec:algorithms} details the resulting $O(n)$ algorithms for QR factorization, application of $Q^\top$, and backward substitution. Building on these foundations, Section~\ref{sec:fastrq} addresses symmetric BPS matrices, proving that the $RQ$ product preserves the BPS structure and presenting a linear-complexity algorithm for its computation. Section~\ref{sec:experiments} presents numerical experiments that confirm the linear complexity, verify the numerical stability, and demonstrate performance advantages of our algorithms. We conclude in Section~\ref{sec:conclusions} with a discussion of future work.
\section{Main Theorem}
\label{sec:main}

\subsection{Problem Formulation and Notation}

We begin by establishing notation for matrix subblocks. For a matrix $M \in \mathbb{R}^{n \times n}$, let $M[i:j, m:n]$ denote the submatrix consisting of rows $i$ through $j$ and columns $m$ through $n$ (inclusive). When extracting a single row or column, the result is a row or column vector, respectively:

 $M[i:j, m] \in \mathbb{R}^{j-i+1}$: the $m$-th column from row $i$ to row $j$ (a column vector).

 $M[i, m:n] \in \mathbb{R}^{n-m+1}$: the $i$-th row from column $m$ to column $n$ (also a column vector).

For brevity, we write $M[k,:]$ for the entire $k$-th row and $M[:,k]$ for the entire $k$-th column.

\begin{definition}
    A {\it banded-plus-semseparable matrix} (BPS) with lower-semisepable rank $r$, upper-semiseparable rank $p$, lower-bandwidth $\ell$ and upper-bandwidth $m$  is $A \in \BPS \subset \bbR^{n \times n}$ such that
\[
A = B + \tril(UV^\top, -1) + \triu(WS^\top, 1)\label{express_A}
\]
where \( U, V \in \mathbb{R}^{n\times r} \), \( W, S \in \mathbb{R}^{n\times p} \), and \( B = (b_{kj})_{k,j=1}^n \in \mathbb{R}^{n \times n} \) is a banded matrix satisfying \( b_{kj}=0 \) for \( k-j>l \) or \( j-k>m \).
\end{definition}


Define vectors \( \bar{\mathbf{u}}_k = U[i,:] \in \mathbb{R}^r \), \( \bar{\mathbf{v}}_k = V[i,:] \in \mathbb{R}^r \), \( \bar{\mathbf{w}}_k = W[i,:] \in \mathbb{R}^p \), and \( \bar{\mathbf{s}}_k = S[i,:] \in \mathbb{R}^p \) for \( k=1,\ldots,n \). The matrix \( A \) can then be expressed element-wise as: 
\begin{equation*}
A =
\begin{bmatrix}
b_{11} & \bar{\mathbf{w}}_1^\top \bar{\mathbf{s}}_2 + b_{12} & \cdots & \bar{\mathbf{w}}_1^\top \bar{\mathbf{s}}_n + b_{1n} \\
\bar{\mathbf{u}}_2^\top \bar{\mathbf{v}}_1 + b_{21} & b_{22} & \cdots & \bar{\mathbf{w}}_2^\top \bar{\mathbf{s}}_n + b_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
\bar{\mathbf{u}}_n^\top \bar{\mathbf{v}}_1 + b_{n1} & \bar{\mathbf{u}}_n^\top \bar{\mathbf{v}}_2 + b_{n2} & \cdots & b_{nn}
\end{bmatrix}.
\end{equation*}

Applying the QR factorization to \( A \) yields a factor matrix \( F \), whose upper triangular part stores the matrix \( R \) and whose lower triangular part contains the Householder reflection vectors \( \mathbf{y} \) generated during the factorization. We will demonstrate that \( F \) itself retains a BPS structure. Specifically, its lower semiseparable part has rank \( r \), its upper semiseparable part has rank \( r+p \), its lower bandwidth is \( \ell \), and its upper bandwidth is \( \ell+m \).

Before proceeding with the detailed proof, let us clarify the precise structure of the factor matrix \( F \) obtained from the QR factorization. In this work, following the convention of LAPACK~\cite{doi:10.1137/1.9780898719604}, we employ a compact representation that stores the complete information of the QR factorization in a single matrix:

\begin{equation*}
F = \begin{bmatrix}
r_{11} & r_{12} & r_{13} & \cdots & r_{1n} \\
y_{2,1} & r_{22} & r_{23} & \cdots & r_{2n} \\
y_{3,1} & y_{3,2} & r_{33} & \cdots & r_{3n} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
y_{n,1} & y_{n,2} & y_{n,3} & \cdots & r_{nn}
\end{bmatrix}
\end{equation*}
where: the upper triangular part (including the main diagonal) of \( F \) stores the elements of the upper triangular matrix \( R \), i.e.: $R = \triu(F)$, and the strictly lower triangular part (excluding the main diagonal) of \( F \) stores the last \( n-k \) elements of (rescaled) Householder reflection vectors \( \mathbf{y}_k \) generated at each step.

More specifically, at the \( k \)-th Householder transformation step (\( k = 1, 2, \ldots, n-1 \)), we construct a reflection vector \( \mathbf{y}_k \) to eliminate the subdiagonal entries of the \( k \)-th column in $A$. This vector takes the form:
\begin{equation}\label{y_k}
\mathbf{y}_k = \begin{bmatrix} 0 \\ \vdots \\ 0 \\ 1 \\ y_{k+1,k} \\ \vdots \\ y_{n,k} \end{bmatrix}
\begin{array}{l} \left.\vphantom{\begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}}\right\} k-1 \text{ zeros} \\ \\ \left.\vphantom{\begin{bmatrix} y_{k+1,k} \\ \vdots \\ y_{n,k} \end{bmatrix}}\right\} n-k \text{ elements}
\end{array}
\end{equation}
Following the LAPACK format, we normalize \( \mathbf{y}_k \) such that its first nonzero element (the \( k \)-th element) equals 1. Consequently, we only need to store the elements from position \( k+1 \) to \( n \) of this vector, which are placed in the \( k \)-th column of \( F \), from row \( k+1 \) to \( n \).

It is important to note that with this normalization convention (where the first nonzero element of each Householder vector \( \mathbf{y}_k \) is 1), the full Householder transformation at the \( k \)-th step is given by \( I - \tau_k \mathbf{y}_k \mathbf{y}_k^T \), where \( \tau_k \) is a scalar coefficient. Therefore, in addition to the factor matrix \( F \), a vector \( \boldsymbol{\tau} = (\tau_1, \tau_2, \ldots, \tau_{n-1})^\top \) is required to completely represent the QR factorization. The orthogonal matrix \( Q \) can be reconstructed as the product \( Q = (I - \tau_1 \mathbf{y}_1 \mathbf{y}_1^\top)(I - \tau_2 \mathbf{y}_2 \mathbf{y}_2^\top) \cdots (I - \tau_{n-1} \mathbf{y}_{n-1} \mathbf{y}_{n-1}^\top) \).

Throughout our analysis, we will focus on the structure of the factor matrix \( F \), while acknowledging that the complete QR representation consists of the pair \( (F, \boldsymbol{\tau}) \). Our main theorem establishes that \( F \) maintains the BPS structure; the scaling coefficients \( \tau_k \) can be stored separately without affecting the structural properties of the algorithm.

We proceed to prove this by induction. First, we introduce two key definitions and a pivotal lemma.

\subsection{Core Definitions and a Key Lemma}

While the final factor matrix of a QR factorization is a BPS matrix, at  intermediate  stages it has a specific structured perturbation. Here we describe this structure in terms of a linear space that, at each stage, the perturbation to the principle submatrix lies in:

\begin{definition}\label{def:hmbpsm}
Given 
\[
A = B + \text{tril}(UV^\top, -1) + \text{triu}(WS^\top, 1)  \in \BPS 
\]
define the vector space:
\begin{align*}
\calP(A) &:= \bigg\{
UJS^\top + UKU^\top A + UE + XS^\top + YU^\top A + Z :\\
&\qquad\quad  J \in \mathbb{R}^{r\times p}, K \in \mathbb{R}^{r\times r}, \\
&\qquad\quad E = [E_s \in \mathbb{R}^{r\times \min(\ell+m,n)}, \mathbf{0}] \in \mathbb{R}^{r \times n} ,\\
 &\qquad\quad  X = \begin{bmatrix} X_s \in \mathbb{R}^{\min(\ell,n)\times p}  \\ \mathbf{0} \end{bmatrix}  \in \mathbb{R}^{n \times p}, \\
 &\qquad\quad  Y = \begin{bmatrix} Y_s \in \mathbb{R}^{\min(\ell,n)\times r}  \\ \mathbf{0} \end{bmatrix}   \in \mathbb{R}^{n \times r},\\ 
&\qquad\quad Z = \begin{bmatrix} Z_s  \in \mathbb{R}^{\min(\ell,n)\times \min(\ell+m,n)} & \mathbf{0} \\ \mathbf{0} & \mathbf{0} \end{bmatrix} \in \mathbb{R}^{n \times n}
\bigg\} \subset \bbR^{n \times n}.
\end{align*}
\end{definition}

In addition, we need to describe the structure of the upper-triangular part in terms of a structured vector:


\begin{definition}
Given \( A \in  \BPS \) and $j=1, 2, ..., n$, define the vector space
\begin{align*}
\calV_j(A) &:= \bigg\{\mathbf{d}^\top + \boldsymbol{\alpha}^\top (S^\top[:,j:n]) + \boldsymbol{\beta}^\top ((U^\top A)[:,j:n]) : \\
&\qquad\quad \mathbf{d} = \begin{bmatrix} \mathbf{d}_s \in \mathbb{R}^{\min(\ell+m,n+1-j)}  \\ \mathbf{0} \end{bmatrix} \in \mathbb{R}^{n+1-j}, \boldsymbol{\alpha} \in \mathbb{R}^p , \boldsymbol{\beta} \in \mathbb{R}^r\bigg\} \subset \bbR^{1 \times (n+1-j)}.
\end{align*}
\end{definition}


\begin{lemma}\label{lemma:structure_preserve}
Given \( A \in \BPS \) and  \( P \in \calP(A) \), let \( \tilde{C} = (I - \tau \mathbf{y} \mathbf{y}^T) (A+P) \) where $\tau$ is a coefficient, $\mathbf{y} = \mathbf{e}_1 + U^{(2)}\bar{\mathbf{k}} + \mathbf{b}$ with $\mathbf{e}_1$ being the first standard basis vector, $U^{(2)}\in\mathbb{R}^{n\times r}$ satisfying $U^{(2)}[1,:]=\mathbf{0}$ and $U^{(2)}[2:n,:]=U[2:n,:]$, $\bar{\mathbf{k}}\in\mathbb{R}^r$, and $\mathbf{b}\in\mathbb{R}^{n}$ nonzero only in entries $2$ through $\min(\ell+1, n)$. Then the following hold:
\begin{enumerate}
    \item The principal submatrix satisfies \( \tilde{C}[2:n, 2:n] = A[2:n, 2:n] + \tilde P\) for \(\tilde P  \in \calP(A[2:n, 2:n]) \).
    \item The first row satisfies \( \tilde{C}[1, 2:n] \in \calV_2(A) \).
\end{enumerate}
\end{lemma}

\begin{proof}
Let us introduce some necessary notations first:

    \( A = B + \tril(UV^\top, -1) + \triu(WS^\top, 1) \), where \( U=(\mathbf{u}_1,...,\mathbf{u}_r) \in \mathbb{R}^{n\times r}\), \( V=(\mathbf{v}_1,...,\mathbf{v}_r)\in \mathbb{R}^{n\times r}\), \( W=(\mathbf{w}_1,...,\mathbf{w}_p)\in \mathbb{R}^{n\times p}\), and \(S=(\mathbf{s}_1,...,\mathbf{s}_p)\in \mathbb{R}^{n \times p}\). Here \( \mathbf{u}_i=(u_1^{(i)},...,u_n^{(i)})^\top \in \mathbb{R}^n\) and \( \mathbf{v}_i=(v_1^{(i)},...,v_n^{(i)})^\top \in \mathbb{R}^n\) for $i=1,...,r$; \( \mathbf{w}_i=(w_1^{(i)},...,w_n^{(i)})^\top \in \mathbb{R}^n\) and \( \mathbf{s}_i=(s_1^{(i)},...,s_n^{(i)})^\top \in \mathbb{R}^n\) for \( i=1,...,p\). Also, \( B=(b_{kj})_{k,j=1}^n\in \mathbb{R}^{n,n}\) with \( b_{kj}=0\) if \( k-j>\ell\) or \( j-k>m\);
    
    \( C = A + UJS^\top + UKU^\top A + UE + XS^\top + YU^\top A + Z \), where \( J, K, E, X, Y, Z \) are as in Definition \ref{def:hmbpsm};
    
    \( \tilde{C} = (I - \tau \mathbf{y} \mathbf{y}^\top)C \), where the vector \( \mathbf{y} \) can be expressed as \( \mathbf{y} = \mathbf{e}_1 + U^{(2)}\bar{\mathbf{k}} + \mathbf{b} \). Here, \(\mathbf{e}_1=(1,0,...,0)^\top \in \mathbb{R}^n\), \( U^{(2)} \in \mathbb{R}^{n\times r} \) satisfies \( U^{(2)}[1,:] = \mathbf{0} \) and \( U^{(2)}[2:n,:] = U[2:n,:] \), \( \bar{\mathbf{k}} \in \mathbb{R}^r \), \( \mathbf{b} = (0, b_2, \ldots, b_{\min(\ell+1,n)}, 0, \ldots, 0)^\top \in \mathbb{R}^n \), and \(\tau\) is a coefficient.


Let \( \bar{\mathbf{u}}_1 = (u_1^{(1)}, \ldots, u_1^{(r)})^\top \in \mathbb{R}^r \). We can write:
\[
\mathbf{e}_1^\top A = \underbrace{\mathbf{d}_1^\top}_{\min(m+1,n)\hbox{ nonzero entries}} + \underbrace{\bar{\mathbf{w}}_1^\top}_{\in \bbR^{1 \times p}} S^\top,
\]
where \( \bar{\mathbf{w}}_1 = (w_1^{(1)}, \ldots, w_1^{(p)})^\top \in \mathbb{R}^p,\) and
\[
\mathbf{b}^\top A = \underbrace{\bar{\mathbf{d}}^\top}_{\min(\ell+m+1, n) \hbox{ nonzero entries}} + {\underbrace{\mathbf{f}^\top}_{\mathbf{b}^\top W  \in \mathbb{R}^{1 \times p} }} S^\top.
\]



Next define the auxiliary vectors: \(\mathbf{c}_1 = J^\top U^\top \mathbf{y} \in \mathbb{R}^p\), \(\mathbf{c}_2 = K^\top U^\top \mathbf{y} \in \mathbb{R}^r\), \(\mathbf{c}_3 = U^\top \mathbf{y} \in \mathbb{R}^r\), \(\mathbf{c}_4 = X^\top \mathbf{y} \in \mathbb{R}^p\), \(\mathbf{c}_5 = Y^\top \mathbf{y} \in \mathbb{R}^r\), \(\mathbf{c}_6 = Z^\top \mathbf{y} \in \mathbb{R}^{n}, \quad \text{which has the form } \mathbf{c}_6 = \begin{bmatrix} \mathbf{c}_{6s} \\ \mathbf{0} \end{bmatrix} \text{ with } \mathbf{c}_{6s} \in \mathbb{R}^{\min(\ell+m,n)}\).

Also, let \( \mathbf{x}^{(1)} = X[1,:] \in \mathbb{R}^p \), \( \mathbf{y}^{(1)} = Y[1,:] \in \mathbb{R}^r \), and \( \mathbf{z}^{(1)} = Z[1,:] \in \mathbb{R}^n \).

We now compute \( (I - \tau \mathbf{y} \mathbf{y}^\top) C \) by distributing the transformation over each term in the definition of \( C \).

(i) Transformation of \( A \):
Substituting the expressions \( \mathbf{y}=\mathbf{e}_1+U^{(2)}\bar{\mathbf{k}}+\mathbf{b}\), \( \mathbf{e}_1^\top A=\mathbf{d}_1^\top+\bar{\mathbf{w}}_1^\top S^\top \), and \( \mathbf{b}^\top A=\bar{\mathbf{d}}^\top+\mathbf{f}^\top S^\top \), we obtain:
\begin{equation}\label{proof_first}
\begin{aligned}
(I - \tau \mathbf{y} \mathbf{y}^\top)A = & A + \mathbf{e}_1 \big[ \underbrace{(-\tau \mathbf{d}_1^\top - \tau \bar{\mathbf{d}}^\top)}_{\min(\ell+m+1,n)\hbox{ nonzero entries}} + \underbrace{(-\tau \bar{\mathbf{w}}_1^\top - \tau \mathbf{f}^\top)}_{\in \bbR^{1 \times p}}S^\top \\
& + \underbrace{(-\tau \bar{\mathbf{k}}^\top)}_{\in \bbR^{1 \times r}}U^{(2)\top}A\big] + U^{(2)}\underbrace{(-\tau \bar{\mathbf{k}} \bar{\mathbf{w}}_1^\top - \tau \bar{\mathbf{k}} \mathbf{f}^\top)}_{\in \bbR^{r \times p}} S^\top \\
& + U^{(2)}\underbrace{(-\tau \bar{\mathbf{k}} \bar{\mathbf{k}}^\top)}_{\in \bbR^{r \times r}}U^{(2)\top}A + U^{(2)}\underbrace{(-\tau \bar{\mathbf{k}} \mathbf{d}_1^\top - \tau \bar{\mathbf{k}} \bar{\mathbf{d}}^\top)}_{\bbR^{r\times \min(\ell+m+1.n)} \hbox{ nonzero entries}} \\
& + \underbrace{(-\tau \mathbf{b} \bar{\mathbf{w}}_1^\top - \tau \mathbf{b} \mathbf{f}^\top)}_{\bbR^{\min(\ell+1,n)\times p} \hbox{ nonzero entries}}S^\top + \underbrace{(-\tau \mathbf{b} \bar{\mathbf{k}}^\top)}_{\bbR^{\min(\ell+1,n)\times r} \hbox{ nonzero entries}}U^{(2)\top}A \\& + \underbrace{(-\tau \mathbf{b} \mathbf{d}_1^\top - \tau \mathbf{b} \bar{\mathbf{d}}^\top)}_{\bbR^{\min(\ell+1,n) \times \min(\ell+m+1,n)} \hbox{ nonzero entries}}.
\end{aligned}
\end{equation}
Dropping the first column, we  see that the first row of the term in brackets is in $\calV_2(A)$. Dropping the first row and column of the remaining terms are in $\calP(A[2:n,2:n])$. 

The transformations of $UJS^\top$, $UKU^\top A$, $UE$, $XS^\top$, $YU^\top A$, and $Z$ yield expressions that follow this same patternâ€”terms involving $\mathbf{e}_1$(after dropping the first column) belong to $\calV_2(A)$, while the remaining terms (after dropping first row and column) belong to $\calP(A[2:n,2:n])$:

(ii) Transformation of \( UJS^\top \): Substituting the expressions \( \mathbf{y}^\top UJ=\mathbf{c}_1^\top \), \( \mathbf{y}=\mathbf{e}_1+U^{(2)}\bar{\mathbf{k}}+\mathbf{b}\), and \( U=\mathbf{e}_1\bar{\mathbf{u}}_1^\top+U^{(2)}\), we obtain
\begin{equation}
\begin{aligned}
(I - \tau \mathbf{y} \mathbf{y}^\top)UJS^\top = & \mathbf{e}_1\underbrace{(\bar{\mathbf{u}}_1^\top J - \tau \mathbf{c}_1^\top)}_{\in \bbR^{1 \times p}}S^\top + U^{(2)}\underbrace{(J - \tau \bar{\mathbf{k}} \mathbf{c}_1^\top)}_{\in \bbR^{r \times p}}S^\top + \underbrace{(-\tau \mathbf{b} \mathbf{c}_1^\top)}_{\bbR^{\min(\ell+1,n)\times p} \hbox{ nonzero entries}}S^\top.
\end{aligned}
\end{equation}

(iii) Transformation of \( UKU^\top A \): Substituting the expressions \( \mathbf{y}^\top UK=\mathbf{c}_2^\top\), \( \mathbf{y}=\mathbf{e}_1+U^{(2)}\bar{\mathbf{k}}+\mathbf{b}\), \( U=\mathbf{e}_1\bar{\mathbf{u}}_1^\top+U^{(2)}\), and \( \mathbf{e}_1^\top A=\mathbf{d}_1^\top+\bar{\mathbf{w}}_1^\top S^\top\), we obtain
\begin{equation}
\begin{aligned}
& (I - \tau \mathbf{y} \mathbf{y}^\top)UKU^\top A \\
= & \mathbf{e}_1\big[\underbrace{(\bar{\mathbf{u}}_1^\top K \bar{\mathbf{u}}_1 \mathbf{d}_1^\top - \tau \mathbf{c}_2^\top \bar{\mathbf{u}}_1 \mathbf{d}_1^\top)}_{\min(\ell+m+1,n)\hbox{ nonzero entries}} + \underbrace{(\bar{\mathbf{u}}_1^\top K \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^\top - \tau \mathbf{c}_2^\top \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^\top)}_{\in \bbR^{1 \times p}}S^\top + \underbrace{(\bar{\mathbf{u}}_1^\top K - \tau \mathbf{c}_2^\top)}_{\in \bbR^{1 \times r}}U^{(2\top}A\big] \\
& + U^{(2)}\underbrace{(K \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^\top - \tau \bar{\mathbf{k}} \mathbf{c}_2^\top \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^\top)}_{\in \bbR^{r \times p}}S^\top + U^{(2)}\underbrace{(K - \tau \bar{\mathbf{k}} \mathbf{c}_2^\top)}_{\in \bbR^{r \times r}}U^{(2)\top}A \\&+ U^{(2)}\underbrace{(K \bar{\mathbf{u}}_1 \mathbf{d}_1^\top - \tau \bar{\mathbf{k}} \mathbf{c}_2^\top \bar{\mathbf{u}}_1 \mathbf{d}_1^\top)}_{\bbR^{r\times \min(\ell+m+1,n)} \hbox{ nonzero entries}} + \underbrace{(-\tau \mathbf{b} \mathbf{c}_2^\top \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^\top)}_{\bbR^{\min(\ell+1,n)\times p} \hbox{ nonzero entries}}S^\top \\&+ \underbrace{(-\tau \mathbf{b} \mathbf{c}_2^\top)}_{\bbR^{\min(\ell+1,n)\times r} \hbox{ nonzero entries}}U^{(2)\top}A + \underbrace{(-\tau \mathbf{b} \mathbf{c}_2^\top \bar{\mathbf{u}}_1 \mathbf{d}_1^\top)}_{\bbR^{\min(\ell+1,n) \times \min(\ell+m+1,n)} \hbox{ nonzero entries}}.
\end{aligned}
\end{equation}

(iv) Transformation of \( UE \): Substituting the expressions \( \mathbf{y}^\top U=\mathbf{c}_3^\top \), \( \mathbf{y}=\mathbf{e}_1+U^{(2)}\bar{\mathbf{k}}+\mathbf{b}\), and \( U=\mathbf{e}_1\bar{\mathbf{u}}_1^\top+U^{(2)}\), we obtain
\begin{equation}
\begin{aligned}
(I - \tau \mathbf{y} \mathbf{y}^\top)UE = & \mathbf{e}_1\underbrace{(\bar{\mathbf{u}}_1^\top E - \tau \mathbf{c}_3^\top E)}_{\min(\ell+m+1,n)\hbox{ nonzero entries}
} + U^{(2)}\underbrace{(E - \tau \bar{\mathbf{k}} \mathbf{c}_3^\top E)}_{\bbR^{r\times \min(\ell+m+1,n)} \hbox{ nonzero entries}} + \\& \underbrace{(-\tau \mathbf{b} \mathbf{c}_3^\top E)}_{\bbR^{\min(\ell+1,n) \times \min(\ell+m+1,n)} \hbox{ nonzero entries}}.
\end{aligned}
\end{equation}

(v) Transformation of \( XS^\top \): Substituting the expressions \(\mathbf{y}^\top X=\mathbf{c}_4^\top \) and \(\mathbf{y}=\mathbf{e}_1+U^{(2)}\bar{\mathbf{k}}+\mathbf{b}\), we obtain
\begin{equation}
\begin{aligned}
(I - \tau \mathbf{y} \mathbf{y}^\top)XS^\top = \mathbf{e}_1\underbrace{(-\tau \mathbf{c}_4^\top)}_{\in \bbR^{1 \times p}}S^\top + U^{(2)}\underbrace{(-\tau \bar{\mathbf{k}} \mathbf{c}_4^\top)}_{\in \bbR^{r \times p}}S^\top + \underbrace{(X - \tau \mathbf{b} \mathbf{c}_4^\top)}_{\bbR^{\min(\ell+1,n)\times p} \hbox{ nonzero entries}}S^\top.
\end{aligned}
\end{equation}

(vi) Transformation of \( YU^\top A \): Substituting the expressions \(\mathbf{y}^\top Y=\mathbf{c}_5^\top \), \(\mathbf{y}=\mathbf{e}_1+U^{(2)}\bar{\mathbf{k}}+\mathbf{b}\), \(U=\mathbf{e}_1\bar{\mathbf{u}}_1^\top+U^{(2)}\), and \(\mathbf{e}_1^\top A=\mathbf{d}_1^\top+\bar{\mathbf{w}}_1^\top S^\top \), we obtain
\begin{equation}
\begin{aligned}
& (I - \tau \mathbf{y} \mathbf{y}^\top)YU^\top A \\
= & \mathbf{e}_1\big[\underbrace{(-\tau \mathbf{c}_5^\top \bar{\mathbf{u}}_1 \mathbf{d}_1^\top)}_{\min(\ell+m+1,n)\hbox{ nonzero entries}} + \underbrace{(-\tau \mathbf{c}_5^\top \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^\top)}_{\in \bbR^{1 \times p}}S^\top + \underbrace{(-\tau \mathbf{c}_5^\top)}_{\in \bbR^{1 \times r}}U^{(2)\top}A\big] \\
& + U^{(2)}\underbrace{(-\tau \bar{\mathbf{k}} \mathbf{c}_5^\top \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^\top)}_{\in \bbR^{r \times p}}S^\top + U^{(2)}\underbrace{(-\tau \bar{\mathbf{k}} \mathbf{c}_5^\top)}_{\in \bbR^{r \times r}}U^{(2)\top}A + U^{(2)}\underbrace{(-\tau \bar{\mathbf{k}} \mathbf{c}_5^\top \bar{\mathbf{u}}_1 \mathbf{d}_1^\top)}_{\bbR^{r\times \min(\ell+m+1,n)} \hbox{ nonzero entries}} \\
& + \underbrace{(Y \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^\top - \tau \mathbf{b} \mathbf{c}_5^\top \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^\top)}_{\bbR^{\min(\ell+1,n)\times p} \hbox{ nonzero entries}}S^\top + \underbrace{(Y - \tau \mathbf{b} \mathbf{c}_5^\top)}_{\bbR^{\min(\ell+1,n)\times r} \hbox{ nonzero entries}}U^{(2)\top}A \\& + \underbrace{(Y \bar{\mathbf{u}}_1 \mathbf{d}_1^\top - \tau \mathbf{b} \mathbf{c}_5^\top \bar{\mathbf{u}}_1 \mathbf{d}_1^\top)}_{\bbR^{\min(\ell+1,n) \times \min(\ell+m+1,n)} \hbox{ nonzero entries}}.
\end{aligned}
\end{equation}

\textbf{(vii) Transformation of \( Z \):} Substituting the expressions \(\mathbf{y}^\top Z=\mathbf{c}_6^\top \) and \(\mathbf{y}=\mathbf{e}_1+U^{(2)}\bar{\mathbf{k}}+\mathbf{b}\), we obtain

\begin{equation}\label{proof_last}
\begin{aligned}
(I - \tau \mathbf{y} \mathbf{y}^\top)Z = &\mathbf{e}_1\underbrace{(-\tau \mathbf{c}_6^\top)}_{\min(\ell+m+1,n)\hbox{ nonzero entries}
} + U^{(2)}\underbrace{(-\tau \bar{\mathbf{k}} \mathbf{c}_6^\top)}_{\bbR^{r\times \min(\ell+m+1,n)} \hbox{ nonzero entries}
} \\&+ \underbrace{(Z - \tau \mathbf{b} \mathbf{c}_6^\top)}_{\bbR^{\min(\ell+1,n) \times \min(\ell+m+1,n)} \hbox{ nonzero entries}}.
\end{aligned}
\end{equation}

Since $\calP(A[2:n,2:n])$ and $\calV_2(A)$ are both vector spaces, their elements are closed under addition. Therefore, we can sum all contributions of each type separately to obtain the overall structure:

Firstly, the submatrix \( \tilde{C}[2:n, 2:n] \) satisfies:
\begin{equation}\label{submatrix_structure}
\tilde{C}[2:n, 2:n] = \tilde{A} + \tilde{U} \tilde{J} \tilde{S}^T + \tilde{U} \tilde{K} \tilde{U}^T \tilde{A} + \tilde{U} \tilde{E} + \tilde{X} \tilde{S}^T + \tilde{Y} \tilde{U}^T \tilde{A} + \tilde{Z},
\end{equation}
where
\begin{align*}
    \tilde{A} &= A[2:n, 2:n] \\
    \tilde{U} &= U[2:n, :] \\
    \tilde{S} &= S[2:n, :]
\end{align*}
and the updated modification matrices are given by:
\begin{align*}
    \tilde{J} &= -\tau \bar{\mathbf{k}} \bar{\mathbf{w}}_1^\top - \tau \bar{\mathbf{k}} \mathbf{f}^\top + J - \tau \bar{\mathbf{k}} \mathbf{c}_1^\top + K \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^\top - \tau \bar{\mathbf{k}} \mathbf{c}_2^\top \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^\top - \tau \bar{\mathbf{k}} \mathbf{c}_4^\top - \tau \bar{\mathbf{k}} \mathbf{c}_5^\top \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^\top, \\
    \tilde{K} &= -\tau \bar{\mathbf{k}} \bar{\mathbf{k}}^\top + K - \tau \bar{\mathbf{k}} \mathbf{c}_2^\top - \tau \bar{\mathbf{k}} \mathbf{c}_5^\top, \\
    \tilde{E} &= [\tilde{E}_s, \mathbf{0}] \in \mathbb{R}^{r \times (n-1)}, \quad \text{with} \\
    \tilde{E}_s &= (-\tau \bar{\mathbf{k}} \mathbf{d}_1^\top - \tau \bar{\mathbf{k}} \bar{\mathbf{d}}^\top + K \bar{\mathbf{u}}_1 \mathbf{d}_1^\top - \tau \bar{\mathbf{k}} \mathbf{c}_2^\top \bar{\mathbf{u}}_1 \mathbf{d}_1^\top + E - \tau \bar{\mathbf{k}} \mathbf{c}_3^\top E \\
    &\quad - \tau \bar{\mathbf{k}} \mathbf{c}_5^\top \bar{\mathbf{u}}_1 \mathbf{d}_1^\top - \tau \bar{\mathbf{k}} \mathbf{c}_6^\top)[:, 2:\min(\ell+m+1, n)], \\
    \tilde{X} &= \begin{bmatrix} \tilde{X}_s \\ \mathbf{0} \end{bmatrix} \in \mathbb{R}^{(n-1) \times p}, \quad \text{with} \\
    \tilde{X}_s &= (-\tau \mathbf{b} \bar{\mathbf{w}}_1^\top - \tau \mathbf{b} \mathbf{f}^\top - \tau \mathbf{b} \mathbf{c}_1^\top - \tau \mathbf{b} \mathbf{c}_2^\top \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^\top + X - \tau \mathbf{b} \mathbf{c}_4^\top \\
    &\quad + Y \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^\top - \tau \mathbf{b} \mathbf{c}_5^\top \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^\top)[2:\min(\ell+1, n), :], \\
    \tilde{Y} &= \begin{bmatrix} \tilde{Y}_s \\ \mathbf{0} \end{bmatrix} \in \mathbb{R}^{(n-1) \times r}, \quad \text{with} \\
    \tilde{Y}_s &= (-\tau \mathbf{b} \bar{\mathbf{k}}^\top - \tau \mathbf{b} \mathbf{c}_2^\top + Y - \tau \mathbf{b} \mathbf{c}_5^\top)[2:\min(\ell+1, n), :], \\
    \tilde{Z} &= \begin{bmatrix} \tilde{Z}_s & \mathbf{0} \\ \mathbf{0} & \mathbf{0} \end{bmatrix} \in \mathbb{R}^{(n-1) \times (n-1)}, \quad \text{with} \\
    \tilde{Z}_s &= (-\tau \mathbf{b} \mathbf{d}_1^\top - \tau \mathbf{b} \bar{\mathbf{d}}^\top - \tau \mathbf{b} \mathbf{c}_2^\top \bar{\mathbf{u}}_1 \mathbf{d}_1^\top - \tau \mathbf{b} \mathbf{c}_3^\top E + Y \bar{\mathbf{u}}_1 \mathbf{d}_1^\top - \tau \mathbf{b} \mathbf{c}_5^\top \bar{\mathbf{u}}_1 \mathbf{d}_1^\top \\
    &\quad + Z - \tau \mathbf{b} \mathbf{c}_6^\top)[2:\min(\ell+1, n), 2:\min(\ell+m+1, n)].
\end{align*}

The forms of \( \tilde{J}, \tilde{K}, \tilde{E}, \tilde{X}, \tilde{Y}, \tilde{Z} \) confirm the first part of the lemma.

Secondly, the first row of the transformed matrix, \( \tilde{C}[1, 2:n]^\top \), can be expressed as:
\begin{equation*}
\tilde{C}[1, 2:n]^\top = \hat{\mathbf{d}}^\top + \hat{\boldsymbol{\alpha}}^\top (S^T[:, 2:n]) + \hat{\boldsymbol{\beta}}^\top ((U^{(2)\top}A)[:, 2:n]),
\end{equation*}
where
\begin{align*}
    \hat{\mathbf{d}} &= \begin{bmatrix} \hat{\mathbf{d}}_s \\ \mathbf{0} \end{bmatrix} \in \mathbb{R}^{n-1}, \quad \text{with} \\
    \hat{\mathbf{d}}_s &= (\mathbf{d}_1^\top - \tau \mathbf{d}_1^\top - \tau \bar{\mathbf{d}}^\top + \bar{\mathbf{u}}_1^\top K \bar{\mathbf{u}}_1 \mathbf{d}_1^\top - \tau \mathbf{c}_2^\top \bar{\mathbf{u}}_1 \mathbf{d}_1^\top + \bar{\mathbf{u}}_1^\top E - \tau \mathbf{c}_3^\top E \\
    &\quad + \mathbf{y}^{(1)\top} \bar{\mathbf{u}}_1 \mathbf{d}_1^\top - \tau \mathbf{c}_5^\top \bar{\mathbf{u}}_1 \mathbf{d}_1^\top + \mathbf{z}^{(1)\top} - \tau \mathbf{c}_6^\top)^\top[2:\min(\ell+m+1, n)], \\
    \hat{\boldsymbol{\alpha}} &= (\bar{\mathbf{w}}_1^\top - \tau \bar{\mathbf{w}}_1^\top - \tau \mathbf{f}^\top + \bar{\mathbf{u}}_1^\top J - \tau \mathbf{c}_1^\top + \bar{\mathbf{u}}_1^\top K \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^\top - \tau \mathbf{c}_2^\top \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^\top \\
    &\quad + \mathbf{x}^{(1)\top} - \tau \mathbf{c}_4^\top + \mathbf{y}^{(1)\top} \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^\top - \tau \mathbf{c}_5^\top \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^\top)^\top \in \mathbb{R}^p, \\
    \hat{\boldsymbol{\beta}} &= (-\tau \bar{\mathbf{k}}^\top + \bar{\mathbf{u}}_1^\top K - \tau \mathbf{c}_2^\top + \mathbf{y}^{(1)\top} - \tau \mathbf{c}_5^\top)^\top \in \mathbb{R}^r.
\end{align*}

Noting that \( U^{(2)} = U - \mathbf{e}_1 \bar{\mathbf{u}}_1^\top \) and \( \mathbf{e}_1^\top A = \mathbf{d}_1^\top + \bar{\mathbf{w}}_1^\top S^\top \), we have \( U^{(2)\top} A = U^\top A - \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^\top S^\top - \bar{\mathbf{u}}_1 \mathbf{d}_1^\top \). Substituting this yields an alternative expression:
\begin{equation}\label{row_structure}
\tilde{C}[1, 2:n]^\top = \tilde{\mathbf{d}}^\top + \tilde{\boldsymbol{\alpha}}^\top (S^\top[:, 2:n]) + \tilde{\boldsymbol{\beta}}^\top ((U^\top A)[:, 2:n]),
\end{equation}
where
\begin{align*}
    \tilde{\mathbf{d}} &= \begin{bmatrix} \tilde{\mathbf{d}}_s \\ \mathbf{0} \end{bmatrix} \in \mathbb{R}^{n-1}, \quad \text{with} \\
    \tilde{\mathbf{d}}_s &= (\mathbf{d}_1^\top - \tau \mathbf{d}_1^\top - \tau \bar{\mathbf{d}}^\top + \bar{\mathbf{u}}_1^\top E - \tau \mathbf{c}_3^\top E + \mathbf{z}^{(1)\top} - \tau \mathbf{c}_6^\top + \tau \bar{\mathbf{k}}^\top \bar{\mathbf{u}}_1 \mathbf{d}_1^\top)^\top[2:\min(\ell+m+1, n)], \\
    \tilde{\boldsymbol{\alpha}} &= (\bar{\mathbf{w}}_1^\top - \tau \bar{\mathbf{w}}_1^\top - \tau \mathbf{f}^\top + \bar{\mathbf{u}}_1^\top J - \tau \mathbf{c}_1^\top + \mathbf{x}^{(1)\top} - \tau \mathbf{c}_4^\top + \tau \bar{\mathbf{k}}^\top \bar{\mathbf{u}}_1 \bar{\mathbf{w}}_1^\top)^\top \in \mathbb{R}^p, \\
    \tilde{\boldsymbol{\beta}} &= (-\tau \bar{\mathbf{k}}^\top + \bar{\mathbf{u}}_1^\top K - \tau \mathbf{c}_2^\top + \mathbf{y}^{(1)\top} - \tau \mathbf{c}_5^\top)^\top \in \mathbb{R}^r.
\end{align*}

This confirms that \( \tilde{C}[1, 2:n] \in \calV_2(A) \), completing the proof of the lemma.
\end{proof}

\subsection{Main Theorem and its Proof}

Equipped with lemma \ref{lemma:structure_preserve}, we now state and prove the main theorem concerning the structure of the QR factor matrix \( F \).

\begin{theorem}\label{thm:structure_preserve}
After applying the QR factorization to a BPS matrix \( A \) (as expressed in Eq. \eqref{express_A}) with lower semiseparable rank \( r \), upper semiseparable rank \( p \), lower bandwidth \( \ell \), and upper bandwidth \( m \), the resulting factor matrix \( F \) is also a BPS matrix. Specifically, its lower semiseparable part has rank \( r \); its upper semiseparable part has rank \( r + p \); its banded part has lower bandwidth \( \ell \) and upper bandwidth \( \ell + m \).

\end{theorem}

\begin{proof}
The QR factorization is computed by performing a sequence of \( n-1 \) Householder transformations, eliminating the subdiagonal entries of \( A \) column by column.

Let \( A^{(i)} \) denote the matrix after the \( i \)-th Householder transformation, with \( A^{(0)} = A \). Define \( A_i = A[i:n, i:n] \), \( U_i = U[i:n, :] \), \( V_i = V[i:n, :] \), \( S_i = S[i:n, :] \), and \( W_i = W[i:n, :] \). Let \( \bar{\mathbf{u}}_i = (u_i^{(1)}, \ldots, u_i^{(r)})^\top \) and \( \bar{\mathbf{w}}_i = (w_i^{(1)}, \ldots, w_i^{(p)})^\top \). Let \( \bar{S} = U^T A \in \mathbb{R}^{r \times n} \).

We prove by induction that after the \( j \)-th transformation (\( 0 \le j < n \)):
\begin{enumerate}
    \item The submatrix \( A^{(j)}[j+1:n, j+1:n] = A_{j+1} + P_{j+1} \) for \(P_{j+1}  \in \calP(A_{j+1}) \)
    \item The \( j \)-th row of the final factor \( F \), \( F[j, j+1:n]^\top \in \calV_{j+1}(A) \).
    \item The \( j \)-th column of \( F \) below the diagonal, \( F[j+1:n, j] \), has the form \( (U \bar{\mathbf{k}}_{j+1})[j+1:n] + \mathbf{b}_{j+1}[2:n+1-j] \), where \( \bar{\mathbf{k}}_{j+1} \in \mathbb{R}^r \) and \( \mathbf{b}_{j+1} \in \mathbb{R}^{n+1-j} \) is non-zero only in its first \( \min(\ell+1, n+1-j) \) entries.
\end{enumerate}

Base Case (j=0): Initially, \( A^{(0)} = A \) is trivially \(= A_{1} + \mathbf{0}_{n\times n}\) for \(\mathbf{0}_{n\times n}\in \calV(A_1)\)(with \( J, K, E, X, Y, Z = \mathbf{0} \)).

Inductive Step: Assume the induction hypothesis holds for \( j \). That is, \( A^{(j)}[j+1:n, j+1:n] = A_{j+1} + P_{j+1} \) for \(P_{j+1}  \in \calP(A_{j+1}) \), and for \( k = 1, \ldots, j \):
\begin{equation}
F[k+1:n, i] = (U \bar{\mathbf{k}}_{k+1})[k+1:n] + \mathbf{b}_{k+1}[2:n+1-k], \label{F_tril}
\end{equation}
\begin{equation}
F[k, k+1:n]^\top = \tilde{\mathbf{d}}_{k+1}^\top + (\tilde{\boldsymbol{\alpha}}_{k+1}^\top S^\top)[k+1:n] + (\tilde{\boldsymbol{\beta}}_{k+1}^\top \bar{S})[k+1:n], \label{F_triu}
\end{equation}
with \( \tilde{\boldsymbol{\alpha}}_{k+1} \in \mathbb{R}^p \), \( \tilde{\boldsymbol{\beta}}_{k+1} \in \mathbb{R}^r \), and \( \tilde{\mathbf{d}}_{k+1} \in \mathbb{R}^{n-i} \) non-zero only in its first \( \min(\ell+m, n-k) \) entries.

Furthermore, assume:
\begin{equation*}
\begin{aligned}
A^{(j)}[j+1:n, j+1:n] = & A_{j+1} + U_{j+1} J_{j+1} S_{j+1}^T + U_{j+1} K_{j+1} U_{j+1}^T A_{j+1} \\
& + U_{j+1} E_{j+1} + X_{j+1} S_{j+1}^T + Y_{j+1} U_{j+1}^T A_{j+1} + Z_{j+1}, \label{After_HT}
\end{aligned}
\end{equation*}
where the modification matrices \( J_{j+1}, K_{j+1}, E_{j+1}, X_{j+1}, Y_{j+1}, Z_{j+1} \) possess the sparsity patterns specified in Definition \ref{def:hmbpsm}.

If \( j < n-1 \), we now perform the \( (j+1) \)-th Householder transformation on $A_{j+1}+P_{j+1}$. Let \( \mathbf{y}_{j+1} \) be the corresponding Householder vector. It can be expressed as  \(\mathbf{y}_{j+1}[1:j] = \mathbf{0}\) and \( \mathbf{y}_{j+1}[j+1:n] = \mathbf{e}_{j+1} + U^{(j+2)} \bar{\mathbf{k}}_{j+2} + \mathbf{b}_{j+2} \), where \( \mathbf{e}_{j+1} \in \mathbb{R}^{n-j} \) is the first standard basis vector, \( U^{(j+2)} \in \mathbb{R}^{(n-j) \times r} \) satisfies \( U^{(j+2)}[1,:] = \mathbf{0} \) and \( U^{(j+2)}[2:n-j, :] = U[j+2:n, :] \), \( \bar{\mathbf{k}}_{j+2} \in \mathbb{R}^r \), and \( \mathbf{b}_{j+2} \in \mathbb{R}^{n-j} \) is non-zero only in entries $2$ through \( \min(\ell+1, n-j) \). This vector defines the \( (j+1) \)-th column of \( F \):
\begin{equation}
F[j+2:n, j+1] = (U \bar{\mathbf{k}}_{j+2})[j+2:n] + \mathbf{b}_{j+2}[2:n-j]. \label{F_tril_2}
\end{equation}

We now apply Lemma \ref{lemma:structure_preserve} to \( A^{(j)}[j+1:n, j+1:n] \), which is \( A_{j+1} + P_{j+1}\). The Householder transformation \( (I - \tau_{j+1} \mathbf{y}_{j+1}[j+1:n] \mathbf{y}_{j+1}[j+1:n]^\top) \) is applied to \( A_{j+1}+P_{j+1} \), here \(\tau_{j+1}\) is a coefficient found to satisfy the definition of a Householder transformation.

From the lemma, the resulting submatrix \( A^{(j+1)}[j+2:n, j+2:n] \) can be expressed as \(A_{j+2}+P_{j+2}\) for \(P_{j+2}\in \calP(A_{j+2})\). Its structure is given by equations analogous to \eqref{submatrix_structure}, with updated modification matrices \( J_{j+2}, K_{j+2}, E_{j+2}, X_{j+2}, Y_{j+2}, Z_{j+2} \), which retain the required sparsity patterns. 

Furthermore, the lemma states that \( A^{(j+1)}[j+1, j+2:n] \in \calV_2(A_j) \). This row becomes \( F[j+1, j+2:n] \) in the final factor matrix. Following the derivation \eqref{row_structure} in the lemma, and using the relation
\begin{equation}\label{eq:relation_UA}
U_{j+1}^\top A_{j+1}[:, 2:n-j] = (U^\top A - \sum_{t=1}^{j} \bar{\mathbf{u}}_t \bar{\mathbf{w}}_t^\top S^\top)[:, j+2:n] - \sum_{t=\max(j-m+2,1)}^{j} \bar{\mathbf{u}}_t (B[t, j+2:n]^\top),
\end{equation}
we can also express this row in the form:
\begin{equation}
\begin{aligned}
F[j+1, j+2:n]^\top = & \tilde{\mathbf{d}}_{j+2}^\top + (\tilde{\boldsymbol{\alpha}}_{j+2}^\top S^\top)[j+2:n] + (\tilde{\boldsymbol{\beta}}_{j+2}^\top \bar{S})[j+2:n], \label{F_triu_2}
\end{aligned}
\end{equation}

where \( \tilde{\mathbf{d}}_{j+2} \) only nonzero in the first $\min(\ell+m, n-j-1)$ entries, \( \tilde{\boldsymbol{\alpha}}_{j+2}\in \mathbb{R}^p\), and \( \tilde{\boldsymbol{\beta}}_{j+2}\in \mathbb{R}^r \). That means \(F[j+1,j+2:n]\in \calV_{j+2}(A)\).

By the principle of induction, the hypotheses hold for all \( j = 0, \ldots, n-1 \).

Upon completion of all \( n-1 \) transformations, the factor matrix \( F \) is fully determined. Aggregating the results from \eqref{F_tril} and \eqref{F_triu}, we conclude that \( F \) can be written in the form:
\begin{equation}
F = B_F + \tril(U \bar{K}^\top, -1) + \triu([\bar{A},\bar{B}] [S,\bar{S}^\top]^\top, 1), \label{express_F}
\end{equation}
where

 \( \bar{K} \in \mathbb{R}^{n \times r} \) is defined by \( \bar{K}[k,:] = \bar{\mathbf{k}}_{k+1}^\top \) for \( k=1,\ldots,n-1 \) and \( \bar{K}[n,:] = \mathbf{0} \).
 
 \( \bar{A} \in \mathbb{R}^{n \times p} \) is defined by \( \bar{A}[k,:] = \boldsymbol{\alpha}_{k+1}^\top \) for \( i=k,\ldots,n-1 \) and \( \bar{A}[n,:] = \mathbf{0} \).
 
\( \bar{B} \in \mathbb{R}^{n \times r} \) is defined by \( \bar{B}[k,:] = \boldsymbol{\beta}_{k+1}^\top \) for \( k=1,\ldots,n-1 \) and \( \bar{B}[n,:] = \mathbf{0} \).

\( B_F \) is a banded matrix with lower bandwidth \( \ell \) and upper bandwidth \( \ell+m \), defined by:
    \[
    B_F[k,j] =
    \begin{cases}
        A^{(k)}[k,k], & k = j < n \\
        A^{(n-1)}[n,n], & k = j = n \\
        \mathbf{b}_{j+1}[k-j+1], & 0 < k - j \le \ell \\
        \tilde{\mathbf{d}}_{k+1}[j-k], & 0 < j - k \le \ell + m \\
        0, & \text{otherwise}.
    \end{cases}
    \]


The representation in \eqref{express_F} explicitly shows that \( F \) is a BPS matrix with a lower semiseparable rank of \( r \), an upper semiseparable rank of \( r+p \), a lower bandwidth of \( \ell \), and an upper bandwidth of \( \ell+m \). This completes the proof.
\end{proof}

\section{Main Algorithms}
\label{sec:algorithms}

\subsection{Fast QR factorization for BPS Matrices}
\label{sec:fastqr}

We now present an $O(n)$ algorithm for computing the QR factorization of a BPS matrix based on theorem \ref{thm:structure_preserve}. 

\begin{algorithm}[H]
\caption{Fast QR Factorization for BPS Matrices}
\label{algo:fastqr}
\begin{algorithmic}
\State \textbf{Input} $B \in \mathbb{R}^{n \times n}$: banded matrix with lower bandwidth $\ell$ and upper bandwidth $m$;
 $U, V \in \mathbb{R}^{n \times r}$: generators for lower semiseparable part (rank $r$);
 $W, S \in \mathbb{R}^{n \times p}$: generators for upper semiseparable part (rank $p$)
\State \textbf{Output} $B_F$: banded part of factor matrix $F$ with bandwidths $\ell$ and $\ell+m$;
$\bar{K} \in \mathbb{R}^{n \times r}$: generator for lower semiseparable part of $F$;
$\bar{A} \in \mathbb{R}^{n \times p}, \bar{B} \in \mathbb{R}^{n \times r}, \bar{S} \in \mathbb{R}^{r\times n}$: generators for upper semiseparable part of $F$;
$\boldsymbol{\tau} \in \mathbb{R}^{n}$: Householder scaling coefficients.

\State \textbf{Initialization:}
\State Compute $\bar{S} \gets U^\top A$ (in $O(n)$ utilizing the semiseparable structure of $A$)
\State Initialize $B_F \gets \mathbf{0}_{n \times n}$, $\bar{K} \gets \mathbf{0}_{n \times r}$, $\bar{A} \gets \mathbf{0}_{n \times p}$, $\bar{B} \gets \mathbf{0}_{n \times r}$
\State Initialize HMBPSM modification matrices: $J \gets \mathbf{0}_{r \times p}$, $K \gets \mathbf{0}_{r \times r}$, $E_s \gets \mathbf{0}_{r \times \min(\ell+m,n)}$, $X_s \gets \mathbf{0}_{\min(\ell,n) \times p}$, $Y_s \gets \mathbf{0}_{\min(\ell,n) \times r}$, $Z_s \gets \mathbf{0}_{\min(\ell,n) \times \min(\ell+m,n)}$

\For{$k = 1$ to $n-1$}
    \State \textbf{Step 1: Form Householder vector $\mathbf{y}_k$}
    \State Run $ \text{FormHouseholderVector}(A^{(k-1)}[k:n, k:n], U[k:n, :], \ell)$ (Algorithm \ref{algo:form_householder}) to get:
    \State $\bar{\mathbf{k}}_{k+1} \gets \text{low-rank part from } \mathbf{y}_k$
    \State $\mathbf{b}_{k+1} \gets \text{banded part from } \mathbf{y}_k$
    \State $\tau_k \gets \text{scaling coefficient}$
    \State $o \gets \text{the diagonal element after the Householder transformation}$ 
    \State $\boldsymbol{\tau}[k] \gets \tau_k$
    
    \State \textbf{Step 2: Store $k$-th column of $F$}
    \State $B_F[k, k] \gets o$ (Diagonal entry)
    \State $\bar{K}[k, :] \gets \bar{\mathbf{k}}_{k+1}^\top$ (Store low-rank generator)
    \For{$j = k+1$ to $\min(k+\ell, n)$}
        \State $B_F[j, k] \gets \mathbf{b}_{k+1}[j-k+1]$ (Store banded part)
    \EndFor
    
    \State \textbf{Step 3: Compute and store $k$-th row of $F$}
    \State $[\tilde{\boldsymbol{\alpha}}_{k+1}, \tilde{\boldsymbol{\beta}}_{k+1}, \tilde{\mathbf{d}}_{k+1}] \gets \text{ComputeRowUpdate}(A^{(k-1)}, U, S, \bar{S}, \mathbf{k}_{k+1}, \mathbf{b}_{k+1}, \tau_k)$ (Algorithm \ref{algo:compute_row})
    \State $\bar{A}[k, :] \gets \tilde{\boldsymbol{\alpha}}_{k+1}^\top$
    \State $\bar{B}[k, :] \gets \tilde{\boldsymbol{\beta}}_{k+1}^\top$
    \For{$j = k+1$ to $\min(k+\ell+m, n)$}
        \State $B_F[k, j] \gets \tilde{\mathbf{d}}_{k+1}[j-k]$ (Store upper banded part)
    \EndFor
    
    \State \textbf{Step 4: Update HMBPSM matrices}
    \State $[J, K, E_s, X_s, Y_s, Z_s] \gets \text{UpdateHMBPSM}(J, K, E_s, X_s, Y_s, Z_s, \bar{\mathbf{k}}_{k+1}, \mathbf{b}_{k+1}, \tau_k)$ (Algorithm \ref{algo:update_hmbpsm})
\EndFor

\State \textbf{Final step:}
\State $B_F[n, n] \gets A^{(n-1)}[n, n]$
\State \Return $B_F, \bar{K}, \bar{A}, \bar{B}, \bar{S}, \boldsymbol{\tau}$. Now $F$ is expressed as $B_F + \text{tril}(U \bar{K}^\top, -1) + \text{triu}([\bar{A},\bar{B}] [S,\bar{S}^\top]^\top, 1)$
\end{algorithmic}
\end{algorithm}



\begin{algorithm}
\caption{FormHouseholderVector (Step 1 of Algorithm \ref{algo:fastqr})}
\label{algo:form_householder}
\begin{algorithmic}
\State \textbf{Input} $C \in \mathbb{R}^{(n-k+1) \times (n-k+1)}$: current HMBPSM submatrix;
$U_k \in \mathbb{R}^{(n-k+1) \times r}$: $U[k:n, :]$;
$\ell$: lower bandwidth.
\State \textbf{Output} $\bar{\mathbf{k}} \in \mathbb{R}^r$, $\mathbf{b} \in \mathbb{R}^{n-k+1}$, $\tau\in \mathbb{R}$ s.t. $\mathbf{y}$: = $\mathbf{e}_1 + U_k^{(2)}\bar{\mathbf{k}} + \mathbf{b}$ is the Householder vector and $I-\tau \mathbf{y}\mathbf{y}^\top$ is the Householder transformation; $o\in \mathbb{R}$ s.t. $\tilde{C}[1,1]=o$ where $\tilde{C}$ is the submatrix after the Householder transformation.

\State Extract first column: $\mathbf{a} \gets C[:, 1]$
\State $\bar{\mathbf{k}} \gets U_k^{\top}\cdot$(the semiseparable part in $\mathbf{a}$)
\State $\mathbf{b} \gets \mathbf{a} - U_k^{(2)}\bar{\mathbf{k}} - \mathbf{e}_1 \cdot \mathbf{a}[1]$
\State $\mathbf{b}$ is only nonzero in entries $2$ through $\min(\ell+1, n-k+1)$.
\State $\tau \gets 2/\mathbf{y}^\top\mathbf{y}$
\State $o \gets -\text{sign}(\textbf{a}[1])\mathbf{a}^\top\mathbf{a}$
\State \Return $\bar{\mathbf{k}}$, $\mathbf{b}$, $\tau$, $o$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{ComputeRowUpdate (Step 3 of Algorithm \ref{algo:fastqr})}
\label{algo:compute_row}
\begin{algorithmic}
\State \textbf{Input} $C$: current HMBPSM represented by modification matrices;
 $U, S, \bar{S}$: generator matrices;
$\mathbf{k}_{k+1}$, $\mathbf{b}_{k+1}$, $\tau_k$: Householder vector and coefficient.
\State \textbf{Output} $\tilde{\boldsymbol{\alpha}} \in \mathbb{R}^p$, $\tilde{\boldsymbol{\beta}} \in \mathbb{R}^r$, $\tilde{\mathbf{d}} \in \mathbb{R}^{n-k}$

\State Compute auxiliary vectors $\mathbf{c}_1, \dots, \mathbf{c}_6$ as in the proof of Lemma~\ref{lemma:structure_preserve} 
\State $\tilde{\boldsymbol{\alpha}} \gets \bar{\mathbf{w}}_k - \tau_k \bar{\mathbf{w}}_k - \tau_k \mathbf{f} + \bar{\mathbf{u}}_k^\top Q - \tau_k \mathbf{c}_1 + \cdots$, full expression from proof
\State $\tilde{\boldsymbol{\beta}} \gets -\tau_k \bar{\mathbf{k}}_{k+1} + K^\top \bar{\mathbf{u}}_k - \tau_k \mathbf{c}_2 + \cdots$,
\State $\tilde{\mathbf{d}} \gets \text{extract banded part from proof, length } \min(\ell+m, n-k)$
\State \Return $\tilde{\boldsymbol{\alpha}}, \tilde{\boldsymbol{\beta}}, \tilde{\mathbf{d}}$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{UpdateHMBPSM (Step 4 of Algorithm \ref{algo:fastqr})}
\label{algo:update_hmbpsm}
\begin{algorithmic}
\State \textbf{Input} $J, K, E_s, X_s, Y_s, Z_s$: current HMBPSM modification matrices;
$\bar{\mathbf{k}}_{k+1}$, $\mathbf{b}_{k+1}$, $\tau_k$.
\Ensure Updated $J, K, E_s, X_s, Y_s, Z_s$

\State Compute updates using formulas from Lemma~\ref{lemma:structure_preserve} proof:
\State $J_{\text{new}} \gets J - \tau_k \bar{\mathbf{k}}_{k+1} \bar{\mathbf{w}}_k^\top - \tau_k \bar{\mathbf{k}}_{k+1} \mathbf{f}^\top - \tau_k \bar{\mathbf{k}}_{k+1} \mathbf{c}_1^\top + \cdots$
\State $K_{\text{new}} \gets K - \tau_k \bar{\mathbf{k}}_{k+1} \bar{\mathbf{k}}_{k+1}^\top - \tau_k \bar{\mathbf{k}}_{k+1} \mathbf{c}_2^\top + \cdots$
\State Update $E_s, X_s, Y_s, Z_s$ similarly with banded truncations
\State \Return $J_{\text{new}}, K_{\text{new}}, E_s^{\text{new}}, X_s^{\text{new}}, Y_s^{\text{new}}, Z_s^{\text{new}}$
\end{algorithmic}
\end{algorithm}

\subsubsection*{Complexity Analysis}

The algorithm runs for $n-1$ steps. The cost per step can be expressed as a polynomial in term of $r$, $p$, $\ell$, and $m$. Since these are constants independent of $n$, the total complexity is $O(n)$. The memory footprint is also $O(n)$, as we store only the generators and banded components.

\begin{remark}

To maintain the $O(1)$ per-step complexity in Algorithm \ref{algo:fastqr}, two key quantities must be computed efficiently during the Householder updates:


    Inner product matrix $U_k^\top U_k$: The computation of intermediate vectors $\mathbf{c}_1, \dots, \mathbf{c}_6$ requires evaluating expressions like $U_k^\top \mathbf{y}_k = U_k^\top(\mathbf{e}_k + U^{(k+1)}\bar{\mathbf{k}}_{k+1} + \mathbf{b}_{k+1})$, which involves $U_k^\top U^{(k+1)}$ that is equal to $U_{k+1}^\top U_{k+1}$. we precompute a lookup table:
    \[
    \text{UU\_lookup}[k] = U[k:n,:]^\top U[k:n,:] \quad \text{for } k = 1,\dots,n
    \]
    This can be computed in $O(nr^2)$ time via a backward accumulation.

    Partial sum $\sum_{t=1}^{j} \bar{\mathbf{u}}_t \bar{\mathbf{w}}_t^\top$: The update of the upper triangular part in equation \eqref{eq:relation_UA} requires this sum. We precompute:
    \[
    \text{UV\_lookup}[j] = \sum_{t=1}^{j} \bar{\mathbf{u}}_t \bar{\mathbf{w}}_t^\top \quad \text{for } j = 1,\dots,n-1
    \]
    This is computed in $O(nrp)$ time via forward accumulation.

Both precomputations require $O(n)$ total time and enable $O(1)$ access to the required quantities at each step of the factorization, thus preserving the overall $O(n)$ complexity.
\end{remark}

\subsection{Fast Solver for BPS Matrices}
\label{sec:solver}

Theorem \ref{thm:structure_preserve} not only enables an efficient QR factorization but also facilitates a complete direct solver for linear systems of the form \( A\mathbf{x} = \mathbf{b} \), where \( A \) is a BPS matrix. The solver consists of two phases after the QR factorization \( A = QR \):
1. Application of \( Q^\top \) to the right-hand side vector \( \mathbf{b} \) to form \( \mathbf{c} = Q^\top \mathbf{b} \).
2. Solution of the upper triangular system \( R\mathbf{x} = \mathbf{c} \) via backward substitution.

We now present \( O(n) \) algorithms for both phases, leveraging the structured representation of the factorization output by Algorithm \ref{algo:fastqr}.

\subsubsection{Fast Application of \( Q^\top \)}

The orthogonal matrix \( Q \) is represented as a product of Householder transformations:
\[
Q = (I - \tau_1 \mathbf{y}_1 \mathbf{y}_1^\top)(I - \tau_2 \mathbf{y}_2 \mathbf{y}_2^\top) \cdots (I - \tau_{n-1} \mathbf{y}_{n-1} \mathbf{y}_{n-1}^\top).
\]
Applying \( Q^\top \) to a vector \( \mathbf{b} \) thus requires computing:
\[
Q^\top\mathbf{b} = (I - \tau_{n-1} \mathbf{y}_{n-1} \mathbf{y}_{n-1}^\top) \cdots (I - \tau_2 \mathbf{y}_2 \mathbf{y}_2^\top)(I - \tau_1 \mathbf{y}_1 \mathbf{y}_1^\top)\mathbf{b}.
\]

The Householder vectors \( \mathbf{y}_k \) are stored in the factor matrix \( F \) according to the normalization convention established in Section~\ref{sec:main}:
\[
\mathbf{y}_k[j] = 
\begin{cases}
0, & j < k \\
1, & j = k \\
F[j,k], & j > k
\end{cases}
\quad \text{for } k = 1, \ldots, n-1.
\]

From Theorem \ref{thm:structure_preserve}, the factor matrix \( F \) admits the BPS representation:
\begin{equation*}
F = B_F + \text{tril}(U_F V_F^\top, -1) + \text{triu}(W_F S_F^\top, 1),
\end{equation*}
where \( U_F, V_F \in \mathbb{R}^{n \times r} \), \( W_F, S_F \in \mathbb{R}^{n \times (r+p)} \), and \( B_F \) is banded with lower bandwidth \( \ell \) and upper bandwidth \( \ell+m \).

This structure implies that each Householder vector \( \mathbf{y}_k \) can be expressed as:
\begin{equation}
\mathbf{y}_k = \bar{\mathbf{e}}_k + U_F^{(k+1)} \bar{\mathbf{v}}_k + \mathbf{d}_k,
\label{eq:y_structure}
\end{equation}
where:

 \( \bar{\mathbf{e}}_k \in \mathbb{R}^n \) is the \( k \)-th standard basis vector,

\( U_F^{(k+1)} \in \mathbb{R}^{n \times r} \) satisfies \( U_F^{(k+1)}[1:k,:] = \mathbf{0} \) and \( U_F^{(k+1)}[k+1:n,:] = U_F[k+1:n,:] \),

\( \bar{\mathbf{v}}_k = V_F[k,:]^\top \in \mathbb{R}^r \),
 
\( \mathbf{d}_k \in \mathbb{R}^n \) is non-zero only in entries \( k+1 \) through \( \min(k+\ell, n) \), with \( \mathbf{d}_k[j] = B_F[j,k] \) for \( j = k+1, \ldots, \min(k+\ell, n) \).


Algorithm~\ref{algo_applyQ} exploits this structure to compute \( Q^\top\mathbf{b} \) in \( O(n) \) operations by maintaining a compressed representation of the intermediate vectors throughout the transformation process.

\begin{algorithm}
\caption{Fast Application of \( Q^\top \) to a Vector}\label{algo_applyQ}
\begin{algorithmic}
\State \textbf{Input:} Factor matrix \( F \) in BPS form: \( F = B_F + \tril(U_F V_F^\top, -1) + \triu(W_F S_F^\top, 1) \); coefficient vector \( \boldsymbol{\tau} = [\tau_1, \ldots, \tau_{n-1},0]^\top \in \mathbb{R}^{n} \); right-hand side vector \( \mathbf{b} \in \mathbb{R}^n \)
\State \textbf{Output:} \( \mathbf{c} = Q^\top\mathbf{b} \in \mathbb{R}^n \)

\State Initialize:

\( O \gets \mathbf{0}_{n \times r} \): Storage for accumulated low-rank updates

\( G \gets \mathbf{0}_{n \times (\ell+1)} \): Storage for banded component updates

\( \mathbf{h} \gets \mathbf{0}_r \): Accumulator for semiseparable component

Let \( \mathbf{o}_i \) denote the \( i \)-th column of \( O \)

Let \( \mathbf{g}_i \) denote the \( i \)-th column of \( G \)


\State Express initial vector: \( \mathbf{b}^{(0)} = \mathbf{b} + U_F^{(1)} \mathbf{h} + \sum_{j=1}^r \mathbf{o}_j + \sum_{j=1}^{l+1} \mathbf{g}_j \)

\For{\( k = 1 \) to \( n-1 \)}
    \State Compute inner product: \( c \gets \mathbf{y}_k^\top \mathbf{b}^{(k-1)} \) (exploit BPS structure of \( \mathbf{y}_k \) and precompute some lookup tables for for \( O(1) \) computation)
    
    \State Update low-rank storage: \( O[k,:] \gets U_F[k,:] \odot \mathbf{h} \) (element-wise multiplication)
    
    \State Update semiseparable accumulator: \( \mathbf{h} \gets \mathbf{h} - \tau_k c V_F[k,:] \)
    
    \State Update banded component:
    \State \( G[k,1] \gets -\tau_k c \) (diagonal contribution)
    \For{\( t = 1 \) to \( \min(\ell, n-k) \)}
        \State \( G[k+t,t+1] \gets -\tau_k c \cdot B_F[k+t,k] \) (subdiagonal contributions)
    \EndFor
    
    \State Current representation: \( \mathbf{b}^{(k)} = \mathbf{b} + U_F^{(k+1)} \mathbf{h} + \sum_{j=1}^r \mathbf{o}_j + \sum_{j=1}^{\ell+1} \mathbf{g}_j \)
\EndFor

\State Compute final result explicitly: \( \mathbf{c} \gets \mathbf{b} + U_F^{(n)} \mathbf{h} + \sum_{j=1}^r \mathbf{o}_j + \sum_{j=1}^{l+1} \mathbf{g}_j \)

\State \Return \( \mathbf{c} \)
\end{algorithmic}
\end{algorithm}

\begin{theorem}
Algorithm~\ref{algo_applyQ} correctly computes \( \mathbf{c} = Q^T\mathbf{b} \) in \( O(n) \) operations.
\end{theorem}

\begin{proof}
The proof proceeds by induction on the transformation steps. Let \( \mathbf{b}^{(0)} = \mathbf{b} \) and assume that after \( k-1 \) steps, the algorithm maintains the representation:
\[
\mathbf{b}^{(k-1)} = \mathbf{b} + U_F^{(k)} \mathbf{h}^{(k-1)} + \sum_{j=1}^r \mathbf{o}_j^{(k-1)} + \sum_{j=1}^{\ell+1} \mathbf{g}_j^{(k-1)},
\]
where the superscripts on $\mathbf{h}$, $\mathbf{o}$, and $\mathbf{g}$ denote the state after the \( (k-1) \)-th iteration.

The \( k \)-th Householder transformation gives:
\[
\mathbf{b}^{(k)} = (I - \tau_k \mathbf{y}_k \mathbf{y}_k^\top) \mathbf{b}^{(k-1)} = \mathbf{b}^{(k-1)} - \tau_k (\mathbf{y}_k^\top \mathbf{b}^{(k-1)}) \mathbf{y}_k.
\]

Substituting the structured form of \( \mathbf{y}_k \) from \eqref{eq:y_structure} and the inductive representation:
\begin{align*}
\mathbf{b}^{(k)} &= \mathbf{b} + U_F^{(k)} \mathbf{h}^{(k-1)} + \sum_{j=1}^r \mathbf{o}_j^{(k-1)} + \sum_{j=1}^{\ell+1} \mathbf{g}_j^{(k-1)} \\
&\quad - \tau_k c (\bar{\mathbf{e}}_k + U_F^{(k+1)} \bar{\mathbf{v}}_k + \mathbf{d}_k) \\
&= \mathbf{b} + U_F^{(k+1)} (\mathbf{h}^{(k-1)} - \tau_k c \bar{\mathbf{v}}_k) \\
&\quad + \left( \sum_{j=1}^r \mathbf{o}_j^{(k-1)} + (U_F^{(k)} - U_F^{(k+1)}) \mathbf{h}^{(k-1)} \right) \\
&\quad + \left( \mathbf{g}_1^{(k-1)} - \tau_k c \bar{\mathbf{e}}_k \right) + \left( \sum_{j=2}^{\ell+1} \mathbf{g}_j^{(k-1)} - \tau_k c \mathbf{d}_k \right).
\end{align*}

The algorithm updates precisely these components:

\( \mathbf{h}^{(k)} = \mathbf{h}^{(k-1)} - \tau_k c \bar{\mathbf{v}}_k \),

\( O[k,:] = U_F[k,:] \odot \mathbf{h}^{(k-1)} \) captures \( (U_F^{(k)} - U_F^{(k+1)}) \mathbf{h}^{(k-1)} \),

Banded updates in \( G \) capture the remaining terms.


Thus, the representation is maintained correctly throughout all \( n-1 \) steps. Each step requires \( O(1) \) operations due to the constant-bounded parameters \( r, p, \ell, m \), yielding overall \( O(n) \) complexity.
\end{proof}

\subsubsection{Fast Backward Substitution}

After computing \( \mathbf{c} = Q^\top \mathbf{b} \), we solve the upper triangular system \( R\mathbf{x} = \mathbf{c} \), where \( R = \triu(F) \) inherits the BPS structure of \( F \). Specifically, the upper triangular part of \( F \) satisfies:
\[
R = B_R + \triu(W_F S_F^\top, 1),
\]
where \( B_R = \triu(B_F) \) is the upper triangular part of the banded component, maintaining upper bandwidth \( \ell+m \).

Algorithm~\ref{algo_backsub}, which is equivalent to the one introduced in~\cite{olver2013fast}, exploits this structure to perform backward substitution in \( O(n) \) operations by maintaining a running sum for the semiseparable contributions.

\begin{algorithm}
\caption{Fast Backward Substitution for Structured \( R \)}\label{algo_backsub}
\begin{algorithmic}
\State \textbf{Input:} Upper triangular matrix \( R = \triu(F) \) in structured form; transformed right-hand side \( \mathbf{c} \in \mathbb{R}^n \)
\State \textbf{Output:} Solution \( \mathbf{x} \in \mathbb{R}^n \) satisfying \( R\mathbf{x} = \mathbf{c} \)

\State Initialize:

\( \mathbf{x} \gets \mathbf{0}_n \): solution vector

\( \mathbf{s} \gets \mathbf{0}_{r+p} \): Accumulator for semiseparable contributions


\For{\( j = n \) down to \( 1 \)}
    \State Initialize residual: \( \text{res} \gets 0 \)
    
    \State Add semiseparable contribution: \( \text{res} \gets \text{res} + W_F[j,:]^\top \mathbf{s} \)
    
    \State Add banded contributions:
    \For{\( k = j+1 \) to \( \min(j+\ell+m, n) \)}
        \State \( \text{res} \gets \text{res} + B_R[j,k] \cdot \mathbf{x}[k] \)
    \EndFor
    
    \State Solve for \( x_j \): \( \mathbf{x}[j] \gets (\mathbf{c}[j] - \text{res}) / B_R[j,j] \)
    
    \State Update semiseparable accumulator: \( \mathbf{s} \gets \mathbf{s} + S_F[j,:]^\top \mathbf{x}[j] \)
\EndFor

\State \Return \( \mathbf{x} \)
\end{algorithmic}
\end{algorithm}

\begin{theorem}
Algorithm~\ref{algo_backsub}  solves \( R\mathbf{x} = \mathbf{c} \) in \( O(n) \) operations.
\end{theorem}

\begin{proof}
For completeness we include the proof from~\cite{olver2013fast}. The algorithm implements standard backward substitution while exploiting the structure of \( R \). For each index \( j \) from \( n \) down to \( 1 \), the equation:
\[
R[j,j] x_j + \sum_{k=j+1}^n R[j,k] x_k = c_j
\]
is solved for \( x_j \).

The key insight is that the off-diagonal entries \( R[j,k] \) for \( k > j \) can be decomposed as:
\[
R[j,k] = B_R[j,k] + W_F[j,:]^\top \cdot S_F[k,:].
\]

The banded contributions \( B_R[j,k] \) are non-zero only for \( k = j+1, \ldots, \min(j+l+m, n) \), requiring \( O(1) \) operations per row. The semiseparable contributions are accumulated in the vector \( \mathbf{s} \), which stores:
\[
\mathbf{s} = \sum_{k=j+1}^n x_k S_F[k,:].
\]

At step \( j \), the product \( W_F[j,:] ^\top \mathbf{s} \) thus captures all semiseparable contributions from previously computed solution components. After computing \( x_j \), the accumulator is updated to include its contribution.

Each iteration requires \( O(1) \) operations, yielding overall \( O(n) \) complexity. The correctness follows by induction from \( j = n \) down to \( 1 \).
\end{proof}

\subsubsection{Overall Solver Complexity}

Combining the QR factorization (Algorithm \ref{algo:fastqr}), the fast application of \( Q^\top \) (Algorithm~\ref{algo_applyQ}), and the fast backward substitution (Algorithm~\ref{algo_backsub}) yields a complete direct solver for BPS linear systems with \( O(n) \) complexity.

\begin{corollary}
For a BPS matrix \( A \in \mathbb{R}^{n \times n} \) with constant-bounded ranks and bandwidths, the linear system \( A\mathbf{x} = \mathbf{b} \) can be solved in \( O(n) \) operations using the QR-based approach.
\end{corollary}


\section{Fast RQ Computation for Symmetric BPS Matrices}
\label{sec:fastrq}

The fast QR factorization developed in the previous section not only provides a direct linear system solver but also forms the foundation for iterative algorithms such as the QR algorithm for computing eigenvalues. A core step in the QR iteration is the formation of the $RQ$ product. For symmetric BPS matrices, we show that the $RQ$ product also preserves the BPS structure, leading to the design of a linear-complexity algorithm for its fast computation.

\subsection{Structure, Definitions, and a Key Lemma for the Symmetric Case}

Consider a symmetric BPS matrix $A$ of the form
\begin{equation}\label{express_A_symm}
    A = B_s + \tril(UV^\top, -1) + \triu(VU^\top, 1),
\end{equation}
where $B_s$ is a symmetric banded matrix with lower and upper bandwidth $\ell$, and $U, V \in \mathbb{R}^{n \times r}$ generate the lower and upper semiseparable parts of rank $r$. Let $A=QR$ be its QR factorization. Since $RQ = (Q^{-1}A)Q=Q^\top A Q$ and $A$ is symmetric, $RQ$ is also symmetric.

To describe the structure of intermediate matrices in the computation of $RQ$, we introduce definitions analogous to those in Section \ref{sec:main} but tailored for the symmetric case and the $R$ factor.


\begin{definition}\label{def:hmutm}
Given an $n\times n$ upper triangular matrix $R$ and an $n\times r$ matrix $U$, define the vector space:
\begin{align*}
\calH(R;U) &:= \bigg\{
R + RU\Omega U^\top + \Phi U^\top + RU\Psi + \Lambda :\\
&\qquad\quad  \Omega \in \mathbb{R}^{r\times r}, \\
 &\qquad\quad  \Phi = \begin{bmatrix} \Phi_s \in \mathbb{R}^{\min(\ell,n)\times r}  \\ \mathbf{0} \end{bmatrix}   \in \mathbb{R}^{n \times r},\\ 
 &\qquad\quad \Psi = [\Psi_s \in \mathbb{R}^{r\times \min(\ell,n)}, \mathbf{0}] \in \mathbb{R}^{r \times n} ,\\
&\qquad\quad \Lambda = \begin{bmatrix} \Lambda_s  \in \mathbb{R}^{\min(\ell,n)\times \min(\ell,n)} & \mathbf{0} \\ \mathbf{0} & \mathbf{0} \end{bmatrix} \in \mathbb{R}^{n \times n}
\bigg\} \subset \bbR^{n \times n}.
\end{align*}
\end{definition}


\begin{definition}\label{def:hmutv}
Given an $n\times n$ upper triangular matrix $R$, an $n\times r$ matrix $U$, and $j=1,2,...,n$, define the vector space:
\begin{align*}
\calW_j(R;U) &:= \bigg\{\boldsymbol{\eta} +  ((RU)[j:n,:])\boldsymbol{\mu} : \\
&\qquad\quad \boldsymbol{\eta} = \begin{bmatrix} \boldsymbol{\eta}_s \in \mathbb{R}^{\min(\ell,n+1-j)}  \\ \mathbf{0} \end{bmatrix} \in \mathbb{R}^{n+1-j},  \boldsymbol{\mu} \in \mathbb{R}^r\bigg\} \subset \bbR^{ n+1-j}.
\end{align*}

\end{definition}

These definitions characterize the structured perturbations that appear in the $R$ factor when it is right-multiplied by a sequence of Householder reflectors (i.e., by $Q$). The following lemma is the symmetric counterpart to Lemma \ref{lemma:structure_preserve} and is the engine of the inductive proof.

\begin{lemma}\label{lemma:structure_preserve_rq}
Given an $n\times n$ upper triangular matrix $R$, an $n\times r$ matrix $U$, and $H\in \calH(R;U)$, let $\Gamma:=R+H$ and consider its right-multiplication by a vector: $\widetilde{\Gamma} = \Gamma (I - \tau \mathbf{y} \mathbf{y}^\top)$. Assume $\mathbf{y}$ has the form $\mathbf{y} = \mathbf{e}_1 + U^{(2)}\bar{\mathbf{k}} + \mathbf{b}$, where $\mathbf{e}_1$ is the first standard basis vector, $U^{(2)}\in\mathbb{R}^{n\times r}$ satisfies $U^{(2)}[1,:]=\mathbf{0}$ and $U^{(2)}[2:n,:]=U[2:n,:]$, $\bar{\mathbf{k}}\in\mathbb{R}^r$, and $\mathbf{b}\in\mathbb{R}^{n}$ is nonzero only in its entries $2$ through $\min(\ell+1, n)$. Then the following hold:
\begin{enumerate}
    \item The principal submatrix satisfies $\widetilde{\Gamma}[2:n, 2:n]=R[2:n,2:n]+\tilde{H}$ for $\tilde{H}\in \calH(R[2:n,2:n];U[2:n,:])$.
    \item The first column satisfies $\widetilde{\Gamma}[2:n, 1]\in \calW_2(R;U)$.
\end{enumerate}
\end{lemma}

\begin{proof}
Let $U = (\mathbf{u}_1,\ldots,\mathbf{u}_r)$ where $\mathbf{u}_i = (u_1^{(i)},\ldots,u_n^{(i)})^\top$, and denote $\bar{\mathbf{u}}_1 = (u_1^{(1)},\ldots,u_1^{(r)})^\top \in \mathbb{R}^r$. Write $\Gamma = R + RU\Omega U^\top + \Phi U^\top + RU\Psi + \Lambda$ with matrices $\Omega, \Phi, \Psi, \Lambda$ having the sparsity patterns specified in Definition \ref{def:hmutm}.

Write $ \boldsymbol{\gamma}:=R\mathbf{b}$ where $\boldsymbol{\gamma}$ is nonzero only in its first $\min(\ell+1,n)$ entries, and define the auxiliary vectors:
\begin{align*}
    \boldsymbol{\delta}_1 &= \Omega U^\top \mathbf{y} \in \mathbb{R}^r, &
    \boldsymbol{\delta}_2 &= U^\top \mathbf{y} \in \mathbb{R}^r, \\
    \boldsymbol{\delta}_3 &= \Psi \mathbf{y} \in \mathbb{R}^r, &
    \boldsymbol{\delta}_4 &= \Lambda \mathbf{y} \in \mathbb{R}^n.
\end{align*}
Note that $\boldsymbol{\delta}_4$ has the form $\boldsymbol{\delta}_4 = \begin{bmatrix} \boldsymbol{\delta}_{4s} \\ \mathbf{0} \end{bmatrix}$ with $\boldsymbol{\delta}_{4s} \in \mathbb{R}^{\min(\ell,n)}$ due to the structure of $\Lambda$.

Also let $\mathbf{r}^{(1)} = R[1,:]$, $\boldsymbol{\phi}^{(1)} = \Phi[1,:]$, $\boldsymbol{\psi}^{(1)} = \Psi[:,1]$, and $\boldsymbol{\lambda}^{(1)} = \Lambda[:,1]$.

We compute $\widetilde{\Gamma} = \Gamma - \tau \Gamma \mathbf{y} \mathbf{y}^\top$ by distributing the operation over each term in $\Gamma$.

(i) Transformation of $R$: Using $\mathbf{y} = \mathbf{e}_1 + U^{(2)}\bar{\mathbf{k}} + \mathbf{b}$ and $R\mathbf{b} = \boldsymbol{\gamma}$, we get:
\begin{equation}\label{eq:r_transform}
\begin{aligned}
R(I - \tau \mathbf{y}\mathbf{y}^\top) = &R + \underbrace{R(-\tau\mathbf{e}_1)\mathbf{e}_1^\top}_{\hbox{the (1,1) entry}} + \big[RU^{(2)}\underbrace{(-\tau\bar{\mathbf{k}})}_{\mathbb{R}^r} + \underbrace{(-\tau\boldsymbol{\gamma})}_{\min(\ell+1,n)\hbox{ nonzero entries}}\big]\mathbf{e}_1^\top \\
&\underbrace{-\tau R\mathbf{e}_1\bar{\mathbf{k}}^\top U^{(2)\top} -\tau R\mathbf{e}_1\mathbf{b}^\top}_{\hbox{only affect the first row}} + \big [RU^{(2)}\underbrace{(-\tau\bar{\mathbf{k}}\bar{\mathbf{k}}^\top)}_{\mathbb{R}^{r\times r}}U^{(2)\top} \\
&+ \underbrace{(-\tau\boldsymbol{\gamma}\bar{\mathbf{k}}^\top)}_{\mathbb{R}^{\min(\ell+1,n)\times r}\hbox{ nonzero entries}}U^{(2)\top} + RU^{(2)}\underbrace{(-\tau\bar{\mathbf{k}}\mathbf{b}^\top)}_{\mathbb{R}^{r\times \min(\ell+1,n)}\hbox{ nonzero entries}} + \\&\underbrace{(-\tau\boldsymbol{\gamma}\mathbf{b}^\top)}_{\mathbb{R}^{\min(\ell+1,n)\times\min(\ell+1,n)}\hbox{ nonzero entries}}\big ].
\end{aligned}
\end{equation}

We  see that the terms in the first bracket after dropping the first row, are in $\calW_2(R;U)$. Dropping the first row and column of the terms in the second bracket are in $\calH(R[2:n,2:n]; U[2:n,:])$. 

Similarly, the following transformations follow the same pattern:


(ii) Transformation of $RU\Omega U^\top$: Using $\mathbf{y} = \mathbf{e}_1 + U^{(2)}\bar{\mathbf{k}} + \mathbf{b}$, $\Omega U^\top\mathbf{y} = \boldsymbol{\delta}_1$, and $U = \mathbf{e}_1\bar{\mathbf{u}}_1^\top + U^{(2)}$, we get:
\begin{equation}
\begin{aligned}
RU\Omega U^\top(I &- \tau\mathbf{y}\mathbf{y}^\top) = \underbrace{R\mathbf{e}_1(\bar{\mathbf{u}}_1^\top\Omega\bar{\mathbf{u}}_1 - \tau\bar{\mathbf{u}}_1^\top\boldsymbol{\delta}_1)\mathbf{e}_1^\top}_{\hbox{the (1,1) entry}} + \big[RU^{(2)}\underbrace{(\Omega\bar{\mathbf{u}}_1 - \tau\boldsymbol{\delta}_1)}_{\mathbb{R}^r}\big]\mathbf{e}_1^\top \\
&\underbrace{+ R\mathbf{e}_1\bar{\mathbf{u}}_1^\top\Omega U^{(2)\top} -\tau R\mathbf{e}_1\bar{\mathbf{u}}_1^\top\boldsymbol{\delta}_1\bar{\mathbf{k}}^\top U^{(2)\top} -\tau R\mathbf{e}_1\bar{\mathbf{u}}_1^\top\boldsymbol{\delta}_1\mathbf{b}^\top }_{\hbox{only affect the first row}}\\
&+ \big[RU^{(2)}\underbrace{(\Omega - \tau \boldsymbol{\delta}_1\bar{\mathbf{k}}^\top)}_{\mathbb{R}^{r\times r}}U^{(2)\top} + RU^{(2)}\underbrace{(-\tau\boldsymbol{\delta}_1\mathbf{b}^\top)}_{\mathbb{R}^{r\times \min(\ell+1,n)} \hbox{ nonzero entries}}\big].
\end{aligned}
\end{equation}

(iii) Transformation of $\Phi U^\top$: Using $\mathbf{y} = \mathbf{e}_1 + U^{(2)}\bar{\mathbf{k}} + \mathbf{b}$, $U^\top\mathbf{y} = \boldsymbol{\delta}_2$, and $U = \mathbf{e}_1\bar{\mathbf{u}}_1^\top + U^{(2)}$, we get:
\begin{equation}
\begin{aligned}
\Phi U^\top(I - \tau\mathbf{y}\mathbf{y}^\top) = &\big[\underbrace{(\Phi\bar{\mathbf{u}}_1 - \tau\Phi\boldsymbol{\delta}_2)}_{\min(\ell+1,n) \hbox{ nonzero entries}}\big]\mathbf{e}_1^\top \\&+ \big[\underbrace{(\Phi - \tau\Phi\boldsymbol{\delta}_2\bar{\mathbf{k}}^\top)}_{\mathbb{R}^{\min(\ell+1,n)\times r}\hbox{ nonzero entries}}U^{(2)\top} + \underbrace{(-\tau\Phi\boldsymbol{\delta}_2\mathbf{b}^\top)}_{\mathbb{R}^{\min(\ell+1,n)\times\min(\ell+1,n)}\hbox{ nonzero entries}}\big].
\end{aligned}
\end{equation}

(iv) Transformation of $RU\Psi$: Using $\mathbf{y} = \mathbf{e}_1 + U^{(2)}\bar{\mathbf{k}} + \mathbf{b}$, $\Psi\mathbf{y} = \boldsymbol{\delta}_3$, and $U = \mathbf{e}_1\bar{\mathbf{u}}_1^\top + U^{(2)}$,  we get:
\begin{equation}
\begin{aligned}
RU\Psi(I &- \tau\mathbf{y}\mathbf{y}^\top) = \underbrace{(-\tau R\mathbf{e}_1\bar{\mathbf{u}}_1^\top\boldsymbol{\delta}_3)\mathbf{e}_1^\top}_{\hbox{the (1,1) entry}} + \big[RU^{(2)}\underbrace{(-\tau\boldsymbol{\delta}_3)}_{\mathbb{R}^r}\big]\mathbf{e}_1^\top \\& \underbrace{+R\mathbf{e}_1\bar{\mathbf{u}}_1^\top\Psi-\tau R\mathbf{e}_1\bar{\mathbf{u}}_1^\top\boldsymbol{\delta}_3\bar{\mathbf{k}}^\top U^{(2)\top} -\tau R\mathbf{e}_1\bar{\mathbf{u}}_1^\top\boldsymbol{\delta}_3\mathbf{b}^\top}_{\hbox{only affect the first row}} \\
&+ \big[RU^{(2)}\underbrace{(-\tau\boldsymbol{\delta}_3\bar{\mathbf{k}}^\top)}_{\mathbb{R}^{r\times r}}U^{(2)\top} + RU^{(2)}\underbrace{(\Psi - \tau\boldsymbol{\delta}_3\mathbf{b}^\top)}_{\mathbb{R}^{r\times\min(\ell+1,n)}\hbox{ nonzero entries}}\big].
\end{aligned}
\end{equation}

(v) Transformation of $\Lambda$: Using $\mathbf{y} = \mathbf{e}_1 + U^{(2)}\bar{\mathbf{k}} + \mathbf{b}$ and $\Lambda \mathbf{y} = \boldsymbol{\delta}_4$, we get:
\begin{equation}\label{eq:lambda_transform}
\begin{aligned}
\Lambda(I - \tau\mathbf{y}\mathbf{y}^\top) = &\big[\underbrace{(-\tau\boldsymbol{\delta}_4)}_{\min(\ell+1,n)\hbox{ nonzero entries}}\big]\mathbf{e}_1^\top + \\&\big[\underbrace{(-\tau\boldsymbol{\delta}_4\bar{\mathbf{k}}^\top)}_{\mathbb{R}^{\min(\ell+1,n)\times r}\hbox{ nonzero entries}}U^{(2)\top} + \underbrace{(\Lambda - \tau \boldsymbol{\delta}_4\mathbf{b}^\top)}_{\mathbb{R}^{\min(\ell+1,n)\times\min(\ell+1,n)} \hbox{ nonzero entries}}\big].
\end{aligned}
\end{equation}

Since $\calH(R[2:n,2:n];U[2:n,:])$ and $\calW_2(R)$ are both vector spaces, their elements are closed under addition. Therefore, summing contributions (i)--(v), we identify the following:

First, the submatrix $\widetilde{\Gamma}[2:n, 2:n]$ satisfies:
\begin{equation}\label{eq:submatrix_structure_rq}
\widetilde{\Gamma}[2:n, 2:n] = \tilde{R} + \tilde{R}\tilde{U}\tilde{\Omega}\tilde{U}^\top + \tilde{\Phi}\tilde{U}^\top + \tilde{R}\tilde{U}\tilde{\Psi} + \tilde{\Lambda},
\end{equation}
where $\tilde{R} = R[2:n,2:n]$, $\tilde{U} = U[2:n,:]$, and the updated matrices are:
\begin{align*}
\tilde{\Omega} &= -\tau\bar{\mathbf{k}}\bar{\mathbf{k}}^\top + \Omega - \tau\boldsymbol{\delta}_1\bar{\mathbf{k}}^\top - \tau\boldsymbol{\delta}_3\bar{\mathbf{k}}^\top \in \mathbb{R}^{r\times r}, \\
\tilde{\Phi} &= \begin{bmatrix} \tilde{\Phi}_s \\ \mathbf{0} \end{bmatrix}\in \mathbb{R}^{(n-1)\times r}, \quad
\tilde{\Phi}_s = (-\tau\boldsymbol{\gamma}\bar{\mathbf{k}}^\top + \Phi - \tau\Phi\boldsymbol{\delta}_2\bar{\mathbf{k}}^\top - \tau\boldsymbol{\delta}_4\bar{\mathbf{k}}^\top)[2:\min(\ell+1,n),:] \in \mathbb{R}^{\min(\ell,n-1)\times r}, \\
\tilde{\Psi} &= [\tilde{\Psi}_s, \mathbf{0}]\in \mathbb{R}^{r\times (n-1)}, \quad
\tilde{\Psi}_s = (-\tau\bar{\mathbf{k}}\mathbf{b}^\top - \tau\boldsymbol{\delta}_1\mathbf{b}^\top + \Psi - \tau\boldsymbol{\delta}_3\mathbf{b}^\top)[:,2:\min(\ell+1,n)] \in \mathbb{R}^{r\times \min(\ell,n-1)}, \\
\tilde{\Lambda} &= \begin{bmatrix} \tilde{\Lambda}_s & \mathbf{0} \\ \mathbf{0} & \mathbf{0} \end{bmatrix}\in \mathbb{R}^{(n-1)\times (n-1)}, \\
\tilde{\Lambda}_s &= (-\tau\boldsymbol{\gamma}\mathbf{b}^\top - \tau\Phi\boldsymbol{\delta}_2\mathbf{b}^\top + \Lambda - \tau\boldsymbol{\delta}_4\mathbf{b}^\top)[2:\min(\ell+1,n),2:\min(\ell+1,n)] \in \mathbb{R}^{\min(\ell,n-1)\times \min(\ell,n-1)}.
\end{align*}
This confirms the first part of the lemma.

Second, the vector $\widetilde{\Gamma}[2:n, 1]$ is given by:
\begin{equation}\label{eq:col_structure_rq}
\widetilde{\Gamma}[2:n, 1] = \boldsymbol{\eta} + (RU\boldsymbol{\mu})[2:n],
\end{equation}
where
\begin{align*}
\boldsymbol{\eta} &= \begin{bmatrix} \boldsymbol{\eta}_s \\ \mathbf{0} \end{bmatrix}, \quad
\boldsymbol{\eta}_s = (-\tau \boldsymbol{\gamma} + \Phi\bar{\mathbf{u}}_1 - \tau\Phi\boldsymbol{\delta}_2 - \tau\boldsymbol{\delta}_4 + \boldsymbol{\lambda}^{(1)})[2:\min(\ell+1,n)] \in \mathbb{R}^{\min(\ell,n-1)}, \\
\boldsymbol{\mu} &= -\tau \bar{\mathbf{k}} + \Omega\bar{\mathbf{u}}_1 - \tau\boldsymbol{\delta}_1 - \tau\boldsymbol{\delta}_3 + \boldsymbol{\psi}^{(1)} \in \mathbb{R}^r.
\end{align*}
This shows the second part of the lemma.

Moreover, the $(1,1)$ entry of $\widetilde{\Gamma}$ is updated as:
\begin{equation}\label{eq:diag_update_rq}
\begin{aligned}
\tilde{\Gamma}[1,1] = &R[1,1] -\tau R[1,1] -\tau\mathbf{r}^{(1)\top}U^{(2)}\bar{\mathbf{k}} -\tau\gamma[1] \\
&+ R[1,1](\bar{\mathbf{u}}_1^\top\Omega\bar{\mathbf{u}}_1 -\tau\bar{\mathbf{u}}_1^\top\boldsymbol{\delta}_1) +\mathbf{r}^{(1)\top}U^{(2)}(\Omega\bar{\mathbf{u}}_1 -\tau\boldsymbol{\delta}_1) \\
&+\boldsymbol{\phi}^{(1)\top}\bar{\mathbf{u}}_1 -\tau\boldsymbol{\phi}^{(1)\top}\boldsymbol{\delta}_2 -\tau R[1,1]\bar{\mathbf{u}}_1^\top\boldsymbol{\delta}_3 +R[1,1]\bar{\mathbf{u}}_1^\top\boldsymbol{\psi}^{(1)} \\
&-\tau\mathbf{r}^{(1)\top}U^{(2)}\boldsymbol{\delta}_3 +\mathbf{r}^{(1)\top}U^{(2)}\boldsymbol{\psi}^{(1)} -\tau \boldsymbol{\delta}_4[1] +\Lambda[1,1].
\end{aligned}
\end{equation}

The updates \eqref{eq:submatrix_structure_rq}, \eqref{eq:col_structure_rq}, and \eqref{eq:diag_update_rq} provide the complete formulas needed to advance the structured representation by one Householder transformation.
\end{proof}

\subsection{Structure-Preserving Theorem for $RQ$}

Equipped with Lemma \ref{lemma:structure_preserve_rq}, we can now state and prove the main structural result for the $RQ$ product.

\begin{theorem}\label{thm:structure_rq}
Let $A$ be a symmetric BPS matrix of the form \eqref{express_A_symm}, with both lower and upper semiseparable rank $r$ and lower and upper bandwidth $\ell$. Let $A=QR$ be its QR factorization. Then the matrix $RQ$ is also a symmetric BPS matrix, with the same lower and upper semiseparable rank $r$ and the same lower and upper bandwidth $\ell$.
\end{theorem}

\begin{proof}
We will show that $RQ$ can be expressed as \begin{equation}\label{express_RQ}
    RQ = B_R + \tril(\Theta \Delta^\top, -1) + \triu(\Delta \Theta^\top, 1),
\end{equation}
where

$\Theta = R U \in \mathbb{R}^{n \times r}$,

$\Delta \in \mathbb{R}^{n \times r}$ is a low-rank generator matrix,

$B_R$ is a symmetric banded matrix with lower and upper bandwidth $\ell$.

The proof proceeds by induction on the steps of applying $Q$ (as a product of Householder reflectors) to $R$ from the right. Define $R^{(0)} = R$ and $R^{(j)} = R (I - \tau_1 \mathbf{y}_1 \mathbf{y}_1^\top) \cdots (I - \tau_j \mathbf{y}_j \mathbf{y}_j^\top)$ for $j=1,\ldots,n-1$, so that $R^{(n-1)} = RQ$.

From the proof of Theorem \ref{thm:structure_preserve}, the Householder vectors have the form:
\begin{equation*}
\mathbf{y}_k[1:k-1]=\mathbf{0} \text{ and }\mathbf{y}_k[k:n] = \mathbf{e}_k + U^{(k+1)}\bar{\mathbf{k}}_{k+1} + \mathbf{b}_{k+1}, \quad k=1,\ldots,n-1,
\end{equation*}
where $\mathbf{e}_k$ is the first standard basis in $\mathbb{R}^{n-k+1}$, $U^{(k+1)}\in\mathbb{R}^{(n-k+1)\times r}$ satisfies $U^{(k+1)}[1,:]=\mathbf{0}$ and $U^{(k+1)}[2:n-k+1,:]=U[k+1:n,:]$, $\bar{\mathbf{k}}_{k+1}\in\mathbb{R}^r$, and $\mathbf{b}_{k+1}\in\mathbb{R}^{n-k+1}$ is nonzero only in its entries $2$ through $\min(\ell+1, n-k+1)$.

Let $R_k = R[k:n, k:n]$ and $U_k = U[k:n, :]$. We prove by induction that for $j=0,\ldots,n-1$:
\begin{enumerate}
    \item $R^{(j)}[j+1:n, j+1:n]=R_{j+1}+H_{j+1}$ for $H_{j+1}\in \calH(R_{j+1};U_{j+1})$.
    \item $R^{(j)}[j+1:n, j]\in \calW_{j+1}(R;U)$.
\end{enumerate}

Base case ($j=0$): $R^{(0)}=R$ is trivially $=R_1+\mathbf{0}_{n\times n}$ for $\mathbf{0}_{n\times n}\in \calH(R_1;U_1)$ (with $\Omega$, $\Phi$, $\Psi$, $\Lambda = \mathbf{0}$).

Inductive step: Assume the statement holds for some $j < n-1$. That is,
\begin{equation*}
R^{(j)}[j+1:n, j+1:n] = R_{j+1} + R_{j+1}U_{j+1}\Omega_{j+1}U_{j+1}^\top + \Phi_{j+1}U_{j+1}^\top + R_{j+1}U_{j+1}\Psi_{j+1} + \Lambda_{j+1},
\end{equation*}
with $\Omega_{j+1}, \Phi_{j+1}, \Psi_{j+1}, \Lambda_{j+1}$ having the required sparsity patterns.

We now compute $R^{(j+1)} = R^{(j)}(I - \tau_{j+1}\mathbf{y}_{j+1}\mathbf{y}_{j+1}^\top)$. Applying Lemma \ref{lemma:structure_preserve_rq} with $\Gamma = R^{(j)}[j+1:n, j+1:n]$, $R = R_{j+1}$, $U = U_{j+1}$, and $\mathbf{y} = \mathbf{y}_{j+1}[j+1:n]$, we obtain:
\begin{enumerate}
    \item $\widetilde{\Gamma}[2:n-j, 2:n-j] = R^{(j+1)}[j+2:n, j+2:n]=R_{j+2}+H_{j+2}$ for $H_{j+2}\in \calS(R_{j+2};U_{j+2})$.
    \item $\widetilde{\Gamma}[2:n-j, 1] = R^{(j+1)}[j+2:n, j+1]\in \calW_{j+2}(R;U)$.
\end{enumerate}
The lemma also provides explicit update formulas for the parameters, showing they retain the correct sparsity patterns. This completes the inductive step.

By induction, the structured form holds for all $j$. In particular, for each $j=0,\ldots,n-2$, we have:
\begin{equation*}
R^{(j+1)}[j+2:n, j+1] = \boldsymbol{\eta}_{j+2} + (R_{j+1}U_{j+1}\boldsymbol{\mu}_{j+2})[2:n-j].
\end{equation*}
Since $R_{j+1}U_{j+1}[2:n-j,:] = RU[j+2:n,:]$, this can be rewritten as:
\begin{equation*}
\begin{aligned}
&(RQ)[j+1, j+2:n] \\& \underbrace{= (RQ)[j+2:n, j+1]}_{\hbox{by symmetry of } RQ} \\&\underbrace{= R^{(j+1)}[j+2:n, j+1]}_{\hbox{by definition of }R^{(j+1)}} \\& =\boldsymbol{\eta}_{j+2} + (RU\boldsymbol{\mu}_{j+2})[j+2:n].
\end{aligned}
\end{equation*}
Let $\Delta \in \mathbb{R}^{n \times r}$ be defined by $\Delta[j,:] = \boldsymbol{\mu}_{j+1}^\top$ for $j=1,\ldots,n-1$ and $\Delta[n,:] = \mathbf{0}$. Let $\Theta = RU$. Then the semiseparable part of $RQ$ is exactly $\tril(\Theta\Delta^\top,-1) + \triu(\Delta\Theta^\top,1)$.

The banded part $B_R$ is constructed from the diagonal entries of $R^{(j)}$ and the vectors $\boldsymbol{\eta}_{j+1}$, which are nonzero only in their first $\min(\ell, n-j)$ entries. The symmetry of $B_R$ follows from the symmetry of $RQ$ and the symmetry of the semiseparable representation.

Thus, $RQ$ admits the representation \eqref{express_RQ}, confirming it is a symmetric BPS matrix with the stated properties.
\end{proof}

\subsection{Fast RQ Algorithm}

Theorem \ref{thm:structure_rq} is constructive and leads directly to a fast algorithm for computing the $RQ$ product without explicitly forming the dense orthogonal matrix $Q$.

\begin{algorithm}\caption{Fast RQ for Symmetric BPS Matrices}\label{algo:fastrq}
\begin{algorithmic}
\State \textbf{Input:} A symmetric BPS matrix $A$ given by its generators: symmetric banded $B_s$ (bandwidth $\ell$), and $U, V \in \mathbb{R}^{n \times r}$ satisfying $A = B_s + \tril(UV^\top, -1) + \triu(VU^\top, 1)$.
\State \textbf{Output:} The $RQ$ product in structured form: symmetric banded matrix $B_R$, and low-rank generators $\Theta$ and $\Delta$.

\Statex
\State 1. \textbf{Compute fast QR factorization:}
\State \quad Run Algorithm \ref{algo:fastqr} on $A$ to obtain the structured factor matrix $F$ and coefficients $\boldsymbol{\tau}$. Extract the structured representation of the $R$ factor and all Householder vectors $\mathbf{y}_k$ (with parameters $\bar{\mathbf{k}}_k$, $\mathbf{b}_k$).
\State \quad Compute and store $\Theta = R U$ using the structured $R$ in $O(n)$.
\Statex
\State 2. \textbf{Initialize HMUTM parameters:}
\State \quad Set $\Omega_1 \gets \mathbf{0}_{r \times r}$, $\Phi_1 \gets \mathbf{0}_{n \times r}$, $\Psi_1 \gets \mathbf{0}_{r \times n}$, $\Lambda_1 \gets \mathbf{0}_{n \times n}$.
\State \quad Initialize $\Delta \gets \mathbf{0}_{n \times r}$.
\Statex
\State 3. \textbf{Forward recursion (apply $Q$ from the right):}
\For{$k = 1$ to $n-1$}
    \State a. Retrieve the Householder parameters $\bar{\mathbf{k}}_{k+1}$ and $\mathbf{b}_{k+1}$ for step $k$.
    \State b. Using the update formulas from Lemma \ref{lemma:structure_preserve_rq} (as derived in its proof), compute the new parameters $(\Omega_{k+1}, \Phi_{k+1}, \Psi_{k+1}, \Lambda_{k+1})$ from $(\Omega_k, \Phi_k, \Psi_k, \Lambda_k, \bar{\mathbf{k}}_{k+1}, \mathbf{b}_{k+1})$.
    \State c. Compute the parameters $\boldsymbol{\mu}_{k+1}$ and $\boldsymbol{\eta}_{k+1}$ from the relevant update formulas in the proof of Lemma \ref{lemma:structure_preserve_rq}.
    \State d. Set $\Delta[k,:] \gets \boldsymbol{\mu}_{k+1}$ and $B_R[k+1:\min(k+1,n),k] \gets \boldsymbol{\eta}_{k+1}[k+1:\min(k+1,n)]$.
    \State e. Compute the diagonal element from the formula in the proof of Lemma \ref{lemma:structure_preserve_rq} and store it in $B_R[k,k]$. 
\EndFor
\Statex
\State 4. \textbf{Final State}
\State \quad Set $B_R[n,n] \gets R^{(n-1)}[n,n]$
\Statex
\State 5. \textbf{Return} $B_R$, $\Theta$, $\Delta$.
\end{algorithmic}
\end{algorithm}

\textbf{Complexity analysis.} Step 1, the QR factorization, requires $O(n)$ operations by Theorem \ref{thm:structure_preserve}. The computation of $\Theta = RU$ can be performed in $O(nr)$ time due to the structure of $R$. The forward recursion in Step 3 performs a constant amount of work per iteration, as all matrix operations involve matrices whose dimensions (e.g., $r \times r$, $r \times \ell$, $\ell \times \ell$) are independent of $n$. Therefore, Algorithm \ref{algo:fastrq} runs in $O(n)$ time and uses $O(n)$ storage.

\begin{remark}\label{rem:symmetric_tracking}
A subtle but crucial point in Algorithm \ref{algo:fastrq} is that during the forward recursion, we track only the evolving structure of the submatrix $R^{(j)}[j+1:n, j+1:n]$, even though the entire matrix $R^{(j)}[:, j+1:n]$ is being modified. More precisely, after applying the first $j$ Householder transformations from the right, the first $j$ columns of $R^{(j)}$ have reached their final state and will not change in subsequent steps. 
However, the first $j$ rows of $R^{(j)}$ (in columns $j+1:n$) are still subject to modification by later transformations. 
Nevertheless, because the final product $RQ$ is known to be symmetric, these pending row entries are not independent: they must eventually equal the corresponding entries in the already-fixed columns. 
This allows the algorithm to safely disregard the explicit updating of these rows and focus solely on the lower-right square submatrix, which is the only part whose future evolution is not predetermined. 
\end{remark}


\begin{remark}[Shifted QR Algorithm]
The results extend directly to the shifted QR algorithm for eigenvalue computations. Given a shift $\mu \in \mathbb{R}$, the shifted matrix $A - \mu I = (B_s - \mu I) + \tril(UV^\top, -1) + \triu(VU^\top, 1)$ remains a symmetric BPS matrix, requiring only a diagonal adjustment to $B$. The resulting $RQ$ product can then be shifted back to obtain $RQ + \mu I$, which maintains the same BPS structure. Hence, each iteration of the shifted QR algorithm---factorization of $A^{(k)} - \mu I = QR$ followed by formation of $A^{(k+1)} = RQ + \mu I$---can both be performed in $O(n)$ time while preserving the BPS structure throughout.
\end{remark}

\section{Numerical results}
\label{sec:experiments}

To validate the theoretical complexity and demonstrate the practical efficiency of our proposed algorithms, we implemented the fast QR factorization, the complete linear solver, and the fast RQ product computation for symmetric BPS matrices in Julia. The implementation is publicly available in the \texttt{BandedPlusSemiseparableMatrices.jl} package~\cite{BandedPlusSemiseparableMatrices2024}, providing an open-source resource for the scientific computing community. Computations were carried out on a MacBook Air equipped with an Apple M2 chip (8-core CPU, 8 GB RAM), without GPU acceleration or access to external computing resources.

\subsection{Linear Complexity Verification}

To verify that the theoretical $O(n)$ complexity holds for matrices with different structural characteristics, we test four parameter sets $(\ell,m,r,p)$ representing different balances between the banded and semiseparable components. Minimal structure: $(1,1,1,1)$; balanced case: $(4,5,5,4)$; band-dominated: $(8,8,1,1)$; semiseparable-dominated: $ (1,1,8,8)$.



\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{qr_linear.pdf}
\caption{Log-log plot of the total solver time (QR factorization + application of $Q^T$ + backward substitution) versus matrix size $n$ for four parameter sets $(\ell, m, r, p)$. The dashed line has slope 1, indicating linear scaling.}
\label{fig:qr_scaling}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{rq_linear.pdf}
\caption{Log-log plot of the $RQ$ computation time for symmetric BPS matrices versus $n$ for four parameter sets $(\ell, r)$. The dashed line has slope 1, indicating linear scaling.}
\label{fig:rq_scaling}
\end{figure}

Figure~\ref{fig:qr_scaling} demonstrates the linear time complexity of our complete QR solver for BPS linear systems. The total execution time scales as $O(n)$ across five orders of magnitude, from $n=100$ to $n=10^6$, for four different parameter configurations. All curves align with the reference line of slope 1, confirming the analysis in Section~\ref{sec:algorithms}.

Figure~\ref{fig:rq_scaling} confirms the $O(n)$ complexity of computing the $RQ$ product for symmetric BPS matrices, a key operation in the QR eigenvalue algorithm. Four parameter sets exhibit linear scaling, with all curves parallel to the $O(n)$ reference line, confirming the analysis in Section~\ref{sec:fastrq}.

\subsection{Numerical Stability of the QR Solver}
\label{subsec:stability}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{qr_stability.pdf}
\caption{Log-log plot of the maximum residual $\max|A\mathbf{x} - \mathbf{b}|$ versus matrix size $n$ for four parameter sets $(\ell, m, r, p)$. Here $\mathbf{x}$ is computed by our complete QR solver (QR factorization followed by application of $Q^\top$ and backward substitution). The dashed gray line indicates the machine precision $\epsilon_{\mathrm{machine}} \approx 2.22 \times 10^{-16}$.}
\label{fig:stability}
\end{figure}

The numerical stability of a QR-based linear solver is crucial for its practical utility. Figure~\ref{fig:stability} demonstrates the excellent stability properties of our structured QR solver across a wide range of matrix sizes and structural parameters. We measure the accuracy through the maximum norm of the residual vector:
\[
\max|\mathbf{A}\mathbf{x} - \mathbf{b}|,
\]
where $\mathbf{x}$ is the solution computed by our solver for a randomly generated right-hand side $\mathbf{b}$.

For all parameter configurations and across six orders of magnitude in $n$ (from $n=10$ to $n=10^6$), the computed residuals remain very small. The growth of the error is gradual: for smaller matrices, it is near machine precision ($\sim 10^{-16}$), while for the largest matrices, the maximum error reaches $O(10^{-6})$, corresponding to approximately six digits of accuracy. Even for the most challenging case (semiseparable-dominated with $r=p=8$) and the largest $n=10^6$, the algorithm achieves a maximum residual of about $6.6 \times 10^{-6}$, demonstrating its robustness and reliable numerical stability across a wide range of matrix structures and sizes.


\begin{remark}
Measuring the accuracy of large-scale QR factorization presents a methodological challenge. Traditional approaches---comparing the factor matrix $\tilde{F}$ generated by our structured QR algorithm against the factor matrix $F_{\text{dense}}$ produced by a standard dense QR routine, or evaluating the reconstruction error $\|\tilde{Q}\tilde{R} - A\|$ where $\tilde{Q},\tilde{R}$ are extracted from $\tilde{F}$---require $O(n^3)$ operations and become infeasible for $n > 10^4$.

We overcome this limitation by leveraging the structure of BPS matrices: after obtaining the solution $\mathbf{x}$ via our $O(n)$ solver, we compute the residual $A\mathbf{x} - \mathbf{b}$ using the $O(n)$ matrix-vector multiplication algorithm for BPS matrices~\cite{chandrasekaran2002fast}. This approach allows us to verify accuracy for matrices as large as $n=10^6$ while maintaining overall $O(n)$ computational cost.
\end{remark}

\subsection{Comparison with HODLR QR}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{qr_comparison.pdf}
\caption{Comparison of QR factorization times between our fast BPS QR algorithm and the HODLR QR implementation from~\cite{massei2020hm}. Both algorithms operate on banded-plus-semiseparable matrices with parameters $\ell=4$, $m=5$, $r=2$, $p=3$. } 
\label{fig:comparison}
\end{figure}

We compare our fast QR factorization against the state-of-the-art HODLR (Hierarchically Off-Diagonal Low-Rank) QR implementation from the hm-toolbox~\cite{massei2020hm}. The hm-toolbox provides efficient MATLAB routines for various structured matrices, including HODLR and HSS matrices, and represents one of the most mature implementations for hierarchical matrix computations.

Figure~\ref{fig:comparison} compares the execution times of QR factorization for BPS matrices obtained using the two approaches. Our algorithm consistently exhibits superior performance as the matrix size increases. This advantage arises from its explicit exploitation of the banded-plus-semiseparable structure, which avoids the overhead associated with general hierarchical representations.

Moreover, the performance gap widens with increasing $n$, confirming that our method is particularly well suited for large-scale problems. For $n = 1{,}000{,}000$, our implementation achieves an approximate $9\times$ speedup over the HODLR approach, highlighting the practical benefits of the proposed specialized algorithm.


\section{Conclusions}
\label{sec:conclusions}

In this paper, we have established a fundamental theoretical result for BPS matrices and developed efficient algorithms based on this foundation. Our main contribution is the proof that the QR factorization of a BPS matrix preserves the BPS structure, with precisely characterized ranks and bandwidths in the resulting factor matrix. This theoretical insight enabled the design of a complete $O(n)$ direct solver for BPS linear systems, comprising:

a structure-preserving QR factorization algorithm (Algorithm~\ref{algo:fastqr});

an efficient $O(n)$ application of $Q^\top$ (Algorithm~\ref{algo_applyQ});

a fast backward substitution routine (Algorithm~\ref{algo_backsub}).


Furthermore, for symmetric BPS matrices, we have shown that the $RQ$ product---a key operation in the QR algorithm for eigenvalues---also maintains the BPS structure. This led to the development of another $O(n)$ algorithm (Algorithm~\ref{algo:fastrq}) for computing $RQ$ without explicitly forming the dense orthogonal matrix $Q$, opening the door to efficient eigenvalue computations for symmetric BPS matrices.

The numerical experiments confirm the linear scaling and accuracy of our approach, while demonstrating significant performance advantages over existing HODLR-based methods. Our implementation in the SemiseparableMatrices.jl package provides the scientific computing community with efficient, open-source tools for working with this important class of structured matrices.

\subsection*{Future Work}


A compelling extension involves applying our methodology to specific blocked banded matrices arising in $hp$-FEM~\cite{knook2024quasi}. These have optimal complexity so-called reverse Cholesky factorizations (Cholesky from the bottom right instead of the top left) for positive definite problems. One of our motativations for the present work is developing an optimal complexity QL factorization for these special block banded matrices.  The key challenge is generalizing our framework to block BPS matrices while maintaining $O(N)$ complexity. The primary difficulty lies in applying Householder transformations from one block to subsequent blocks in $O(n)$ time(where $n$ is block size and $N$ the total size), rather than $O(n^3)$. While our current framework doesn't directly apply, the core insight of structure preservation provides a promising foundation for this challenging extension.

\bibliographystyle{siamplain}
\bibliography{references}
\end{document}
